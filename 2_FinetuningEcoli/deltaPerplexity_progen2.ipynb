{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d35882-bb68-48ea-8369-42b16f991ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "from Bio import SeqIO \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7cc7e-3398-49d4-a016-11426e1cd54b",
   "metadata": {},
   "source": [
    "# Load pretrained and finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d8bd431-7bfb-4053-80a5-afe39af22d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sdowell/scratch/Thesis/Ch3Metrics'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "531817bd-21dc-45a2-a008-aab808af4d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ProGenForCausalLM(\n",
       "      (transformer): ProGenModel(\n",
       "        (wte): Embedding(32, 1024)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x ProGenBlock(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ProGenAttention(\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (mlp): ProGenMLP(\n",
       "              (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_model_name = \"hugohrban/progen2-small\"\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/progen2_151m_ecoli_finetuning_1/checkpoint-11500\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "# Load base model\n",
    "model_pretrained = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model_pretrained.eval()\n",
    "\n",
    "# Load finetuned model with LoRA adapter\n",
    "model_with_adapter = AutoModelForCausalLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "model_finetuned.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc99907-5166-4758-90ac-de679c34cb70",
   "metadata": {},
   "source": [
    "# Compute perplexity for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cf91b80-99a1-4367-9d8f-238427266a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "\n",
    "def compute_causal_perplexity_batched(model, tokenizer, sequences, batch_size=16):\n",
    "    # ensure we have a PAD token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Make batches\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch = sequences[i : i + batch_size]\n",
    "\n",
    "        # tokenize *without* truncation\n",
    "        enc = tokenizer(batch,\n",
    "                        padding=True,\n",
    "                        return_tensors=\"pt\")  # no truncation\n",
    "        input_ids = enc.input_ids.to(device)\n",
    "        attention_mask = enc.attention_mask.to(device)\n",
    "\n",
    "        # prepare labels: mask pad tokens\n",
    "        labels = input_ids.clone()\n",
    "        labels[attention_mask == 0] = -100  # HuggingFaceâ€™s ignore_index\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            # outputs.loss is already avg per *unmasked* token\n",
    "            batch_loss = outputs.loss * attention_mask.sum().item()\n",
    "            total_loss += batch_loss\n",
    "            total_tokens += attention_mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def load_sequences_from_fasta(fasta_path, max_seqs=None):\n",
    "    \"\"\"\n",
    "    Reads protein sequences from a FASTA file.\n",
    "    \n",
    "    Args:\n",
    "        fasta_path (str): Path to the FASTA file.\n",
    "        max_seqs (int, optional): Maximum number of sequences to read.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: List of amino acid sequences.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        seq_str = str(record.seq)\n",
    "        sequences.append(seq_str)\n",
    "        if max_seqs and len(sequences) >= max_seqs:\n",
    "            break\n",
    "    return sequences\n",
    "\n",
    "def evaluate_models(pretrained_model, finetuned_model, tokenizer, test_sequences, \n",
    "                    batch_size=16, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate and compare perplexity between pretrained and finetuned causal models.\n",
    "    \n",
    "    Args:\n",
    "        pretrained_model: The original (pretrained) model.\n",
    "        finetuned_model: The model after fine-tuning.\n",
    "        tokenizer: The tokenizer corresponding to the models.\n",
    "        test_sequences (list[str]): List of test sequences.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        verbose (bool): Whether to print and return evaluation results.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with perplexity results if verbose is True.\n",
    "    \"\"\"\n",
    "    # Evaluate pretrained model\n",
    "    print(\"Evaluating pretrained model...\")\n",
    "    ppl_pretrained = compute_causal_perplexity_batched(\n",
    "        pretrained_model, tokenizer, test_sequences, batch_size\n",
    "    )\n",
    "    \n",
    "    # Evaluate finetuned model\n",
    "    print(\"Evaluating finetuned model...\")\n",
    "    ppl_finetuned = compute_causal_perplexity_batched(\n",
    "        finetuned_model, tokenizer, test_sequences, batch_size\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(\"\\n----- PERPLEXITY RESULTS -----\")\n",
    "    print(f\"Pretrained Model: {ppl_pretrained:.4f}\")\n",
    "    print(f\"Finetuned Model:  {ppl_finetuned:.4f}\")\n",
    "    print(f\"Delta (Fine - Pre): {ppl_finetuned - ppl_pretrained:.4f}\")\n",
    "    if ppl_pretrained != 0:\n",
    "        print(f\"Relative Change:  {(ppl_finetuned - ppl_pretrained) / ppl_pretrained * 100:.2f}%\")\n",
    "    \n",
    "    if verbose:\n",
    "        return {\n",
    "            \"pretrained\": ppl_pretrained,\n",
    "            \"finetuned\": ppl_finetuned,\n",
    "            \"delta\": ppl_finetuned - ppl_pretrained,\n",
    "            \"relative_change\": ((ppl_finetuned - ppl_pretrained) / ppl_pretrained * 100) if ppl_pretrained != 0 else None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ad560-e63b-4d1b-98ce-1765544d4b2e",
   "metadata": {},
   "source": [
    "# Prepare Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ca9a41-82b3-459e-9614-47a20a509e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 469 sequences\n"
     ]
    }
   ],
   "source": [
    "# Define fasta test set path\n",
    "test_set_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/dataset_splits/finetuning_dataset/test.fasta\"\n",
    "\n",
    "# Read in sequences from fasta into list\n",
    "test_sequences = load_sequences_from_fasta(test_set_path, max_seqs=None)\n",
    "\n",
    "print(f\"Loaded {len(test_sequences)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba2c61-3631-41fe-8e65-6ba3e8424751",
   "metadata": {},
   "source": [
    "# Delta perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b899719-e694-410f-81d5-0cbbce40cf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pretrained model...\n",
      "Evaluating finetuned model...\n",
      "\n",
      "----- PERPLEXITY RESULTS -----\n",
      "Pretrained Model: 3.7176\n",
      "Finetuned Model:  1.0250\n",
      "Delta (Fine - Pre): -2.6926\n",
      "Relative Change:  -72.43%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define testing parameters\n",
    "test_params = {\n",
    "    \"batch_size\": 16,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_models(\n",
    "    model_pretrained, \n",
    "    model_finetuned, \n",
    "    tokenizer, \n",
    "    test_sequences, \n",
    "    **test_params\n",
    ")\n",
    "\n",
    "# Save results to JSON file\n",
    "#with open(\"progen2_151m_perplexity_results.json\", \"w\") as f:\n",
    "#    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b71089be-80aa-4004-9b91-d0b2953b84e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pretrained model...\n",
      "Evaluating finetuned model...\n",
      "\n",
      "----- PERPLEXITY RESULTS -----\n",
      "Pretrained Model: 3.7176 Â± 0.0000\n",
      "Finetuned Model:  1.0250 Â± 0.0000\n",
      "Delta (Fine - Pre): -2.6926\n",
      "Relative Change:  -72.43%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "\n",
    "def compute_autoregressive_perplexity_multiple_runs(\n",
    "    model, tokenizer, sequences,\n",
    "    batch_size=16, num_runs=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes perplexity for causal (autoregressive) models (e.g., ProGen2)\n",
    "    across multiple runs and returns the average perplexity and standard deviation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure we have a pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    perplexities = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        # Batch the sequences\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = sequences[i : i + batch_size]\n",
    "\n",
    "            # Tokenize without truncation\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"  # NO truncation!\n",
    "            )\n",
    "            input_ids = enc.input_ids.to(device)\n",
    "            attention_mask = enc.attention_mask.to(device)\n",
    "\n",
    "            # Prepare labels, masking padding tokens\n",
    "            labels = input_ids.clone()\n",
    "            labels[attention_mask == 0] = -100  # ignore_index\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                # outputs.loss is mean over non-ignored tokens\n",
    "                # multiply by token count to get summed loss\n",
    "                token_count = attention_mask.sum().item()\n",
    "                total_loss += outputs.loss.item() * token_count\n",
    "                total_tokens += token_count\n",
    "\n",
    "        # Compute run perplexity\n",
    "        if total_tokens > 0:\n",
    "            avg_loss = total_loss / total_tokens\n",
    "            perplexities.append(math.exp(avg_loss))\n",
    "        else:\n",
    "            perplexities.append(float(\"inf\"))\n",
    "\n",
    "    # Aggregate\n",
    "    avg_ppl = sum(perplexities) / len(perplexities)\n",
    "    var = sum((p - avg_ppl) ** 2 for p in perplexities) / len(perplexities)\n",
    "    std_ppl = math.sqrt(var)\n",
    "\n",
    "    return avg_ppl, std_ppl\n",
    "\n",
    "\n",
    "# ---------- Usage Example ----------\n",
    "\n",
    "def load_sequences_from_fasta(fasta_path, max_seqs=None):\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        if max_seqs and len(sequences) >= max_seqs:\n",
    "            break\n",
    "    return sequences\n",
    "\n",
    "# load your test sequences\n",
    "test_sequences = load_sequences_from_fasta(test_set_path)\n",
    "\n",
    "# evaluate\n",
    "print(\"Evaluating pretrained model...\")\n",
    "avg_pre, std_pre = compute_autoregressive_perplexity_multiple_runs(\n",
    "    model_pretrained, tokenizer, test_sequences,\n",
    "    batch_size=16, num_runs=10\n",
    ")\n",
    "\n",
    "print(\"Evaluating finetuned model...\")\n",
    "avg_fine, std_fine = compute_autoregressive_perplexity_multiple_runs(\n",
    "    model_finetuned, tokenizer, test_sequences,\n",
    "    batch_size=16, num_runs=10\n",
    ")\n",
    "\n",
    "print(\"\\n----- PERPLEXITY RESULTS -----\")\n",
    "print(f\"Pretrained Model: {avg_pre:.4f} Â± {std_pre:.4f}\")\n",
    "print(f\"Finetuned Model:  {avg_fine:.4f} Â± {std_fine:.4f}\")\n",
    "print(f\"Delta (Fine - Pre): {avg_fine - avg_pre:.4f}\")\n",
    "if avg_pre != 0:\n",
    "    pct = (avg_fine - avg_pre) / avg_pre * 100\n",
    "    print(f\"Relative Change:  {pct:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15178b2c-7e4d-4f2c-bd48-b52a780217b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

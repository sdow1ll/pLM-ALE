2025-03-17 00:06:51,776 INFO: Training configuration saved to runs/esm_650m_ecoli_finetuning/train_config.yaml
2025-03-17 00:06:52,751 INFO: LoRA integration complete for the ESM model.
2025-03-17 00:06:52,969 INFO: Loaded 12000 training and 1500 evaluation sequences.
2025-03-17 00:06:52,969 INFO: Using masked language modeling (MLM) data collator for ESM model.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|                                                                                                | 0/18700 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetuneESM2_ProGen2_LoRA.py", line 521, in <module>
    main()
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetuneESM2_ProGen2_LoRA.py", line 500, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/idies/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2241, in train
    return inner_training_loop(
  File "/home/idies/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 2500, in _inner_training_loop
    batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)
  File "/home/idies/miniconda3/lib/python3.9/site-packages/transformers/trainer.py", line 5180, in get_batch_samples
    batch_samples += [next(epoch_iterator)]
  File "/home/idies/miniconda3/lib/python3.9/site-packages/accelerate/data_loader.py", line 566, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/idies/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/home/idies/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1480, in _next_data
    return self._process_data(data)
  File "/home/idies/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1505, in _process_data
    data.reraise()
  File "/home/idies/miniconda3/lib/python3.9/site-packages/torch/_utils.py", line 733, in reraise
    raise exception
ValueError: Caught ValueError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/idies/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
  File "/home/idies/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetuneESM2_ProGen2_LoRA.py", line 193, in __call__
    batch = self.tokenizer(
  File "/home/idies/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2877, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/idies/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2965, in _call_one
    return self.batch_encode_plus(
  File "/home/idies/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 3158, in batch_encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/idies/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 2793, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Truncation and padding are both activated but truncation length (1026) is not a multiple of pad_to_multiple_of (8).

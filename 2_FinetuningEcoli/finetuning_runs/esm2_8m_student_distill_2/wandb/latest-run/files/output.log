2025-05-11 20:22:54,345 INFO: Training configuration saved to runs/esm2_8m_student_distill_2/train_config.yaml
2025-05-11 20:22:58,664 INFO: Found potential target modules: ['esm.encoder.layer.0.attention.self.key', 'esm.encoder.layer.0.attention.self.value', 'esm.encoder.layer.1.attention.self.key', 'esm.encoder.layer.1.attention.self.value', 'esm.encoder.layer.2.attention.self.key', 'esm.encoder.layer.2.attention.self.value', 'esm.encoder.layer.3.attention.self.key', 'esm.encoder.layer.3.attention.self.value', 'esm.encoder.layer.4.attention.self.key', 'esm.encoder.layer.4.attention.self.value', 'esm.encoder.layer.5.attention.self.key', 'esm.encoder.layer.5.attention.self.value']
2025-05-11 20:22:58,665 INFO: Using target module names: ['key', 'value']
2025-05-11 20:22:58,666 INFO: Parameters requiring gradients before LoRA: 7840794
2025-05-11 20:22:58,688 INFO: Trainable parameter: base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,688 INFO: Trainable parameter: base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,688 INFO: Trainable parameter: base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,688 INFO: Trainable parameter: base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,688 INFO: Trainable parameter: base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,689 INFO: Trainable parameter: base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,689 INFO: Trainable parameter: base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,689 INFO: Trainable parameter: base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,689 INFO: Trainable parameter: base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,689 INFO: Trainable parameter: base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,690 INFO: Trainable parameter: base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,690 INFO: Trainable parameter: base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,690 INFO: Trainable parameter: base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,690 INFO: Trainable parameter: base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,690 INFO: Trainable parameter: base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,690 INFO: Trainable parameter: base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,691 INFO: Trainable parameter: base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,691 INFO: Trainable parameter: base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,691 INFO: Trainable parameter: base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,691 INFO: Trainable parameter: base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,691 INFO: Trainable parameter: base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,692 INFO: Trainable parameter: base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,692 INFO: Trainable parameter: base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight (shape: torch.Size([8, 320]))
2025-05-11 20:22:58,692 INFO: Trainable parameter: base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight (shape: torch.Size([320, 8]))
2025-05-11 20:22:58,692 INFO: Trainable parameter: base_model.model.lm_head.bias (shape: torch.Size([33]))
2025-05-11 20:22:58,693 INFO: Trainable parameter: base_model.model.lm_head.dense.weight (shape: torch.Size([320, 320]))
2025-05-11 20:22:58,693 INFO: Trainable parameter: base_model.model.lm_head.dense.bias (shape: torch.Size([320]))
2025-05-11 20:22:58,693 INFO: Trainable parameter: base_model.model.lm_head.layer_norm.weight (shape: torch.Size([320]))
2025-05-11 20:22:58,693 INFO: Trainable parameter: base_model.model.lm_head.layer_norm.bias (shape: torch.Size([320]))
2025-05-11 20:22:58,694 INFO: LoRA integration complete. Trainable parameters: 164833 (2.09% of total)
2025-05-11 20:22:58,742 INFO: Loaded 7489 training and 1404 evaluation sequences.
2025-05-11 20:22:58,743 INFO: Adjusted max_length to 1024 to be a multiple of 8
2025-05-11 20:22:58,743 INFO: Using masked language modeling (MLM) data collator for ESM model.
2025-05-11 20:22:58,744 INFO: Number of trainable parameters before training: 164833
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
  0%|                                                                                                         | 0/300 [00:00<?, ?it/s]/home/sdowell/miniconda3/envs/thesis/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [41:46<00:00,  8.35s/it]
{'loss': 2.4829, 'grad_norm': 20140.583984375, 'learning_rate': 4.000000000000001e-06, 'epoch': 3.27}
{'loss': 2.4805, 'grad_norm': 18389.388671875, 'learning_rate': 8.000000000000001e-06, 'epoch': 6.54}
{'loss': 2.4774, 'grad_norm': 18939.751953125, 'learning_rate': 1.2e-05, 'epoch': 9.81}
{'loss': 2.4897, 'grad_norm': 19225.251953125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 13.27}
{'loss': 2.4665, 'grad_norm': 18324.662109375, 'learning_rate': 2e-05, 'epoch': 16.54}
{'loss': 2.4562, 'grad_norm': 17556.36328125, 'learning_rate': 2.4e-05, 'epoch': 19.81}
{'loss': 2.4605, 'grad_norm': 19106.314453125, 'learning_rate': 2.8000000000000003e-05, 'epoch': 23.27}
{'loss': 2.4317, 'grad_norm': 18801.45703125, 'learning_rate': 3.2000000000000005e-05, 'epoch': 26.54}
{'loss': 2.4157, 'grad_norm': 18081.376953125, 'learning_rate': 3.6e-05, 'epoch': 29.81}
{'loss': 2.4129, 'grad_norm': 17040.990234375, 'learning_rate': 4e-05, 'epoch': 33.27}
{'loss': 2.3777, 'grad_norm': 15818.947265625, 'learning_rate': 4.4000000000000006e-05, 'epoch': 36.54}
{'loss': 2.3537, 'grad_norm': 15732.2802734375, 'learning_rate': 4.8e-05, 'epoch': 39.81}
{'loss': 2.3395, 'grad_norm': 17381.232421875, 'learning_rate': 5.2000000000000004e-05, 'epoch': 43.27}
{'loss': 2.2946, 'grad_norm': 18089.52734375, 'learning_rate': 5.6000000000000006e-05, 'epoch': 46.54}
{'loss': 2.2581, 'grad_norm': 19310.087890625, 'learning_rate': 6e-05, 'epoch': 49.81}
{'loss': 2.2288, 'grad_norm': 20463.7265625, 'learning_rate': 6.400000000000001e-05, 'epoch': 53.27}
{'loss': 2.1655, 'grad_norm': 19422.947265625, 'learning_rate': 6.800000000000001e-05, 'epoch': 56.54}
{'loss': 2.1098, 'grad_norm': 19896.451171875, 'learning_rate': 7.2e-05, 'epoch': 59.81}
{'loss': 2.0571, 'grad_norm': 21313.9765625, 'learning_rate': 7.6e-05, 'epoch': 63.27}
{'loss': 1.9752, 'grad_norm': 20665.103515625, 'learning_rate': 8e-05, 'epoch': 66.54}
{'loss': 1.8979, 'grad_norm': 21087.40625, 'learning_rate': 8.4e-05, 'epoch': 69.81}
{'loss': 1.8274, 'grad_norm': 23415.9609375, 'learning_rate': 8.800000000000001e-05, 'epoch': 73.27}
{'loss': 1.7289, 'grad_norm': 21824.521484375, 'learning_rate': 9.200000000000001e-05, 'epoch': 76.54}
{'loss': 1.6413, 'grad_norm': 23264.69921875, 'learning_rate': 9.6e-05, 'epoch': 79.81}
{'loss': 1.56, 'grad_norm': 24441.30078125, 'learning_rate': 0.0001, 'epoch': 83.27}
{'loss': 1.4602, 'grad_norm': 30610.271484375, 'learning_rate': 0.00010400000000000001, 'epoch': 86.54}
{'loss': 1.3751, 'grad_norm': 29891.0703125, 'learning_rate': 0.00010800000000000001, 'epoch': 89.81}
{'loss': 1.301, 'grad_norm': 43933.04296875, 'learning_rate': 0.00011200000000000001, 'epoch': 93.27}
{'loss': 1.2216, 'grad_norm': 34158.49609375, 'learning_rate': 0.000116, 'epoch': 96.54}
{'loss': 1.1497, 'grad_norm': 37339.2109375, 'learning_rate': 0.00012, 'epoch': 99.81}
{'train_runtime': 2506.3985, 'train_samples_per_second': 298.795, 'train_steps_per_second': 0.12, 'train_loss': 2.063239466349284, 'epoch': 99.81}
[34m[1mwandb[0m: Adding directory to artifact (./runs/esm2_8m_student_distill_2)... Done. 0.1s
***** train metrics *****
  epoch                    =    99.8136
  total_flos               = 27138333GF
  train_loss               =     2.0632
  train_runtime            = 0:41:46.39
  train_samples_per_second =    298.795
  train_steps_per_second   =       0.12
2025-05-11 21:04:46,171 INFO: Training complete.
/home/sdowell/miniconda3/envs/thesis/lib/python3.13/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00,  5.74it/s]
***** eval metrics *****
  epoch                   =    99.8136
  eval_loss               =     1.0769
  eval_runtime            = 0:00:02.66
  eval_samples_per_second =    526.628
  eval_steps_per_second   =      4.126
Evaluation metrics: {'eval_loss': 1.076943039894104, 'eval_runtime': 2.666, 'eval_samples_per_second': 526.628, 'eval_steps_per_second': 4.126, 'epoch': 99.8135593220339}

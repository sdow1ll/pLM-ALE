Unfrozen: Language modeling head (lm_head)
Unfrozen: Transformer layer 4 of 5
Unfrozen: Transformer layer 5 of 5
Trainable parameters: 2,579,873 of 7,840,794 (32.90%)

Detailed trainable parameters:
esm.embeddings.word_embeddings.weight: shape=torch.Size([33, 320]), requires_grad=True
esm.encoder.layer.4.attention.self.query.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.4.attention.self.query.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.attention.self.key.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.4.attention.self.key.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.attention.self.value.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.4.attention.self.value.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.attention.output.dense.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.4.attention.output.dense.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.attention.LayerNorm.weight: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.attention.LayerNorm.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.intermediate.dense.weight: shape=torch.Size([1280, 320]), requires_grad=True
esm.encoder.layer.4.intermediate.dense.bias: shape=torch.Size([1280]), requires_grad=True
esm.encoder.layer.4.output.dense.weight: shape=torch.Size([320, 1280]), requires_grad=True
esm.encoder.layer.4.output.dense.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.LayerNorm.weight: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.4.LayerNorm.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.attention.self.query.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.5.attention.self.query.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.attention.self.key.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.5.attention.self.key.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.attention.self.value.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.5.attention.self.value.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.attention.output.dense.weight: shape=torch.Size([320, 320]), requires_grad=True
esm.encoder.layer.5.attention.output.dense.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.attention.LayerNorm.weight: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.attention.LayerNorm.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.intermediate.dense.weight: shape=torch.Size([1280, 320]), requires_grad=True
esm.encoder.layer.5.intermediate.dense.bias: shape=torch.Size([1280]), requires_grad=True
esm.encoder.layer.5.output.dense.weight: shape=torch.Size([320, 1280]), requires_grad=True
esm.encoder.layer.5.output.dense.bias: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.LayerNorm.weight: shape=torch.Size([320]), requires_grad=True
esm.encoder.layer.5.LayerNorm.bias: shape=torch.Size([320]), requires_grad=True
lm_head.bias: shape=torch.Size([33]), requires_grad=True
lm_head.dense.weight: shape=torch.Size([320, 320]), requires_grad=True
lm_head.dense.bias: shape=torch.Size([320]), requires_grad=True
lm_head.layer_norm.weight: shape=torch.Size([320]), requires_grad=True
lm_head.layer_norm.bias: shape=torch.Size([320]), requires_grad=True
Traceback (most recent call last):
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetune-freeze.py", line 517, in <module>
    main()
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetune-freeze.py", line 412, in main
    train_sequences = [seq.sequence for seq in read_fasta(config.train_path)]
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetune-freeze.py", line 48, in read_fasta
    text = Path(fasta_file).read_text()
  File "/home/idies/miniconda3/envs/finetuning/lib/python3.9/pathlib.py", line 1266, in read_text
    with self.open(mode='r', encoding=encoding, errors=errors) as f:
  File "/home/idies/miniconda3/envs/finetuning/lib/python3.9/pathlib.py", line 1252, in open
    return io.open(self, mode, buffering, encoding, errors, newline,
  File "/home/idies/miniconda3/envs/finetuning/lib/python3.9/pathlib.py", line 1120, in _opener
    return self._accessor.open(self, flags, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'Storage/sdowell/persistent/ALEdb/split_data/train.fasta'
Traceback (most recent call last):
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetune-freeze.py", line 517, in <module>
    main()
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetune-freeze.py", line 412, in main
    train_sequences = [seq.sequence for seq in read_fasta(config.train_path)]
  File "/home/idies/workspace/Storage/sdowell/persistent/ALEdb/BenchmarkingFinetuning/finetune-freeze.py", line 48, in read_fasta
    text = Path(fasta_file).read_text()
  File "/home/idies/miniconda3/envs/finetuning/lib/python3.9/pathlib.py", line 1266, in read_text
    with self.open(mode='r', encoding=encoding, errors=errors) as f:
  File "/home/idies/miniconda3/envs/finetuning/lib/python3.9/pathlib.py", line 1252, in open
    return io.open(self, mode, buffering, encoding, errors, newline,
  File "/home/idies/miniconda3/envs/finetuning/lib/python3.9/pathlib.py", line 1120, in _opener
    return self._accessor.open(self, flags, mode)
FileNotFoundError: [Errno 2] No such file or directory: 'Storage/sdowell/persistent/ALEdb/split_data/train.fasta'

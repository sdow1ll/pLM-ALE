{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d35882-bb68-48ea-8369-42b16f991ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 17:46:28.466675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-14 17:46:28.484410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-14 17:46:28.484439: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-14 17:46:28.496913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-14 17:46:29.606233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "from Bio import SeqIO \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7cc7e-3398-49d4-a016-11426e1cd54b",
   "metadata": {},
   "source": [
    "# Load pretrained and finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8bd431-7bfb-4053-80a5-afe39af22d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sdowell/scratch/Thesis/Ch4Metrics'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531817bd-21dc-45a2-a008-aab808af4d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForMaskedLM(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 640, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 640, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-29): 30 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=640, out_features=2560, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): EsmLMHead(\n",
       "          (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (decoder): Linear(in_features=640, out_features=33, bias=False)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): EsmLMHead(\n",
       "            (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (decoder): Linear(in_features=640, out_features=33, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\"\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "# Load base model\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model_pretrained.eval()\n",
    "\n",
    "# Load finetuned model with LoRA adapter\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "model_finetuned.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc99907-5166-4758-90ac-de679c34cb70",
   "metadata": {},
   "source": [
    "# Compute perplexity for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ad560-e63b-4d1b-98ce-1765544d4b2e",
   "metadata": {},
   "source": [
    "# Prepare Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ca9a41-82b3-459e-9614-47a20a509e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 469 sequences\n"
     ]
    }
   ],
   "source": [
    "# Define fasta test set path\n",
    "test_set_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/dataset_splits/finetuning_dataset/test.fasta\"\n",
    "\n",
    "# Read in sequences from fasta into list\n",
    "test_sequences = load_sequences_from_fasta(test_set_path, max_seqs=None)\n",
    "\n",
    "print(f\"Loaded {len(test_sequences)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba2c61-3631-41fe-8e65-6ba3e8424751",
   "metadata": {},
   "source": [
    "# Delta perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b71089be-80aa-4004-9b91-d0b2953b84e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.30s/it]\n",
      "Run 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.32s/it]\n",
      "Run 3/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 4/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 5/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.32s/it]\n",
      "Run 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.32s/it]\n",
      "Run 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 8/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 9/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 10/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Average Perplexity: 6.04 ± 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.36s/it]\n",
      "Run 3/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]\n",
      "Run 4/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]\n",
      "Run 5/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]\n",
      "Run 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 8/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]\n",
      "Run 9/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 10/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned Average Perplexity: 1.02 ± 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_mlm_perplexity_multiple_runs(model, tokenizer, sequences, batch_size=16, mask_prob=0.15, num_runs=10):\n",
    "    \"\"\"\n",
    "    Computes perplexity for masked language models (MLM) over multiple runs.\n",
    "    \n",
    "    For each run, a new random masking is applied to the input sequences, and the MLM loss is computed.\n",
    "    The final perplexity is calculated by taking the exponential of the average loss across all masked tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequences (list[str]): List of sequences (e.g., protein sequences) to evaluate.\n",
    "        batch_size (int): Number of sequences per batch.\n",
    "        mask_prob (float): Fraction of tokens to mask in each sequence.\n",
    "        num_runs (int): Number of runs to perform (to capture run-to-run variability).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_perplexity, standard_deviation) calculated across the multiple runs.\n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Disable dropout and other training-specific layers\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        total_loss = 0.0\n",
    "        total_masked_tokens = 0\n",
    "        \n",
    "        # Create batches from sequences\n",
    "        batches = [sequences[i:i + batch_size] for i in range(0, len(sequences), batch_size)]\n",
    "        \n",
    "        for batch in tqdm(batches, desc=f\"Run {run+1}/{num_runs}\"):\n",
    "            # Tokenize and pad the batch\n",
    "            encoded_inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = encoded_inputs.input_ids.to(device)\n",
    "            attention_mask = encoded_inputs.attention_mask.to(device)\n",
    "            \n",
    "            # Prepare copies for masking\n",
    "            masked_input_ids = input_ids.clone()\n",
    "            # Set labels to -100 initially; only masked tokens will have a valid label.\n",
    "            labels = torch.full_like(input_ids, -100)\n",
    "            \n",
    "            # Apply random masking per sequence in the batch\n",
    "            for i in range(input_ids.shape[0]):\n",
    "                # Identify positions with actual (non-padding) tokens\n",
    "                active_indices = torch.where(attention_mask[i] == 1)[0]\n",
    "                num_to_mask = max(1, int(len(active_indices) * mask_prob))\n",
    "                # Randomly sample tokens to mask\n",
    "                to_mask = active_indices[torch.randperm(len(active_indices))[:num_to_mask]]\n",
    "                labels[i, to_mask] = input_ids[i, to_mask]\n",
    "                masked_input_ids[i, to_mask] = tokenizer.mask_token_id\n",
    "            \n",
    "            # Compute loss for the current batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "            \n",
    "            # Count how many tokens were masked (i.e. tokens with a label other than -100)\n",
    "            batch_masked_tokens = (labels != -100).sum().item()\n",
    "            if batch_masked_tokens > 0:\n",
    "                total_loss += outputs.loss.item() * batch_masked_tokens\n",
    "                total_masked_tokens += batch_masked_tokens\n",
    "        \n",
    "        # Compute perplexity for this run\n",
    "        if total_masked_tokens > 0:\n",
    "            avg_loss = total_loss / total_masked_tokens\n",
    "            perplexity = math.exp(avg_loss)\n",
    "        else:\n",
    "            perplexity = float('inf')\n",
    "        \n",
    "        perplexities.append(perplexity)\n",
    "    \n",
    "    # Calculate average and standard deviation of perplexities over the runs\n",
    "    average_perplexity = sum(perplexities) / len(perplexities)\n",
    "    variance = sum((p - average_perplexity) ** 2 for p in perplexities) / len(perplexities)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    \n",
    "    return average_perplexity, std_dev\n",
    "\n",
    "# Example usage:\n",
    "# Assume `model`, `tokenizer`, and `test_sequences` are predefined.\n",
    "# test_sequences can be loaded from a FASTA file using the function below:\n",
    "\n",
    "def load_sequences_from_fasta(fasta_path, max_seqs=None):\n",
    "    \"\"\"\n",
    "    Reads protein sequences from a FASTA file.\n",
    "    \n",
    "    Args:\n",
    "        fasta_path (str): Path to the FASTA file.\n",
    "        max_seqs (int, optional): Maximum number of sequences to read.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: List of protein sequences.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        if max_seqs and len(sequences) >= max_seqs:\n",
    "            break\n",
    "    return sequences\n",
    "\n",
    "# Usage:\n",
    "test_sequences = load_sequences_from_fasta(test_set_path, max_seqs=None)\n",
    "avg_perp, perp_std = compute_mlm_perplexity_multiple_runs(model_pretrained, tokenizer, test_sequences)\n",
    "print(f\"Pretrained Average Perplexity: {avg_perp:.2f} ± {perp_std:.2f}\")\n",
    "avg_perp, perp_std = compute_mlm_perplexity_multiple_runs(model_finetuned, tokenizer, test_sequences)\n",
    "print(f\"Finetuned Average Perplexity: {avg_perp:.2f} ± {perp_std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e33110-f64d-4af8-af3c-33ab75e8daba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:38<00:00,  1.29s/it]\n",
      "Run 3/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 4/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.30s/it]\n",
      "Run 5/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.30s/it]\n",
      "Run 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 8/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.30s/it]\n",
      "Run 9/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n",
      "Run 10/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:39<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating finetuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 2/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 3/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 4/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]\n",
      "Run 5/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 6/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]\n",
      "Run 7/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.34s/it]\n",
      "Run 8/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 9/10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]\n",
      "Run 10/10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:40<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to perplexity_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_mlm_perplexity_multiple_runs(model, tokenizer, sequences, batch_size=16, mask_prob=0.15, num_runs=10):\n",
    "    \"\"\"\n",
    "    Computes perplexity for masked language models (MLM) over multiple runs.\n",
    "    \n",
    "    For each run, a new random masking is applied to the input sequences, and the MLM loss is computed.\n",
    "    The final perplexity is calculated by taking the exponential of the average loss across all masked tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequences (list[str]): List of sequences (e.g., protein sequences) to evaluate.\n",
    "        batch_size (int): Number of sequences per batch.\n",
    "        mask_prob (float): Fraction of tokens to mask in each sequence.\n",
    "        num_runs (int): Number of runs to perform (to capture run-to-run variability).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average_perplexity, standard_deviation) calculated across the multiple runs.\n",
    "    \"\"\"\n",
    "    perplexities = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Disable dropout and other training-specific layers\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        total_loss = 0.0\n",
    "        total_masked_tokens = 0\n",
    "        \n",
    "        # Create batches from sequences\n",
    "        batches = [sequences[i:i + batch_size] for i in range(0, len(sequences), batch_size)]\n",
    "        \n",
    "        for batch in tqdm(batches, desc=f\"Run {run+1}/{num_runs}\"):\n",
    "            # Tokenize and pad the batch\n",
    "            encoded_inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            input_ids = encoded_inputs.input_ids.to(device)\n",
    "            attention_mask = encoded_inputs.attention_mask.to(device)\n",
    "            \n",
    "            # Prepare copies for masking\n",
    "            masked_input_ids = input_ids.clone()\n",
    "            # Set labels to -100 initially; only masked tokens will have a valid label.\n",
    "            labels = torch.full_like(input_ids, -100)\n",
    "            \n",
    "            # Apply random masking per sequence in the batch\n",
    "            for i in range(input_ids.shape[0]):\n",
    "                # Identify positions with actual (non-padding) tokens\n",
    "                active_indices = torch.where(attention_mask[i] == 1)[0]\n",
    "                num_to_mask = max(1, int(len(active_indices) * mask_prob))\n",
    "                # Randomly sample tokens to mask\n",
    "                to_mask = active_indices[torch.randperm(len(active_indices))[:num_to_mask]]\n",
    "                labels[i, to_mask] = input_ids[i, to_mask]\n",
    "                masked_input_ids[i, to_mask] = tokenizer.mask_token_id\n",
    "            \n",
    "            # Compute loss for the current batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "            \n",
    "            # Count how many tokens were masked (i.e. tokens with a label other than -100)\n",
    "            batch_masked_tokens = (labels != -100).sum().item()\n",
    "            if batch_masked_tokens > 0:\n",
    "                total_loss += outputs.loss.item() * batch_masked_tokens\n",
    "                total_masked_tokens += batch_masked_tokens\n",
    "        \n",
    "        # Compute perplexity for this run\n",
    "        if total_masked_tokens > 0:\n",
    "            avg_loss = total_loss / total_masked_tokens\n",
    "            perplexity = math.exp(avg_loss)\n",
    "        else:\n",
    "            perplexity = float('inf')\n",
    "        \n",
    "        perplexities.append(perplexity)\n",
    "    \n",
    "    # Calculate average perplexity and standard deviation across runs\n",
    "    average_perplexity = sum(perplexities) / len(perplexities)\n",
    "    variance = sum((p - average_perplexity) ** 2 for p in perplexities) / len(perplexities)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    \n",
    "    return average_perplexity, std_dev\n",
    "\n",
    "# Function to load sequences from a FASTA file\n",
    "def load_sequences_from_fasta(fasta_path, max_seqs=None):\n",
    "    \"\"\"\n",
    "    Reads protein sequences from a FASTA file.\n",
    "    \n",
    "    Args:\n",
    "        fasta_path (str): Path to the FASTA file.\n",
    "        max_seqs (int, optional): Maximum number of sequences to read.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: List of protein sequences.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        if max_seqs and len(sequences) >= max_seqs:\n",
    "            break\n",
    "    return sequences\n",
    "\n",
    "# --- Main evaluation and saving results to JSON --- #\n",
    "\n",
    "# Load test sequences\n",
    "test_fasta_path = test_set_path \n",
    "test_sequences = load_sequences_from_fasta(test_fasta_path, max_seqs=None)\n",
    "\n",
    "# Define testing parameters for MLM (with masking)\n",
    "test_params = {\n",
    "    \"batch_size\": 16,\n",
    "    \"mask_prob\": 0.15,\n",
    "    \"num_runs\": 10\n",
    "}\n",
    "\n",
    "# Evaluate pretrained and finetuned models\n",
    "print(\"Evaluating pretrained model...\")\n",
    "avg_pretrained, std_pretrained = compute_mlm_perplexity_multiple_runs(\n",
    "    model_pretrained, tokenizer, test_sequences, **test_params\n",
    ")\n",
    "\n",
    "print(\"Evaluating finetuned model...\")\n",
    "avg_finetuned, std_finetuned = compute_mlm_perplexity_multiple_runs(\n",
    "    model_finetuned, tokenizer, test_sequences, **test_params\n",
    ")\n",
    "\n",
    "# Organize results in a dictionary\n",
    "results = {\n",
    "    \"pretrained\": {\n",
    "         \"average_perplexity\": avg_pretrained,\n",
    "         \"std_dev\": std_pretrained\n",
    "    },\n",
    "    \"finetuned\": {\n",
    "         \"average_perplexity\": avg_finetuned,\n",
    "         \"std_dev\": std_finetuned\n",
    "    },\n",
    "    \"delta\": avg_finetuned - avg_pretrained,\n",
    "    \"relative_change\": ((avg_finetuned - avg_pretrained) / avg_pretrained * 100) if avg_pretrained != 0 else None\n",
    "}\n",
    "\n",
    "# Save the results dictionary to a JSON file\n",
    "output_json_path = \"perplexity_results.json\"\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b3a4e-30a5-4586-a01c-8f842e05e013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

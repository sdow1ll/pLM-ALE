{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b3c8adc-61a2-4d15-a8f8-588f862f4cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation K662I:\n",
      "  Pretrained -> LLR: -0.7654, log_prob(wt): -3.1817, log_prob(mut): -3.9471\n",
      "  Finetuned  -> LLR: -5.5716, log_prob(wt): -8.6611, log_prob(mut): -14.2327\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "# The protein sequence (concatenated)\n",
    "topA_seq = (\n",
    "\"MGKALVIVESPAKAKTINKYLGSDYVVKSSVGHIRDLPTSGSAAKKSADSTSTKTAKKPK\" \n",
    "    \"KDERGALVNRMGVDPWHNWEAHYEVLPGKEKVVSELKQLAEKADHIYLATDLDREGEAIA\" \n",
    "    \"WHLREVIGGDDARYSRVVFNEITKNAIRQAFNKPGELNIDRVNAQQARRFMDRVVGYMVS\" \n",
    "    \"PLLWKKIARGLSAGRVQSVAVRLVVEREREIKAFVPEEFWEVDASTTTPSGEALALQVTH\" \n",
    "    \"QNDKPFRPVNKEQTQAAVSLLEKARYSVLEREDKPTTSKPGAPFITSTLQQAASTRLGFG\" \n",
    "    \"VKKTMMMAQRLYEAGYITYMRTDSTNLSQDAVNMVRGYISDNFGKKYLPESPNQYASKEN\" \n",
    "    \"SQEAHEAIRPSDVNVMAESLKDMEADAQKLYQLIWRQFVACQMTPAKYDSTTLTVGAGDF\" \n",
    "    \"RLKARGRILRFDGWTKVMPALRKGDEDRILPAVNKGDALTLVELTPAQHFTKPPARFSEA\" \n",
    "    \"SLVKELEKRGIGRPSTYASIISTIQDRGYVRVENRRFYAEKMGEIVTDRLEENFRELMNY\" \n",
    "    \"DFTAQMENSLDQVANHEAEWKAVLDHFFSDFTQQLDKAEKDPEEGGMRPNQMVLTSIDCP\" \n",
    "    \"TCGRKMGIRTASTGVFLGCSGYALPPKERCKTTINLVPENEVLNVLEGEDAETNALRAKR\" \n",
    "    \"RCPKCGTAMDSYLIDPKRKLHVCGNNPTCDGYEIEEGEFRIKGYDGPIVECEKCGSEMHL\" \n",
    "    \"KMGRFGKYMACTNEECKNTRKILRNGEVAPPKEDPVPLPELPCEKSDAYFVLRDGAAGVF\" \n",
    "    \"LAANTFPKSRETRAPLVEELYRFRDRLPEKLRYLADAPQQDPEGNKTMVRFSRKTKQQYV\" \n",
    "    \"SSEKDGKATGWSAFYVDGKWVEGKK\" \n",
    ")\n",
    "\n",
    "spoT_seq = (\"MYLFESLNQLIQTYLPEDQIKRLRQAYLVARDAHEGQTRSSGEPYITHPVAVACILAEMK\"\n",
    "                    \"LDYETLMAALLHDVIEDTPATYQDMEQLFGKSVAELVEGVSKLDKLKFRDKKEAQAENFR\"\n",
    "                    \"KMIMAMVQDIRVILIKLADRTHNMRTLGSLRPDKRRRIARETLEIYSPLAHRLGIHHIKT\"\n",
    "                    \"ELEELGFEALYPNRYRVIKEVVKAARGNRKEMIQKILSEIEGRLQEAGIPCRVSGREKHL\"\n",
    "                    \"YSIYCKMVLKEQRFHSIMDIYAFRVIVNDSDTCYRVLGQMHSLYKPRPGRVKDYIAIPKA\"\n",
    "                    \"NGYQSLHTSMIGPHGVPVEVQIRTEDMDQMAEMGVAAHWAYKEHGETSTTAQIRAQRWMQ\"\n",
    "                    \"SLLELQQSAGSSFEFIESVKSDLFPDEIYVFTPEGRIVELPAGATPVDFAYAVHTDIGHA\"\n",
    "                    \"CVGARVDRQPYPLSQPLTSGQTVEIITAPGARPNAAWLNFVVSSKARAKIRQLLKNLKRD\"\n",
    "                    \"DSVSLGRRLLNHALGGSRKLNEIPQENIQRELDRMKLATLDDLLAEIGLGNAMSVVVAKN\"\n",
    "                    \"LQHGDASIPPATQSHGHLPIKGADGVLITFAKCCRPIPGDPIIAHVSPGKGLVIHHESCR\"\n",
    "                    \"NIRGYQKEPEKFMAVEWDKETAQEFITEIKVEMFNHQGALANLTAAINTTTSNIQSLNTE\"\n",
    "                    \"EKDGRVYSAFIRLTARDRVHLANIMRKIRVMPDVIKVTRNRN\")\n",
    "\n",
    "yeiB_seq = (\"MERNVTLDFVRGVAILGILLLNISAFGLPKAAYLNPAWYGAITPRDAWTWAFLDLIGQVK\"\n",
    "\"FLTLFALLFGAGLQMLLPRGRRWIQSRLTLLVLLGFIHGLLFWDGDILLAYGLVGLICWR\"\n",
    "\"LVRDAPSVKSLFNTGVMLYLVGLGVLLLLGLISDSQTSRAWTPDASAILYEKYWKLHGGV\"\n",
    "\"EAISNRADGVGNSLLALGAQYGWQLAGMMLIGAALMRSGWLKGQFSLRHYRRTGFVLVAI\"\n",
    "\"GVTINLPAIALQWQLDWAYRWCAFLLQMPRELSAPFQAIGYASLFYGFWPQLSRFKLVLA\"\n",
    "\"IACVGRMALTNYLLQTLICTTLFYHLGLFMHFDRLELLAFVIPVWLANILFSVIWLRYFR\"\n",
    "\"QGPVEWLWRQLTLRAAGPAISKTSR\")\n",
    "\n",
    "# List of mutations provided as strings\n",
    "gene_mutation = {\"topA\":\"H33Y\", \"spoT\":\"K662I\", \"yeiB\":\"L143I\"}\n",
    "\n",
    "mutations = [gene_mutation[\"spoT\"]]\n",
    "\n",
    "# Function to parse a mutation string\n",
    "def parse_mutation(mutation_str):\n",
    "    wt = mutation_str[0]  # wild-type residue\n",
    "    mutant = mutation_str[-1]  # mutant residue\n",
    "    pos = int(mutation_str[1:-1])  # position as provided (assumed 1-indexed)\n",
    "    return wt, pos, mutant\n",
    "\n",
    "def compute_mutation_llr(model, tokenizer, sequence, mutation, device):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood ratio for the mutation at a specified position.\n",
    "    \n",
    "    Args:\n",
    "       model: The masked language model (either pretrained or finetuned)\n",
    "       tokenizer: Corresponding tokenizer\n",
    "       sequence: Protein sequence string\n",
    "       mutation: Mutation string, e.g. \"F33I\"\n",
    "       device: torch.device instance\n",
    "       \n",
    "    Returns:\n",
    "       A tuple: (llr, log_prob_wildtype, log_prob_mutant)\n",
    "    \"\"\"\n",
    "    wt, pos, mutant = parse_mutation(mutation)\n",
    "    \n",
    "    # Adjust position from 1-indexed to 0-indexed.\n",
    "    seq_index = pos - 1\n",
    "    \n",
    "    # Optional check: if there are special tokens added, you might need to add an offset.\n",
    "    # For example, if the tokenizer prepends a BOS token:\n",
    "    # extra_offset = 1 if tokenizer.cls_token_id is not None else 0\n",
    "    # token_index = seq_index + extra_offset\n",
    "    # For simplicity, we assume a direct 1:1 mapping.\n",
    "    token_index = seq_index\n",
    "\n",
    "    # Make sure the wild-type residue in the sequence matches what you expect:\n",
    "    if sequence[seq_index] != wt:\n",
    "        print(f\"Warning: at position {pos}, expected wild-type '{wt}', found '{sequence[seq_index]}'.\")\n",
    "\n",
    "    # Tokenize sequence. (This returns a batch of one.)\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a masked input copy.\n",
    "    masked_input_ids = input_ids.clone()\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "\n",
    "    # Replace the token at token_index with the mask token.\n",
    "    # Note: adjust token_index if your tokenizer adds extra tokens.\n",
    "    masked_input_ids[0, token_index] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        # outputs.logits: shape [batch, seq_length, vocab_size]\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get logits for the masked position\n",
    "    masked_logits = logits[0, token_index]\n",
    "    \n",
    "    # Compute probabilities over the vocabulary\n",
    "    probs = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
    "\n",
    "    # Get the token ids for the wild-type and mutant residues\n",
    "    wt_token_id = tokenizer.convert_tokens_to_ids(wt)\n",
    "    mutant_token_id = tokenizer.convert_tokens_to_ids(mutant)\n",
    "    \n",
    "    # Extract probabilities. (A small epsilon is added to avoid log(0).)\n",
    "    eps = 1e-10\n",
    "    prob_wt = probs[wt_token_id].item()\n",
    "    prob_mutant = probs[mutant_token_id].item()\n",
    "    log_prob_wt = math.log(prob_wt + eps)\n",
    "    log_prob_mutant = math.log(prob_mutant + eps)\n",
    "    \n",
    "    # Calculate log-likelihood ratio\n",
    "    llr = log_prob_mutant - log_prob_wt\n",
    "    return llr, log_prob_wt, log_prob_mutant\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Evaluate for each mutation on both models.\n",
    "results = {}\n",
    "for mutation in mutations:\n",
    "    wt, pos, mutant = parse_mutation(mutation)\n",
    "    llr_pretrained, log_wt_pretrained, log_mut_pretrained = compute_mutation_llr(\n",
    "        model_pretrained, tokenizer, spoT_seq, mutation, device\n",
    "    )\n",
    "    llr_finetuned, log_wt_finetuned, log_mut_finetuned = compute_mutation_llr(\n",
    "        model_finetuned, tokenizer, spoT_seq, mutation, device\n",
    "    )\n",
    "    \n",
    "    results[mutation] = {\n",
    "        \"pretrained\": {\n",
    "            \"llr\": llr_pretrained,\n",
    "            \"log_prob_wt\": log_wt_pretrained,\n",
    "            \"log_prob_mutant\": log_mut_pretrained\n",
    "        },\n",
    "        \"finetuned\": {\n",
    "            \"llr\": llr_finetuned,\n",
    "            \"log_prob_wt\": log_wt_finetuned,\n",
    "            \"log_prob_mutant\": log_mut_finetuned\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Print the results.\n",
    "for mutation, scores in results.items():\n",
    "    print(f\"Mutation {mutation}:\")\n",
    "    print(f\"  Pretrained -> LLR: {scores['pretrained']['llr']:.4f}, \"\n",
    "          f\"log_prob(wt): {scores['pretrained']['log_prob_wt']:.4f}, \"\n",
    "          f\"log_prob(mut): {scores['pretrained']['log_prob_mutant']:.4f}\")\n",
    "    print(f\"  Finetuned  -> LLR: {scores['finetuned']['llr']:.4f}, \"\n",
    "          f\"log_prob(wt): {scores['finetuned']['log_prob_wt']:.4f}, \"\n",
    "          f\"log_prob(mut): {scores['finetuned']['log_prob_mutant']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c103729f-d583-4076-a0af-98d566f4dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR results at position 33 (wildtype: F):\n",
      "   Residue  Token_ID Token_Str   Log_Prob        LLR\n",
      "0        A         5         A  -7.392007   5.114078\n",
      "1        C        23         C  -9.025281   3.480805\n",
      "2        D        13         D  -6.565214   5.940872\n",
      "3        E         9         E  -7.239599   5.266487\n",
      "4        F        18         F -12.506085   0.000000\n",
      "5        G         6         G  -0.004772  12.501314\n",
      "6        H        21         H  -8.125887   4.380199\n",
      "7        I        12         I -13.383285  -0.877199\n",
      "8        K        15         K  -8.696727   3.809359\n",
      "9        L         4         L -11.507031   0.999054\n",
      "10       M        20         M -12.534882  -0.028796\n",
      "11       N        17         N  -8.653258   3.852827\n",
      "12       P        14         P -10.269735   2.236351\n",
      "13       Q        16         Q  -7.690768   4.815317\n",
      "14       R        10         R  -8.078082   4.428003\n",
      "15       S         8         S  -8.037423   4.468662\n",
      "16       T        11         T -10.585841   1.920244\n",
      "17       V         7         V  -9.535138   2.970947\n",
      "18       W        22         W -12.653695  -0.147609\n",
      "19       Y        19         Y -11.093699   1.412386\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Example configuration\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "# Load one of your models; for instance, the pretrained model:\n",
    "model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "\n",
    "def compute_llrs_at_position(model, tokenizer, sequence, position, device):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood ratios (LLRs) for all possible amino acid substitutions\n",
    "    at a given position and returns a DataFrame including the token string corresponding\n",
    "    to each candidate token.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: The protein sequence (string).\n",
    "        position: The position to analyze (1-indexed).\n",
    "        device: torch.device to run the model.\n",
    "        \n",
    "    Returns:\n",
    "        A pandas DataFrame with columns:\n",
    "          - 'Residue': the candidate amino acid character,\n",
    "          - 'Token_ID': the candidate token ID,\n",
    "          - 'Token_Str': the decoded token string,\n",
    "          - 'Log_Prob': the log probability of that candidate at the masked position,\n",
    "          - 'LLR': log(prob(candidate)) - log(prob(wild_type))\n",
    "    \"\"\"\n",
    "    # List of the 20 standard amino acids\n",
    "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    \n",
    "    # Adjust position from 1-indexed to 0-indexed.\n",
    "    idx = position - 1\n",
    "    \n",
    "    # Confirm the wild-type residue from the provided sequence.\n",
    "    wild_type = sequence[idx]\n",
    "    \n",
    "    # Tokenize the full sequence (assumes one token per residue)\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of input_ids and replace the target position with the mask token.\n",
    "    masked_input_ids = input_ids.clone()\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token!\")\n",
    "    \n",
    "    masked_input_ids[0, idx] = mask_token_id\n",
    "    \n",
    "    # Run the model in evaluation mode.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        # logits shape: [batch_size, sequence_length, vocab_size]\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Extract the logits for our masked position.\n",
    "    position_logits = logits[0, idx]\n",
    "    # Convert logits to a probability distribution (using softmax)\n",
    "    probs = torch.nn.functional.softmax(position_logits, dim=-1)\n",
    "    \n",
    "    # Small epsilon to prevent taking log(0)\n",
    "    eps = 1e-10\n",
    "\n",
    "    # Get the token ID for the wild-type residue.\n",
    "    wt_token_id = tokenizer.convert_tokens_to_ids(wild_type)\n",
    "    if wt_token_id is None:\n",
    "        raise ValueError(f\"Wildtype residue '{wild_type}' could not be converted to a token ID.\")\n",
    "    prob_wt = probs[wt_token_id].item()\n",
    "    log_prob_wt = math.log(prob_wt + eps)\n",
    "    \n",
    "    # Prepare a list to gather results for each candidate amino acid.\n",
    "    results = []\n",
    "    for aa in amino_acids:\n",
    "        # Get the token id for the candidate amino acid.\n",
    "        aa_token_id = tokenizer.convert_tokens_to_ids(aa)\n",
    "        if aa_token_id is None:\n",
    "            # If this amino acid isn't in the vocabulary, skip it.\n",
    "            continue\n",
    "\n",
    "        # Convert the token id to the decoded token string using decode.\n",
    "        # tokenizer.decode expects a list of token ids.\n",
    "        token_str = tokenizer.decode([aa_token_id]).strip()\n",
    "        \n",
    "        prob_aa = probs[aa_token_id].item()\n",
    "        log_prob_aa = math.log(prob_aa + eps)\n",
    "        # Compute LLR: difference between the candidate's and wild-type's log probabilities.\n",
    "        llr = log_prob_aa - log_prob_wt\n",
    "        \n",
    "        results.append({\n",
    "            \"Residue\": aa,\n",
    "            \"Token_ID\": aa_token_id,\n",
    "            \"Token_Str\": token_str,\n",
    "            \"Log_Prob\": log_prob_aa,\n",
    "            \"LLR\": llr\n",
    "        })\n",
    "    \n",
    "    # Convert the results list into a pandas DataFrame.\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "position_to_check = 33  # for example, position 33 (1-indexed)\n",
    "protein_sequence = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "df_llrs = compute_llrs_at_position(model, tokenizer, protein_sequence, position=position_to_check, device=device)\n",
    "print(f\"LLR results at position {position_to_check} (wildtype: {protein_sequence[position_to_check - 1]}):\")\n",
    "print(df_llrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca89d28a-4ee1-4dcf-b663-c31b3d632b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR results at position 33 (wildtype: F):\n",
      "   Residue  Token_ID Token_Str   Log_Prob        LLR\n",
      "0        A         5         A -10.526323   2.403356\n",
      "1        C        23         C  -9.709476   3.220203\n",
      "2        D        13         D  -8.404804   4.524875\n",
      "3        E         9         E  -6.972060   5.957619\n",
      "4        F        18         F -12.929679   0.000000\n",
      "5        G         6         G  -0.003592  12.926087\n",
      "6        H        21         H -15.173178  -2.243499\n",
      "7        I        12         I -12.218802   0.710876\n",
      "8        K        15         K -11.628019   1.301660\n",
      "9        L         4         L -11.770803   1.158876\n",
      "10       M        20         M -12.972339  -0.042660\n",
      "11       N        17         N -16.749253  -3.819575\n",
      "12       P        14         P -15.240294  -2.310615\n",
      "13       Q        16         Q -14.210980  -1.281301\n",
      "14       R        10         R  -6.639997   6.289682\n",
      "15       S         8         S  -8.042963   4.886716\n",
      "16       T        11         T -13.673189  -0.743511\n",
      "17       V         7         V  -7.322606   5.607073\n",
      "18       W        22         W -11.151234   1.778444\n",
      "19       Y        19         Y -12.567825   0.361853\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use the function:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Choose a position to analyze (e.g., position 33, 1-indexed)\n",
    "position_to_check = 33\n",
    "df_llrs = compute_llrs_at_position(model_finetuned, tokenizer, \n",
    "                                   sequence=(\n",
    "                                       'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "                                       'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "                                       'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "                                       'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    "                                   ),\n",
    "                                   position=position_to_check,\n",
    "                                   device=device)\n",
    "print(f\"LLR results at position {position_to_check} (wildtype: {('MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA')[position_to_check - 1]}):\")\n",
    "print(df_llrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0713d120-33e7-4bf7-89a2-818438a177d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR heatmap values saved to pretrained_spoT_esm2_ecoli_llr_heatmap.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "def compute_llrs_at_position(model, tokenizer, sequence, position, device):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood ratios (LLRs) for all possible amino acid substitutions\n",
    "    at a given position and returns a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: Protein sequence (string).\n",
    "        position: The position to analyze (1-indexed).\n",
    "        device: torch.device to run the model.\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame with columns:\n",
    "          - 'Residue': candidate amino acid,\n",
    "          - 'Token_ID': the candidate token id,\n",
    "          - 'Token_Str': the decoded token string,\n",
    "          - 'Log_Prob': log probability for candidate at masked position,\n",
    "          - 'LLR': log(prob(candidate)) - log(prob(wild_type)).\n",
    "    \"\"\"\n",
    "    # List of 20 standard amino acids.\n",
    "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    \n",
    "    # Adjust position from 1-indexed to 0-indexed.\n",
    "    idx = position - 1\n",
    "    wild_type = sequence[idx]\n",
    "    \n",
    "    # Tokenize the full sequence (assumes one token per residue).\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy and mask the target position.\n",
    "    masked_input_ids = input_ids.clone()\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token!\")\n",
    "    masked_input_ids[0, idx] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch, seq_length, vocab_size]\n",
    "    \n",
    "    # Get logits at the masked position and convert to probabilities.\n",
    "    position_logits = logits[0, idx]\n",
    "    probs = torch.nn.functional.softmax(position_logits, dim=-1)\n",
    "    eps = 1e-10  # small epsilon to avoid log(0)\n",
    "    \n",
    "    # Get wild-type probability and its log probability.\n",
    "    wt_token_id = tokenizer.convert_tokens_to_ids(wild_type)\n",
    "    if wt_token_id is None:\n",
    "        raise ValueError(f\"Wildtype residue '{wild_type}' could not be converted to a token ID.\")\n",
    "    prob_wt = probs[wt_token_id].item()\n",
    "    log_prob_wt = math.log(prob_wt + eps)\n",
    "    \n",
    "    results = []\n",
    "    for aa in amino_acids:\n",
    "        aa_token_id = tokenizer.convert_tokens_to_ids(aa)\n",
    "        if aa_token_id is None:\n",
    "            continue\n",
    "        \n",
    "        # Get the token string from the token id.\n",
    "        token_str = tokenizer.decode([aa_token_id]).strip()\n",
    "        prob_aa = probs[aa_token_id].item()\n",
    "        log_prob_aa = math.log(prob_aa + eps)\n",
    "        llr = log_prob_aa - log_prob_wt\n",
    "        results.append({\n",
    "            \"Residue\": aa,\n",
    "            \"Token_ID\": aa_token_id,\n",
    "            \"Token_Str\": token_str,\n",
    "            \"Log_Prob\": log_prob_aa,\n",
    "            \"LLR\": llr\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "def compute_llr_matrix(model, tokenizer, sequence, device):\n",
    "    \"\"\"\n",
    "    Computes the LLR for all 20 amino acids at every position in the sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: The protein sequence (string).\n",
    "        device: torch.device to run the model.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy array of shape (20, L) containing the LLR values,\n",
    "        and a list of positions (1-indexed) corresponding to the columns.\n",
    "        The rows correspond to the 20 standard amino acids in the order \"ACDEFGHIKLMNPQRSTVWY\".\n",
    "    \"\"\"\n",
    "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    n_positions = len(sequence)\n",
    "    llr_matrix = np.zeros((len(amino_acids), n_positions))\n",
    "    \n",
    "    # Loop over every position in the sequence.\n",
    "    for pos in range(1, n_positions + 1):\n",
    "        df_llrs = compute_llrs_at_position(model, tokenizer, sequence, position=pos, device=device)\n",
    "        # Reorder rows to match the fixed order of amino_acids.\n",
    "        df_llrs = df_llrs.set_index(\"Residue\").loc[amino_acids]\n",
    "        llr_matrix[:, pos - 1] = df_llrs[\"LLR\"].values\n",
    "        \n",
    "    return llr_matrix, list(range(1, n_positions + 1))\n",
    "\n",
    "def plot_llr_heatmap(llr_matrix, sequence, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of LLR values across the entire sequence and optionally saves the figure.\n",
    "    \n",
    "    Args:\n",
    "        llr_matrix: A (20 x L) numpy array where L is the sequence length.\n",
    "        sequence: The protein sequence (string). Used to annotate the x-axis.\n",
    "        save_path: File path to save the figure (e.g. \"llr_heatmap.png\"). If None, the figure isn't saved.\n",
    "    \"\"\"\n",
    "    amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    n_positions = len(sequence)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(max(10, n_positions/5), 8))\n",
    "    cax = ax.imshow(llr_matrix, cmap='viridis', aspect='auto')\n",
    "    \n",
    "    # Set y-axis ticks: one for each amino acid.\n",
    "    ax.set_yticks(np.arange(len(amino_acids)))\n",
    "    ax.set_yticklabels(amino_acids, fontsize=12)\n",
    "    \n",
    "    # Set x-axis ticks (display a subset if the sequence is long)\n",
    "    pos_ticks = np.arange(0, n_positions, max(1, n_positions // 20))\n",
    "    ax.set_xticks(pos_ticks)\n",
    "    ax.set_xticklabels([str(p) for p in (np.array(pos_ticks) + 1)], rotation=45, fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel(\"Position in Sequence\", fontsize=14)\n",
    "    ax.set_ylabel(\"Residue\", fontsize=14)\n",
    "    ax.set_title(\"LLR Heatmap Across Entire Sequence\", fontsize=16)\n",
    "    fig.colorbar(cax, ax=ax, label=\"LLR\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Heatmap figure saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "protein_sequence = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "# The protein sequence (concatenated)\n",
    "topA_seq = (\n",
    "\"MGKALVIVESPAKAKTINKYLGSDYVVKSSVGHIRDLPTSGSAAKKSADSTSTKTAKKPK\" \n",
    "    \"KDERGALVNRMGVDPWHNWEAHYEVLPGKEKVVSELKQLAEKADHIYLATDLDREGEAIA\" \n",
    "    \"WHLREVIGGDDARYSRVVFNEITKNAIRQAFNKPGELNIDRVNAQQARRFMDRVVGYMVS\" \n",
    "    \"PLLWKKIARGLSAGRVQSVAVRLVVEREREIKAFVPEEFWEVDASTTTPSGEALALQVTH\" \n",
    "    \"QNDKPFRPVNKEQTQAAVSLLEKARYSVLEREDKPTTSKPGAPFITSTLQQAASTRLGFG\" \n",
    "    \"VKKTMMMAQRLYEAGYITYMRTDSTNLSQDAVNMVRGYISDNFGKKYLPESPNQYASKEN\" \n",
    "    \"SQEAHEAIRPSDVNVMAESLKDMEADAQKLYQLIWRQFVACQMTPAKYDSTTLTVGAGDF\" \n",
    "    \"RLKARGRILRFDGWTKVMPALRKGDEDRILPAVNKGDALTLVELTPAQHFTKPPARFSEA\" \n",
    "    \"SLVKELEKRGIGRPSTYASIISTIQDRGYVRVENRRFYAEKMGEIVTDRLEENFRELMNY\" \n",
    "    \"DFTAQMENSLDQVANHEAEWKAVLDHFFSDFTQQLDKAEKDPEEGGMRPNQMVLTSIDCP\" \n",
    "    \"TCGRKMGIRTASTGVFLGCSGYALPPKERCKTTINLVPENEVLNVLEGEDAETNALRAKR\" \n",
    "    \"RCPKCGTAMDSYLIDPKRKLHVCGNNPTCDGYEIEEGEFRIKGYDGPIVECEKCGSEMHL\" \n",
    "    \"KMGRFGKYMACTNEECKNTRKILRNGEVAPPKEDPVPLPELPCEKSDAYFVLRDGAAGVF\" \n",
    "    \"LAANTFPKSRETRAPLVEELYRFRDRLPEKLRYLADAPQQDPEGNKTMVRFSRKTKQQYV\" \n",
    "    \"SSEKDGKATGWSAFYVDGKWVEGKK\" \n",
    ")\n",
    "\n",
    "spoT_seq = (\"MYLFESLNQLIQTYLPEDQIKRLRQAYLVARDAHEGQTRSSGEPYITHPVAVACILAEMK\"\n",
    "                    \"LDYETLMAALLHDVIEDTPATYQDMEQLFGKSVAELVEGVSKLDKLKFRDKKEAQAENFR\"\n",
    "                    \"KMIMAMVQDIRVILIKLADRTHNMRTLGSLRPDKRRRIARETLEIYSPLAHRLGIHHIKT\"\n",
    "                    \"ELEELGFEALYPNRYRVIKEVVKAARGNRKEMIQKILSEIEGRLQEAGIPCRVSGREKHL\"\n",
    "                    \"YSIYCKMVLKEQRFHSIMDIYAFRVIVNDSDTCYRVLGQMHSLYKPRPGRVKDYIAIPKA\"\n",
    "                    \"NGYQSLHTSMIGPHGVPVEVQIRTEDMDQMAEMGVAAHWAYKEHGETSTTAQIRAQRWMQ\"\n",
    "                    \"SLLELQQSAGSSFEFIESVKSDLFPDEIYVFTPEGRIVELPAGATPVDFAYAVHTDIGHA\"\n",
    "                    \"CVGARVDRQPYPLSQPLTSGQTVEIITAPGARPNAAWLNFVVSSKARAKIRQLLKNLKRD\"\n",
    "                    \"DSVSLGRRLLNHALGGSRKLNEIPQENIQRELDRMKLATLDDLLAEIGLGNAMSVVVAKN\"\n",
    "                    \"LQHGDASIPPATQSHGHLPIKGADGVLITFAKCCRPIPGDPIIAHVSPGKGLVIHHESCR\"\n",
    "                    \"NIRGYQKEPEKFMAVEWDKETAQEFITEIKVEMFNHQGALANLTAAINTTTSNIQSLNTE\"\n",
    "                    \"EKDGRVYSAFIRLTARDRVHLANIMRKIRVMPDVIKVTRNRN\")\n",
    "\n",
    "yeiB_seq = (\"MERNVTLDFVRGVAILGILLLNISAFGLPKAAYLNPAWYGAITPRDAWTWAFLDLIGQVK\"\n",
    "\"FLTLFALLFGAGLQMLLPRGRRWIQSRLTLLVLLGFIHGLLFWDGDILLAYGLVGLICWR\"\n",
    "\"LVRDAPSVKSLFNTGVMLYLVGLGVLLLLGLISDSQTSRAWTPDASAILYEKYWKLHGGV\"\n",
    "\"EAISNRADGVGNSLLALGAQYGWQLAGMMLIGAALMRSGWLKGQFSLRHYRRTGFVLVAI\"\n",
    "\"GVTINLPAIALQWQLDWAYRWCAFLLQMPRELSAPFQAIGYASLFYGFWPQLSRFKLVLA\"\n",
    "\"IACVGRMALTNYLLQTLICTTLFYHLGLFMHFDRLELLAFVIPVWLANILFSVIWLRYFR\"\n",
    "\"QGPVEWLWRQLTLRAAGPAISKTSR\")\n",
    "\n",
    "# List of mutations provided as strings\n",
    "gene_mutation = {\"topA\":\"H33Y\", \"spoT\":\"K662I\", \"yeiB\":\"L143I\"}\n",
    "\n",
    "mutations = [gene_mutation[\"yeiB\"]]\n",
    "\n",
    "# Example configuration\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "# Load your model; here we use the pretrained model as an example.\n",
    "model = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "    \n",
    "# Compute the full LLR matrix across the sequence.\n",
    "llr_matrix, positions = compute_llr_matrix(model, tokenizer, spoT_seq, device)\n",
    "\n",
    "# Create a DataFrame for the heatmap values.\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "df_heatmap = pd.DataFrame(llr_matrix, index=amino_acids, columns=positions)\n",
    "\n",
    "# Save the heatmap values to a CSV file.\n",
    "csv_filename = \"pretrained_spoT_esm2_ecoli_llr_heatmap.csv\"\n",
    "df_heatmap.to_csv(csv_filename)\n",
    "print(f\"LLR heatmap values saved to {csv_filename}\")\n",
    "\n",
    "# Plot the heatmap and save the figure.\n",
    "#figure_filename = \"pretrained_esm2_ecoli_llr_heatmap.png\"\n",
    "#plot_llr_heatmap(llr_matrix, protein_sequence, save_path=figure_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f459af13-9627-4df9-a1ec-10bd32c6491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR heatmap values saved to finetuned_spoT_esm2_ecoli_llr_heatmap.csv\n"
     ]
    }
   ],
   "source": [
    "# Compute the full LLR matrix across the sequence.\n",
    "llr_matrix, positions = compute_llr_matrix(model_finetuned, tokenizer, spoT_seq, device)\n",
    "\n",
    "# Create a DataFrame for the heatmap values.\n",
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "df_heatmap = pd.DataFrame(llr_matrix, index=amino_acids, columns=positions)\n",
    "\n",
    "# Save the heatmap values to a CSV file.\n",
    "csv_filename = \"finetuned_spoT_esm2_ecoli_llr_heatmap.csv\"\n",
    "df_heatmap.to_csv(csv_filename)\n",
    "print(f\"LLR heatmap values saved to {csv_filename}\")\n",
    "\n",
    "# Plot the heatmap and save the figure.\n",
    "#figure_filename = \"finetuned_dgoa_llr_heatmap.png\"\n",
    "#plot_llr_heatmap(llr_matrix, protein_sequence, save_path=figure_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e806e8b8-b5f4-4210-8c34-a50d8254493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR values and delta for each mutation:\n",
      "\n",
      "Mutation H33Y:\n",
      "  Pretrained LLR: -0.7890436046188132\n",
      "  Finetuned LLR:  2.149750789218812\n",
      "  Delta (Finetuned - Pretrained): 2.9387943938376253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of mutations as strings.\n",
    "gene_mutation = {\"topA\":\"H33Y\", \"spoT\":\"K662I\", \"yeiB\":\"L143I\"}\n",
    "\n",
    "def parse_mutation(mutation_str):\n",
    "    \"\"\"\n",
    "    Parses a mutation string such as \"F33I\" into:\n",
    "      - wt: the wild-type residue (for reference)\n",
    "      - pos: the position (1-indexed)\n",
    "      - mut: the mutant residue.\n",
    "    \"\"\"\n",
    "    wt = mutation_str[0]      # For example, 'F'\n",
    "    mutant = mutation_str[-1]  # For example, 'I'\n",
    "    pos = int(mutation_str[1:-1])\n",
    "    return wt, pos, mutant\n",
    "\n",
    "# Load CSV files.\n",
    "pretrained_csv = \"pretrained_topA_esm2_ecoli_llr_heatmap.csv\"\n",
    "finetuned_csv = \"finetuned_topA_esm2_ecoli_llr_heatmap.csv\"\n",
    "\n",
    "# Read CSV with the first column as the index (the amino acid letters)\n",
    "pretrained_df = pd.read_csv(pretrained_csv, index_col=0)\n",
    "finetuned_df = pd.read_csv(finetuned_csv, index_col=0)\n",
    "\n",
    "# Ensure column headers (positions) are strings.\n",
    "pretrained_df.columns = pretrained_df.columns.astype(str)\n",
    "finetuned_df.columns = finetuned_df.columns.astype(str)\n",
    "\n",
    "print(\"LLR values and delta for each mutation:\\n\")\n",
    "\n",
    "mutations = [gene_mutation[\"topA\"]]\n",
    "\n",
    "for mutation in mutations:\n",
    "    wt, pos, mut = parse_mutation(mutation)\n",
    "    pos_str = str(pos)\n",
    "    \n",
    "    try:\n",
    "        # Look up LLR values for the mutant residue at the given position.\n",
    "        pretrained_llr = pretrained_df.loc[mut, pos_str]\n",
    "        finetuned_llr = finetuned_df.loc[mut, pos_str]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Could not find data for mutation {mutation}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Compute the delta: finetuned LLR minus pretrained LLR.\n",
    "    delta = finetuned_llr - pretrained_llr\n",
    "        \n",
    "    # Print the results.\n",
    "    print(f\"Mutation {mutation}:\")\n",
    "    print(f\"  Pretrained LLR: {pretrained_llr}\")\n",
    "    print(f\"  Finetuned LLR:  {finetuned_llr}\")\n",
    "    print(f\"  Delta (Finetuned - Pretrained): {delta}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1516cc6b-bb53-4f69-8120-2113547784d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutation K662I:\n",
      "  Pretrained LLR: 3.547046870046472\n",
      "  Finetuned LLR:  -4.420149542104095\n",
      "  Delta (Finetuned - Pretrained): -7.967196412150567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mutations = [gene_mutation[\"spoT\"]]\n",
    "\n",
    "for mutation in mutations:\n",
    "    wt, pos, mut = parse_mutation(mutation)\n",
    "    pos_str = str(pos)\n",
    "    \n",
    "    try:\n",
    "        # Look up LLR values for the mutant residue at the given position.\n",
    "        pretrained_llr = pretrained_df.loc[mut, pos_str]\n",
    "        finetuned_llr = finetuned_df.loc[mut, pos_str]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Could not find data for mutation {mutation}: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Compute the delta: finetuned LLR minus pretrained LLR.\n",
    "    delta = finetuned_llr - pretrained_llr\n",
    "        \n",
    "    # Print the results.\n",
    "    print(f\"Mutation {mutation}:\")\n",
    "    print(f\"  Pretrained LLR: {pretrained_llr}\")\n",
    "    print(f\"  Finetuned LLR:  {finetuned_llr}\")\n",
    "    print(f\"  Delta (Finetuned - Pretrained): {delta}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d5e14-5555-4aa8-8ab5-2d438622a641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

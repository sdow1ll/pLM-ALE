{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5da533-bfca-406f-86e0-fe76a37871ce",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34b39b2-55de-4145-9c95-6ccc655f865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e39e9bf-e2ce-43c3-b85e-4520d0e9f729",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b44a2e9-1801-4dc8-88f9-cca76ad77e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapGenerator:\n",
    "    def __init__(self, model=None, tokenizer=None, model_name=None, is_progen=False):\n",
    "        \"\"\"\n",
    "        Initialize the HeatmapGenerator with either:\n",
    "        1. A pre-loaded model and tokenizer\n",
    "        2. A model_name to load from HuggingFace\n",
    "        \n",
    "        Args:\n",
    "            model: A pre-loaded model instance\n",
    "            tokenizer: A pre-loaded tokenizer\n",
    "            model_name: HuggingFace model name (used only if model and tokenizer are None)\n",
    "            is_progen: Set to True when using ProGen models that don't support MLM\n",
    "        \"\"\"\n",
    "        self.is_progen = is_progen\n",
    "        \n",
    "        if model is not None and tokenizer is not None:\n",
    "            # Use provided model and tokenizer\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "        elif model_name is not None:\n",
    "            # Load model and tokenizer from HuggingFace\n",
    "            if \"progen\" in model_name.lower():\n",
    "                self.is_progen = True\n",
    "                from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "            else:\n",
    "                from transformers import EsmForMaskedLM, EsmTokenizer\n",
    "                self.tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "                self.model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "            \n",
    "            # Put model in evaluation mode if it has the method\n",
    "            if hasattr(self.model, 'eval') and callable(self.model.eval):\n",
    "                self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(\"Either provide a model and tokenizer, or a model_name\")\n",
    "\n",
    "        # Print model type for debugging\n",
    "        print(f\"Using model type: {'ProGen' if self.is_progen else 'ESM'}\")\n",
    "\n",
    "    def _get_logits(self, input_ids, position=None):\n",
    "        \"\"\"\n",
    "        Runs the model and extracts logits, handling different model interfaces.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Tensor with input IDs or BatchEncoding\n",
    "            position: Position in the original sequence (for error reporting)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Tensor of logits\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Try different methods of running the model\n",
    "                if hasattr(self.model, '__call__') and callable(self.model.__call__):\n",
    "                    outputs = self.model(input_ids)\n",
    "                elif hasattr(self.model, 'forward') and callable(self.model.forward):\n",
    "                    outputs = self.model.forward(input_ids)\n",
    "                elif hasattr(self.model, 'predict') and callable(self.model.predict):\n",
    "                    outputs = self.model.predict(input_ids)\n",
    "                elif hasattr(self.model, 'infer') and callable(self.model.infer):\n",
    "                    outputs = self.model.infer(input_ids)\n",
    "                else:\n",
    "                    raise ValueError(\"Could not find a suitable method to run the model\")\n",
    "                \n",
    "                # Extract logits from the model output\n",
    "                if isinstance(outputs, torch.Tensor):\n",
    "                    return outputs\n",
    "                elif hasattr(outputs, 'logits'):\n",
    "                    return outputs.logits\n",
    "                elif isinstance(outputs, dict) and 'logits' in outputs:\n",
    "                    return outputs['logits']\n",
    "                elif isinstance(outputs, tuple) and len(outputs) > 0:\n",
    "                    if isinstance(outputs[0], torch.Tensor):\n",
    "                        return outputs[0]\n",
    "                    elif hasattr(outputs[0], 'logits'):\n",
    "                        return outputs[0].logits\n",
    "                \n",
    "                # If we get here, we couldn't identify the logits\n",
    "                raise ValueError(f\"Could not extract logits from model output: {type(outputs)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error running model at position {position}: {str(e)}\")\n",
    "\n",
    "    def llrData(self, protein_sequence, start_pos=1, end_pos=None):\n",
    "        \"\"\"\n",
    "        Computes log likelihood ratio data for mutations at each position.\n",
    "        Modified to work with both ESM (MLM) and ProGen (CLM) models.\n",
    "        \n",
    "        Args:\n",
    "            protein_sequence: The protein sequence to analyze\n",
    "            start_pos: The starting position (1-indexed) to analyze\n",
    "            end_pos: The ending position (1-indexed) to analyze\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with LLR values for each amino acid at each position\n",
    "        \"\"\"\n",
    "        # Calculate sequence length\n",
    "        sequence_length = len(protein_sequence)\n",
    "    \n",
    "        # Adjust end_pos if not specified or if it exceeds the actual sequence length\n",
    "        if end_pos is None:\n",
    "            end_pos = sequence_length\n",
    "        end_pos = min(end_pos, sequence_length)\n",
    "    \n",
    "        # List of amino acids\n",
    "        amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    \n",
    "        # Initialize heatmap array: 20 rows (AAs) x (end_pos - start_pos + 1) columns\n",
    "        heatmap = np.zeros((20, end_pos - start_pos + 1))\n",
    "    \n",
    "        # Process each position\n",
    "        for position in range(start_pos, end_pos + 1):\n",
    "            try:\n",
    "                if not self.is_progen:\n",
    "                    # ESM uses masked language modeling\n",
    "                    # Tokenize the sequence\n",
    "                    tokens = self.tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "                    input_ids = tokens[\"input_ids\"]\n",
    "                    \n",
    "                    # Clone the input_ids to mask one position\n",
    "                    masked_input_ids = input_ids.clone()\n",
    "                    \n",
    "                    # ESM position offset: Adjust if needed\n",
    "                    token_position = position\n",
    "                    \n",
    "                    # Get the token ID for the mask token\n",
    "                    mask_token_id = self.tokenizer.mask_token_id\n",
    "                    if mask_token_id is None:\n",
    "                        raise ValueError(\"Tokenizer doesn't have a mask token\")\n",
    "                    \n",
    "                    # Mask the target position\n",
    "                    masked_input_ids[0, token_position] = mask_token_id\n",
    "                    \n",
    "                    # Get logits from the model for masked position\n",
    "                    logits = self._get_logits(masked_input_ids, position=position)\n",
    "                    \n",
    "                    # Get logits for the masked position\n",
    "                    masked_position_logits = logits[0, token_position]\n",
    "                    \n",
    "                    # Get the wildtype residue ID from input_ids\n",
    "                    wt_residue_id = input_ids[0, token_position].item()\n",
    "                    \n",
    "                else:  # ProGen or other causal models\n",
    "                    # Use causal language modeling (predict next token)\n",
    "                    \n",
    "                    # For ProGen, we need special handling for the first position\n",
    "                    if position <= 1:\n",
    "                        # For the first position, use a single letter to avoid empty tensor\n",
    "                        # This is just to get the vocabulary distribution\n",
    "                        prefix = \"A\"  # Use any amino acid here\n",
    "                    else:\n",
    "                        # Extract the sequence up to the position we want to predict\n",
    "                        prefix = protein_sequence[:position-1]  # -1 for 0-indexing\n",
    "                    \n",
    "                    # Tokenize just the prefix\n",
    "                    prefix_tokens = self.tokenizer(prefix, return_tensors=\"pt\")\n",
    "                    \n",
    "                    # Debug output to see what's happening\n",
    "                    #print(f\"Position {position}: Prefix '{prefix}', Prefix shape after tokenization: {prefix_tokens['input_ids'].shape}\")\n",
    "                    \n",
    "                    # Run the model to get the next token prediction\n",
    "                    logits = self._get_logits(prefix_tokens, position=position)\n",
    "                    \n",
    "                    # Get the last position logits (predicting the next token)\n",
    "                    if logits.size(1) > 0:\n",
    "                        masked_position_logits = logits[0, -1]\n",
    "                    else:\n",
    "                        # If we still get an empty tensor, use a default distribution\n",
    "                        # For safety, create an array of zeros with the correct vocab size\n",
    "                        vocab_size = len(self.tokenizer.get_vocab())\n",
    "                        masked_position_logits = torch.zeros(vocab_size, device=logits.device)\n",
    "                        print(f\"Warning: Empty logits at position {position}, using zeros with shape {masked_position_logits.shape}\")\n",
    "                    \n",
    "                    # For ProGen, get the wildtype residue directly from the sequence\n",
    "                    wt_residue = protein_sequence[position-1]  # 0-indexed\n",
    "                    # Map the character to a token ID\n",
    "                    wt_residue_id = None\n",
    "                    \n",
    "                    # Try different ways to get the token ID\n",
    "                    try:\n",
    "                        # Method 1: Direct conversion\n",
    "                        wt_residue_id = self.tokenizer.convert_tokens_to_ids(wt_residue)\n",
    "                        \n",
    "                        # Method 2: Try as a list\n",
    "                        if wt_residue_id is None or wt_residue_id == 0:\n",
    "                            wt_residue_id = self.tokenizer.convert_tokens_to_ids([wt_residue])[0]\n",
    "                            \n",
    "                        # Method 3: Use encode\n",
    "                        if wt_residue_id is None or wt_residue_id == 0:\n",
    "                            encoded = self.tokenizer.encode(wt_residue, add_special_tokens=False)\n",
    "                            if len(encoded) > 0:\n",
    "                                wt_residue_id = encoded[0]\n",
    "                        \n",
    "                        print(f\"Wildtype residue '{wt_residue}' at position {position} mapped to token ID {wt_residue_id}\")\n",
    "                    except Exception as e:\n",
    "                        # If all methods fail, print a warning\n",
    "                        print(f\"Warning: Could not convert '{wt_residue}' to a token ID: {str(e)}\")\n",
    "                \n",
    "                # Convert to probabilities and log probabilities\n",
    "                probs = torch.nn.functional.softmax(masked_position_logits, dim=0)\n",
    "                log_probs = torch.log(probs)\n",
    "    \n",
    "                # Set up wildtype log probability\n",
    "                log_prob_wt = 0.0\n",
    "                if wt_residue_id is not None:\n",
    "                    try:\n",
    "                        log_prob_wt = log_probs[wt_residue_id].item()\n",
    "                    except (IndexError, TypeError) as e:\n",
    "                        # If the ID is out of range, use 0.0\n",
    "                        print(f\"Warning: Token ID {wt_residue_id} for wildtype at position {position} is out of range: {str(e)}\")\n",
    "                        log_prob_wt = 0.0\n",
    "    \n",
    "                # Compute LLRs for all 20 amino acids\n",
    "                for i, aa in enumerate(amino_acids):\n",
    "                    try:\n",
    "                        # Get token ID for the amino acid - try multiple methods\n",
    "                        aa_id = None\n",
    "                        \n",
    "                        # Method 1: Direct conversion\n",
    "                        aa_id = self.tokenizer.convert_tokens_to_ids(aa)\n",
    "                        \n",
    "                        # Method 2: Try as a list\n",
    "                        if aa_id is None or aa_id == 0:\n",
    "                            aa_id = self.tokenizer.convert_tokens_to_ids([aa])[0]\n",
    "                            \n",
    "                        # Method 3: Use encode\n",
    "                        if aa_id is None or aa_id == 0:\n",
    "                            # Skip special tokens when encoding\n",
    "                            encoded = self.tokenizer.encode(aa, add_special_tokens=False)\n",
    "                            if len(encoded) > 0:\n",
    "                                aa_id = encoded[0]\n",
    "                        \n",
    "                        # If token ID exists, compute log likelihood ratio\n",
    "                        if aa_id is not None and aa_id > 0:\n",
    "                            try:\n",
    "                                log_prob_mut = log_probs[aa_id].item()\n",
    "                                heatmap[i, position - start_pos] = log_prob_mut - log_prob_wt\n",
    "                            except IndexError as e:\n",
    "                                # If the ID is out of range, use 0.0\n",
    "                                print(f\"Warning: Token ID {aa_id} for '{aa}' is out of range: {str(e)}\")\n",
    "                                heatmap[i, position - start_pos] = 0.0\n",
    "                        else:\n",
    "                            # Skip tokens not in the vocabulary\n",
    "                            heatmap[i, position - start_pos] = 0.0\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing amino acid '{aa}' at position {position}: {str(e)}\")\n",
    "                        heatmap[i, position - start_pos] = 0.0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing position {position}: {str(e)}\")\n",
    "                # Fill with zeros for this position\n",
    "                heatmap[:, position - start_pos] = 0\n",
    "    \n",
    "        # Create a DataFrame for readability\n",
    "        columns = [f\"{pos}\" for pos in range(start_pos, end_pos + 1)]\n",
    "        df = pd.DataFrame(heatmap, index=amino_acids, columns=columns)\n",
    "        return df\n",
    "\n",
    "        \n",
    "\n",
    "    def generate_heatmap(self, protein_sequence, start_pos=1, end_pos=None, figsize=(10, 5), \n",
    "                        cmap=\"viridis\", tick_interval=5, title=None):\n",
    "        \"\"\"\n",
    "        Plots the heatmap of log_prob_mutant - log_prob_wildtype.\n",
    "        \n",
    "        Args:\n",
    "            protein_sequence: The protein sequence to analyze\n",
    "            start_pos: The starting position (1-indexed) to analyze\n",
    "            end_pos: The ending position (1-indexed) to analyze\n",
    "            figsize: Figure size (width, height) in inches\n",
    "            cmap: Colormap to use for the heatmap\n",
    "            tick_interval: Label x-axis ticks at this interval\n",
    "            title: Custom title for the plot (if None, uses default)\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib figure object\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        heatmap = df.values  # shape [20, num_positions]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(heatmap, cmap=cmap, aspect=\"auto\")\n",
    "\n",
    "        ax.set_xticks(range(heatmap.shape[1]))  # Set ticks for every position\n",
    "        ax.set_xticklabels(\n",
    "            [df.columns[i] if i % tick_interval == 0 else \"\" for i in range(len(df.columns))],\n",
    "            rotation=90,\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "        ax.set_yticks(range(20))\n",
    "        ax.set_yticklabels(df.index)\n",
    "\n",
    "        ax.set_xlabel(\"Position in Protein Sequence\")\n",
    "        ax.set_ylabel(\"Amino Acid Mutations\")\n",
    "        \n",
    "        model_type = \"ProGen2\" if self.is_progen else \"ESM\"\n",
    "        if title is None:\n",
    "            title = f\"Predicted Effects of Mutations ({model_type} Model)\"\n",
    "        ax.set_title(title)\n",
    "\n",
    "        cbar = fig.colorbar(cax, ax=ax)\n",
    "        cbar.set_label(\"Log Likelihood Ratio (mutant vs. wild-type)\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def save_heatmap_data(self, protein_sequence, filename, start_pos=1, end_pos=None):\n",
    "        \"\"\"\n",
    "        Compute the LLR data and save it to a CSV file\n",
    "        \n",
    "        Args:\n",
    "            protein_sequence: The protein sequence to analyze\n",
    "            filename: Path to save the CSV file\n",
    "            start_pos: The starting position (1-indexed) to analyze\n",
    "            end_pos: The ending position (1-indexed) to analyze\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the LLR data\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        df.to_csv(filename)\n",
    "        return df\n",
    "        \n",
    "def parse_mutation(mutation_str):\n",
    "    # Find the position (number) in the mutation string\n",
    "    i = 0\n",
    "    while i < len(mutation_str) and not mutation_str[i].isdigit():\n",
    "        i += 1\n",
    "    j = i\n",
    "    while j < len(mutation_str) and mutation_str[j].isdigit():\n",
    "        j += 1\n",
    "    \n",
    "    original = mutation_str[:i]\n",
    "    position = int(mutation_str[i:j])\n",
    "    new = mutation_str[j:]\n",
    "    \n",
    "    return (original, position, new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d18850-7d14-4b28-8fa1-fc0f8838f1cc",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "afc0e977-9696-47a0-9d3b-1c77c3533317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForMaskedLM(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 640, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 640, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-29): 30 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=640, out_features=2560, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): EsmLMHead(\n",
       "          (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (decoder): Linear(in_features=640, out_features=33, bias=False)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): EsmLMHead(\n",
       "            (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (decoder): Linear(in_features=640, out_features=33, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Pretrained model\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "\n",
    "# Finetuned model\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_pretrained.to(device).eval()\n",
    "model_finetuned.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d8a74-cd21-4dca-a561-81690ca4b86c",
   "metadata": {},
   "source": [
    "# Prepare DgoA mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "590e8634-ebf6-40d6-8771-b8f5aa681d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DgoA_seq = ('MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "            'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "            'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "            'RAGQSVERTAQQAAAFVKAYREAVQ')\n",
    "\n",
    "mutations = ['F33I','D58N','A75V','Q72H','V85A','V154F','Y180F']\n",
    "\n",
    "Dgoa_mut_seqs = []\n",
    "\n",
    "# Convert all mutation strings into tuples\n",
    "mutation_tuples = [parse_mutation(m) for m in mutations]\n",
    "#print(mutation_tuples)\n",
    "\n",
    "# Mutate DgoA sequence\n",
    "for orig_aa, pos, new_aa in mutation_tuples:\n",
    "    if DgoA_seq[pos - 1] != orig_aa:\n",
    "        print(f\"Warning: expected {orig_aa} at position {pos}, but found {DgoA_seq[pos - 1]}\")\n",
    "    # Apply mutation\n",
    "    mutated_seq = DgoA_seq[:pos - 1] + new_aa + DgoA_seq[pos:]\n",
    "    Dgoa_mut_seqs.append(mutated_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e035b0bd-6926-43ce-95bb-f30fbed74c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class HeatmapGenerator:\n",
    "    def __init__(self, model_pretrained, model_finetuned, tokenizer):\n",
    "        self.model_pre = model_pretrained\n",
    "        self.model_fine = model_finetuned\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model_pretrained.parameters()).device\n",
    "\n",
    "    def compute_llr(self, sequence, start_pos, end_pos):\n",
    "        encoded = self.tokenizer(sequence, return_tensors=\"pt\")\n",
    "        input_ids = encoded.input_ids.to(self.device)\n",
    "        attention_mask = encoded.attention_mask.to(self.device)\n",
    "\n",
    "        llr_scores = []\n",
    "\n",
    "        for i in range(start_pos - 1, end_pos):\n",
    "            masked_input = input_ids.clone()\n",
    "            masked_input[0, i] = self.tokenizer.mask_token_id\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_pre = self.model_pre(masked_input, attention_mask=attention_mask).logits\n",
    "                logits_fine = self.model_fine(masked_input, attention_mask=attention_mask).logits\n",
    "\n",
    "            true_token_id = input_ids[0, i]\n",
    "            token = tokenizer.convert_ids_to_tokens(true_token_id.item())\n",
    "            print(\"True token at position\", i, \":\", token)\n",
    "                \n",
    "            log_probs_pre = torch.log_softmax(logits_pre[0, i], dim=-1)\n",
    "            log_probs_fine = torch.log_softmax(logits_fine[0, i], dim=-1)\n",
    "            \n",
    "            llr = (log_probs_fine[true_token_id] - log_probs_pre[true_token_id]).item()\n",
    "            print(\"log_probs_pre=\\n\",log_probs_pre[true_token_id].item())\n",
    "            print(\"log_probs_fine=\\n\", log_probs_fine[true_token_id].item())\n",
    "            print(\"\\n\")\n",
    "            llr_scores.append(llr)\n",
    "\n",
    "        return llr_scores\n",
    "\n",
    "    def generate_heatmap(self, protein_sequence, start_pos, end_pos, title=\"\", figsize=(12, 4), tick_interval=10):\n",
    "        llr_values = self.compute_llr(protein_sequence, start_pos, end_pos)\n",
    "        positions = list(range(start_pos, end_pos + 1))\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.barplot(x=positions, y=llr_values, palette=\"coolwarm\")\n",
    "        plt.axhline(0, color='black', linestyle='--')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Residue Position\")\n",
    "        plt.ylabel(\"LLR (log-prob fine - log-prob pre)\")\n",
    "        plt.xticks(ticks=positions[::tick_interval])\n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "    def save_heatmap_data(self, protein_sequence, start_pos, end_pos, filename=\"llr_data.csv\"):\n",
    "        import pandas as pd\n",
    "        llr_values = self.compute_llr(protein_sequence, start_pos, end_pos)\n",
    "        df = pd.DataFrame({\n",
    "            \"position\": list(range(start_pos, end_pos + 1)),\n",
    "            \"llr\": llr_values\n",
    "        })\n",
    "        df.to_csv(filename, index=False)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "498b2d10-506e-410d-9ae9-0981a81d5705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Generate and plot\\nfig = heatmap_generator.generate_heatmap(\\n    protein_sequence=DgoA_seq,\\n    start_pos=1,\\n    end_pos=len(DgoA_seq),\\n    title=\"LLR Heatmap: Fine-tuned vs Pretrained ESM2\",\\n    figsize=(15, 5),\\n    tick_interval=20\\n)\\n\\n# Save LLR data\\ndf = heatmap_generator.save_heatmap_data(\\n    protein_sequence=DgoA_seq,\\n    start_pos=1,\\n    end_pos=len(DgoA_seq),\\n    filename=\"DeltaLLR_esm2_dgoa.csv\"\\n)\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap_generator = HeatmapGenerator(\n",
    "    model_pretrained=model_pretrained,\n",
    "    model_finetuned=model_finetuned,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "'''\n",
    "# Generate and plot\n",
    "fig = heatmap_generator.generate_heatmap(\n",
    "    protein_sequence=DgoA_seq,\n",
    "    start_pos=1,\n",
    "    end_pos=len(DgoA_seq),\n",
    "    title=\"LLR Heatmap: Fine-tuned vs Pretrained ESM2\",\n",
    "    figsize=(15, 5),\n",
    "    tick_interval=20\n",
    ")\n",
    "\n",
    "# Save LLR data\n",
    "df = heatmap_generator.save_heatmap_data(\n",
    "    protein_sequence=DgoA_seq,\n",
    "    start_pos=1,\n",
    "    end_pos=len(DgoA_seq),\n",
    "    filename=\"DeltaLLR_esm2_dgoa.csv\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5c58bb0-740a-426c-b015-1fca80bddbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True token at position -1 : <eos>\n",
      "log_probs_pre=\n",
      " -1.1920928244535389e-07\n",
      "log_probs_fine=\n",
      " -0.005357787944376469\n",
      "\n",
      "\n",
      "True token at position 0 : <cls>\n",
      "log_probs_pre=\n",
      " 0.0\n",
      "log_probs_fine=\n",
      " 0.0\n",
      "\n",
      "\n",
      "True token at position 1 : M\n",
      "log_probs_pre=\n",
      " -0.0015460216673091054\n",
      "log_probs_fine=\n",
      " -0.0012446045875549316\n",
      "\n",
      "\n",
      "True token at position 2 : Q\n",
      "log_probs_pre=\n",
      " -2.5927248001098633\n",
      "log_probs_fine=\n",
      " -0.004087665118277073\n",
      "\n",
      "\n",
      "True token at position 3 : W\n",
      "log_probs_pre=\n",
      " -3.4336624145507812\n",
      "log_probs_fine=\n",
      " -0.004165069665759802\n",
      "\n",
      "\n",
      "True token at position 4 : Q\n",
      "log_probs_pre=\n",
      " -2.5638318061828613\n",
      "log_probs_fine=\n",
      " -0.0033945576287806034\n",
      "\n",
      "\n",
      "True token at position 5 : T\n",
      "log_probs_pre=\n",
      " -2.8189826011657715\n",
      "log_probs_fine=\n",
      " -0.046371106058359146\n",
      "\n",
      "\n",
      "True token at position 6 : K\n",
      "log_probs_pre=\n",
      " -1.815788984298706\n",
      "log_probs_fine=\n",
      " -0.004978284705430269\n",
      "\n",
      "\n",
      "True token at position 7 : L\n",
      "log_probs_pre=\n",
      " -2.4636898040771484\n",
      "log_probs_fine=\n",
      " -0.0068692718632519245\n",
      "\n",
      "\n",
      "True token at position 8 : P\n",
      "log_probs_pre=\n",
      " -0.2631039619445801\n",
      "log_probs_fine=\n",
      " -0.00609468063339591\n",
      "\n",
      "\n",
      "True token at position 9 : L\n",
      "log_probs_pre=\n",
      " -0.9234394431114197\n",
      "log_probs_fine=\n",
      " -0.00018285033002030104\n",
      "\n",
      "\n",
      "True token at position 10 : I\n",
      "log_probs_pre=\n",
      " -0.7896395325660706\n",
      "log_probs_fine=\n",
      " -0.005694476887583733\n",
      "\n",
      "\n",
      "True token at position 11 : A\n",
      "log_probs_pre=\n",
      " -0.22047504782676697\n",
      "log_probs_fine=\n",
      " -0.0039001840632408857\n",
      "\n",
      "\n",
      "True token at position 12 : I\n",
      "log_probs_pre=\n",
      " -0.06248435005545616\n",
      "log_probs_fine=\n",
      " -0.0003333727945573628\n",
      "\n",
      "\n",
      "True token at position 13 : L\n",
      "log_probs_pre=\n",
      " -0.37673473358154297\n",
      "log_probs_fine=\n",
      " -0.0006194579764269292\n",
      "\n",
      "\n",
      "True token at position 14 : R\n",
      "log_probs_pre=\n",
      " -0.000446696620201692\n",
      "log_probs_fine=\n",
      " -0.0009389282786287367\n",
      "\n",
      "\n",
      "True token at position 15 : G\n",
      "log_probs_pre=\n",
      " -0.25750023126602173\n",
      "log_probs_fine=\n",
      " -0.004182639066129923\n",
      "\n",
      "\n",
      "True token at position 16 : I\n",
      "log_probs_pre=\n",
      " -0.8828946352005005\n",
      "log_probs_fine=\n",
      " -0.0003766304289456457\n",
      "\n",
      "\n",
      "True token at position 17 : T\n",
      "log_probs_pre=\n",
      " -1.6518439054489136\n",
      "log_probs_fine=\n",
      " -1.2006940841674805\n",
      "\n",
      "\n",
      "True token at position 18 : P\n",
      "log_probs_pre=\n",
      " -0.8822022676467896\n",
      "log_probs_fine=\n",
      " -0.003509911010041833\n",
      "\n",
      "\n",
      "True token at position 19 : D\n",
      "log_probs_pre=\n",
      " -1.5188926458358765\n",
      "log_probs_fine=\n",
      " -0.004565767012536526\n",
      "\n",
      "\n",
      "True token at position 20 : E\n",
      "log_probs_pre=\n",
      " -0.6130849719047546\n",
      "log_probs_fine=\n",
      " -0.0037443782202899456\n",
      "\n",
      "\n",
      "True token at position 21 : A\n",
      "log_probs_pre=\n",
      " -0.34395831823349\n",
      "log_probs_fine=\n",
      " -0.005559577606618404\n",
      "\n",
      "\n",
      "True token at position 22 : L\n",
      "log_probs_pre=\n",
      " -1.0365272760391235\n",
      "log_probs_fine=\n",
      " -0.001167092937976122\n",
      "\n",
      "\n",
      "True token at position 23 : A\n",
      "log_probs_pre=\n",
      " -1.2058377265930176\n",
      "log_probs_fine=\n",
      " -0.01674700528383255\n",
      "\n",
      "\n",
      "True token at position 24 : H\n",
      "log_probs_pre=\n",
      " -5.563421726226807\n",
      "log_probs_fine=\n",
      " -0.0014643670292571187\n",
      "\n",
      "\n",
      "True token at position 25 : V\n",
      "log_probs_pre=\n",
      " -2.3637518882751465\n",
      "log_probs_fine=\n",
      " -0.0016550427535548806\n",
      "\n",
      "\n",
      "True token at position 26 : G\n",
      "log_probs_pre=\n",
      " -3.3777031898498535\n",
      "log_probs_fine=\n",
      " -0.00271428469568491\n",
      "\n",
      "\n",
      "True token at position 27 : A\n",
      "log_probs_pre=\n",
      " -0.34742701053619385\n",
      "log_probs_fine=\n",
      " -0.0058381278067827225\n",
      "\n",
      "\n",
      "True token at position 28 : V\n",
      "log_probs_pre=\n",
      " -1.2434442043304443\n",
      "log_probs_fine=\n",
      " -0.0015676839975640178\n",
      "\n",
      "\n",
      "True token at position 29 : I\n",
      "log_probs_pre=\n",
      " -2.04860520362854\n",
      "log_probs_fine=\n",
      " -0.0011650687083601952\n",
      "\n",
      "\n",
      "True token at position 30 : D\n",
      "log_probs_pre=\n",
      " -1.0212465524673462\n",
      "log_probs_fine=\n",
      " -0.0010526598198339343\n",
      "\n",
      "\n",
      "True token at position 31 : A\n",
      "log_probs_pre=\n",
      " -0.7933388352394104\n",
      "log_probs_fine=\n",
      " -0.01806824654340744\n",
      "\n",
      "\n",
      "True token at position 32 : G\n",
      "log_probs_pre=\n",
      " -0.004771632142364979\n",
      "log_probs_fine=\n",
      " -0.0035916364286094904\n",
      "\n",
      "\n",
      "True token at position 33 : F\n",
      "log_probs_pre=\n",
      " -2.160510301589966\n",
      "log_probs_fine=\n",
      " -0.3129328787326813\n",
      "\n",
      "\n",
      "True token at position 34 : D\n",
      "log_probs_pre=\n",
      " -2.485961437225342\n",
      "log_probs_fine=\n",
      " -0.007108516059815884\n",
      "\n",
      "\n",
      "True token at position 35 : A\n",
      "log_probs_pre=\n",
      " -1.7204698324203491\n",
      "log_probs_fine=\n",
      " -0.019924849271774292\n",
      "\n",
      "\n",
      "True token at position 36 : V\n",
      "log_probs_pre=\n",
      " -1.1007626056671143\n",
      "log_probs_fine=\n",
      " -0.002185577293857932\n",
      "\n",
      "\n",
      "True token at position 37 : E\n",
      "log_probs_pre=\n",
      " -0.00044645831803791225\n",
      "log_probs_fine=\n",
      " -0.00010442188795423135\n",
      "\n",
      "\n",
      "True token at position 38 : I\n",
      "log_probs_pre=\n",
      " -0.9973047375679016\n",
      "log_probs_fine=\n",
      " -0.0010785006452351809\n",
      "\n",
      "\n",
      "True token at position 39 : P\n",
      "log_probs_pre=\n",
      " -0.33630064129829407\n",
      "log_probs_fine=\n",
      " -0.006885610055178404\n",
      "\n",
      "\n",
      "True token at position 40 : L\n",
      "log_probs_pre=\n",
      " -0.20492662489414215\n",
      "log_probs_fine=\n",
      " -0.0021467991173267365\n",
      "\n",
      "\n",
      "True token at position 41 : N\n",
      "log_probs_pre=\n",
      " -0.09015130251646042\n",
      "log_probs_fine=\n",
      " -0.0008434075862169266\n",
      "\n",
      "\n",
      "True token at position 42 : S\n",
      "log_probs_pre=\n",
      " -0.21852914988994598\n",
      "log_probs_fine=\n",
      " -0.0028618115466088057\n",
      "\n",
      "\n",
      "True token at position 43 : P\n",
      "log_probs_pre=\n",
      " -0.16748405992984772\n",
      "log_probs_fine=\n",
      " -0.006712033413350582\n",
      "\n",
      "\n",
      "True token at position 44 : Q\n",
      "log_probs_pre=\n",
      " -2.401291847229004\n",
      "log_probs_fine=\n",
      " -0.006758331321179867\n",
      "\n",
      "\n",
      "True token at position 45 : W\n",
      "log_probs_pre=\n",
      " -4.132401466369629\n",
      "log_probs_fine=\n",
      " -0.00026603974401950836\n",
      "\n",
      "\n",
      "True token at position 46 : E\n",
      "log_probs_pre=\n",
      " -1.011765956878662\n",
      "log_probs_fine=\n",
      " -0.005807787179946899\n",
      "\n",
      "\n",
      "True token at position 47 : Q\n",
      "log_probs_pre=\n",
      " -1.6661754846572876\n",
      "log_probs_fine=\n",
      " -0.0054173097014427185\n",
      "\n",
      "\n",
      "True token at position 48 : S\n",
      "log_probs_pre=\n",
      " -0.7247971296310425\n",
      "log_probs_fine=\n",
      " -0.014675643295049667\n",
      "\n",
      "\n",
      "True token at position 49 : I\n",
      "log_probs_pre=\n",
      " -0.6068129539489746\n",
      "log_probs_fine=\n",
      " -0.00101062236353755\n",
      "\n",
      "\n",
      "True token at position 50 : P\n",
      "log_probs_pre=\n",
      " -5.742485046386719\n",
      "log_probs_fine=\n",
      " -0.012747352011501789\n",
      "\n",
      "\n",
      "True token at position 51 : A\n",
      "log_probs_pre=\n",
      " -0.6929182410240173\n",
      "log_probs_fine=\n",
      " -0.010923351161181927\n",
      "\n",
      "\n",
      "True token at position 52 : I\n",
      "log_probs_pre=\n",
      " -0.9589237570762634\n",
      "log_probs_fine=\n",
      " -0.0033890926279127598\n",
      "\n",
      "\n",
      "True token at position 53 : V\n",
      "log_probs_pre=\n",
      " -1.3018434047698975\n",
      "log_probs_fine=\n",
      " -0.0031343402806669474\n",
      "\n",
      "\n",
      "True token at position 54 : D\n",
      "log_probs_pre=\n",
      " -2.3609280586242676\n",
      "log_probs_fine=\n",
      " -0.0023762343917042017\n",
      "\n",
      "\n",
      "True token at position 55 : A\n",
      "log_probs_pre=\n",
      " -0.8601856827735901\n",
      "log_probs_fine=\n",
      " -0.014604922384023666\n",
      "\n",
      "\n",
      "True token at position 56 : Y\n",
      "log_probs_pre=\n",
      " -0.7253760099411011\n",
      "log_probs_fine=\n",
      " -0.001762266969308257\n",
      "\n",
      "\n",
      "True token at position 57 : G\n",
      "log_probs_pre=\n",
      " -0.22149863839149475\n",
      "log_probs_fine=\n",
      " -0.002866447437554598\n",
      "\n",
      "\n",
      "True token at position 58 : D\n",
      "log_probs_pre=\n",
      " -0.5805288553237915\n",
      "log_probs_fine=\n",
      " -0.21782241761684418\n",
      "\n",
      "\n",
      "True token at position 59 : K\n",
      "log_probs_pre=\n",
      " -1.6039091348648071\n",
      "log_probs_fine=\n",
      " -0.003870378714054823\n",
      "\n",
      "\n",
      "True token at position 60 : A\n",
      "log_probs_pre=\n",
      " -0.6330380439758301\n",
      "log_probs_fine=\n",
      " -0.006478853523731232\n",
      "\n",
      "\n",
      "True token at position 61 : L\n",
      "log_probs_pre=\n",
      " -0.9879871606826782\n",
      "log_probs_fine=\n",
      " -0.005816439166665077\n",
      "\n",
      "\n",
      "True token at position 62 : I\n",
      "log_probs_pre=\n",
      " -0.6081094145774841\n",
      "log_probs_fine=\n",
      " -0.000753357307985425\n",
      "\n",
      "\n",
      "True token at position 63 : G\n",
      "log_probs_pre=\n",
      " -0.03448616340756416\n",
      "log_probs_fine=\n",
      " -0.00827645044773817\n",
      "\n",
      "\n",
      "True token at position 64 : A\n",
      "log_probs_pre=\n",
      " -0.054745983332395554\n",
      "log_probs_fine=\n",
      " -0.0051250034011900425\n",
      "\n",
      "\n",
      "True token at position 65 : G\n",
      "log_probs_pre=\n",
      " -0.0017362057697027922\n",
      "log_probs_fine=\n",
      " -0.0013434203574433923\n",
      "\n",
      "\n",
      "True token at position 66 : T\n",
      "log_probs_pre=\n",
      " -0.035822488367557526\n",
      "log_probs_fine=\n",
      " -0.0019276624079793692\n",
      "\n",
      "\n",
      "True token at position 67 : V\n",
      "log_probs_pre=\n",
      " -0.06933736056089401\n",
      "log_probs_fine=\n",
      " -0.009597588330507278\n",
      "\n",
      "\n",
      "True token at position 68 : L\n",
      "log_probs_pre=\n",
      " -0.4371407926082611\n",
      "log_probs_fine=\n",
      " -0.00047958316281437874\n",
      "\n",
      "\n",
      "True token at position 69 : K\n",
      "log_probs_pre=\n",
      " -2.5808122158050537\n",
      "log_probs_fine=\n",
      " -0.0002803409588523209\n",
      "\n",
      "\n",
      "True token at position 70 : P\n",
      "log_probs_pre=\n",
      " -1.1778696775436401\n",
      "log_probs_fine=\n",
      " -0.00886138342320919\n",
      "\n",
      "\n",
      "True token at position 71 : E\n",
      "log_probs_pre=\n",
      " -0.633319616317749\n",
      "log_probs_fine=\n",
      " -0.0007484733941964805\n",
      "\n",
      "\n",
      "True token at position 72 : Q\n",
      "log_probs_pre=\n",
      " -0.48964977264404297\n",
      "log_probs_fine=\n",
      " -0.3612799346446991\n",
      "\n",
      "\n",
      "True token at position 73 : V\n",
      "log_probs_pre=\n",
      " -0.45639485120773315\n",
      "log_probs_fine=\n",
      " -0.0026853950694203377\n",
      "\n",
      "\n",
      "True token at position 74 : D\n",
      "log_probs_pre=\n",
      " -0.3477659523487091\n",
      "log_probs_fine=\n",
      " -0.0018148632952943444\n",
      "\n",
      "\n",
      "True token at position 75 : A\n",
      "log_probs_pre=\n",
      " -1.7560138702392578\n",
      "log_probs_fine=\n",
      " -0.19019311666488647\n",
      "\n",
      "\n",
      "True token at position 76 : L\n",
      "log_probs_pre=\n",
      " -1.437319278717041\n",
      "log_probs_fine=\n",
      " -0.0031885281205177307\n",
      "\n",
      "\n",
      "True token at position 77 : A\n",
      "log_probs_pre=\n",
      " -1.2409796714782715\n",
      "log_probs_fine=\n",
      " -0.007815614342689514\n",
      "\n",
      "\n",
      "True token at position 78 : R\n",
      "log_probs_pre=\n",
      " -2.7952933311462402\n",
      "log_probs_fine=\n",
      " -0.014988071285188198\n",
      "\n",
      "\n",
      "True token at position 79 : M\n",
      "log_probs_pre=\n",
      " -4.738519191741943\n",
      "log_probs_fine=\n",
      " -0.004567903000861406\n",
      "\n",
      "\n",
      "True token at position 80 : G\n",
      "log_probs_pre=\n",
      " -0.029828984290361404\n",
      "log_probs_fine=\n",
      " -0.1726541370153427\n",
      "\n",
      "\n",
      "True token at position 81 : C\n",
      "log_probs_pre=\n",
      " -4.152578353881836\n",
      "log_probs_fine=\n",
      " -0.004526843782514334\n",
      "\n",
      "\n",
      "True token at position 82 : Q\n",
      "log_probs_pre=\n",
      " -1.5145264863967896\n",
      "log_probs_fine=\n",
      " -0.004086596891283989\n",
      "\n",
      "\n",
      "True token at position 83 : L\n",
      "log_probs_pre=\n",
      " -2.276428461074829\n",
      "log_probs_fine=\n",
      " -0.004413269460201263\n",
      "\n",
      "\n",
      "True token at position 84 : I\n",
      "log_probs_pre=\n",
      " -1.1126281023025513\n",
      "log_probs_fine=\n",
      " -0.0011250602547079325\n",
      "\n",
      "\n",
      "True token at position 85 : V\n",
      "log_probs_pre=\n",
      " -0.15809769928455353\n",
      "log_probs_fine=\n",
      " -0.28206825256347656\n",
      "\n",
      "\n",
      "True token at position 86 : T\n",
      "log_probs_pre=\n",
      " -1.746335506439209\n",
      "log_probs_fine=\n",
      " -0.0009010545909404755\n",
      "\n",
      "\n",
      "True token at position 87 : P\n",
      "log_probs_pre=\n",
      " -0.0026085893623530865\n",
      "log_probs_fine=\n",
      " -0.0036543512251228094\n",
      "\n",
      "\n",
      "True token at position 88 : N\n",
      "log_probs_pre=\n",
      " -1.1241577863693237\n",
      "log_probs_fine=\n",
      " -0.0010418231831863523\n",
      "\n",
      "\n",
      "True token at position 89 : I\n",
      "log_probs_pre=\n",
      " -3.501636505126953\n",
      "log_probs_fine=\n",
      " -0.0012325793504714966\n",
      "\n",
      "\n",
      "True token at position 90 : H\n",
      "log_probs_pre=\n",
      " -3.485553741455078\n",
      "log_probs_fine=\n",
      " -0.004291254561394453\n",
      "\n",
      "\n",
      "True token at position 91 : S\n",
      "log_probs_pre=\n",
      " -3.629967212677002\n",
      "log_probs_fine=\n",
      " -0.0049257357604801655\n",
      "\n",
      "\n",
      "True token at position 92 : E\n",
      "log_probs_pre=\n",
      " -1.1369513273239136\n",
      "log_probs_fine=\n",
      " -0.0035887856502085924\n",
      "\n",
      "\n",
      "True token at position 93 : V\n",
      "log_probs_pre=\n",
      " -0.8066934943199158\n",
      "log_probs_fine=\n",
      " -0.003981877584010363\n",
      "\n",
      "\n",
      "True token at position 94 : I\n",
      "log_probs_pre=\n",
      " -0.6551611423492432\n",
      "log_probs_fine=\n",
      " -0.004722869023680687\n",
      "\n",
      "\n",
      "True token at position 95 : R\n",
      "log_probs_pre=\n",
      " -1.230291485786438\n",
      "log_probs_fine=\n",
      " -0.011923706158995628\n",
      "\n",
      "\n",
      "True token at position 96 : R\n",
      "log_probs_pre=\n",
      " -2.188167095184326\n",
      "log_probs_fine=\n",
      " -0.0070274327881634235\n",
      "\n",
      "\n",
      "True token at position 97 : A\n",
      "log_probs_pre=\n",
      " -0.5270064473152161\n",
      "log_probs_fine=\n",
      " -0.0052256896160542965\n",
      "\n",
      "\n",
      "True token at position 98 : V\n",
      "log_probs_pre=\n",
      " -1.5966330766677856\n",
      "log_probs_fine=\n",
      " -0.002092435723170638\n",
      "\n",
      "\n",
      "True token at position 99 : G\n",
      "log_probs_pre=\n",
      " -2.642530918121338\n",
      "log_probs_fine=\n",
      " -0.0043678125366568565\n",
      "\n",
      "\n",
      "True token at position 100 : Y\n",
      "log_probs_pre=\n",
      " -3.2495052814483643\n",
      "log_probs_fine=\n",
      " -0.00013755806139670312\n",
      "\n",
      "\n",
      "True token at position 101 : G\n",
      "log_probs_pre=\n",
      " -0.32736483216285706\n",
      "log_probs_fine=\n",
      " -0.011364017613232136\n",
      "\n",
      "\n",
      "True token at position 102 : M\n",
      "log_probs_pre=\n",
      " -0.6656482815742493\n",
      "log_probs_fine=\n",
      " -0.0077240606769919395\n",
      "\n",
      "\n",
      "True token at position 103 : T\n",
      "log_probs_pre=\n",
      " -2.1633448600769043\n",
      "log_probs_fine=\n",
      " -0.01016770675778389\n",
      "\n",
      "\n",
      "True token at position 104 : V\n",
      "log_probs_pre=\n",
      " -0.9548540115356445\n",
      "log_probs_fine=\n",
      " -0.005349013488739729\n",
      "\n",
      "\n",
      "True token at position 105 : C\n",
      "log_probs_pre=\n",
      " -4.311650276184082\n",
      "log_probs_fine=\n",
      " -0.006270614918321371\n",
      "\n",
      "\n",
      "True token at position 106 : P\n",
      "log_probs_pre=\n",
      " -0.03450113534927368\n",
      "log_probs_fine=\n",
      " -0.006681719329208136\n",
      "\n",
      "\n",
      "True token at position 107 : G\n",
      "log_probs_pre=\n",
      " -0.04731594771146774\n",
      "log_probs_fine=\n",
      " -0.004897146951407194\n",
      "\n",
      "\n",
      "True token at position 108 : C\n",
      "log_probs_pre=\n",
      " -2.8573708534240723\n",
      "log_probs_fine=\n",
      " -0.001501148217357695\n",
      "\n",
      "\n",
      "True token at position 109 : A\n",
      "log_probs_pre=\n",
      " -2.019869804382324\n",
      "log_probs_fine=\n",
      " -0.004475220572203398\n",
      "\n",
      "\n",
      "True token at position 110 : T\n",
      "log_probs_pre=\n",
      " -0.10392315685749054\n",
      "log_probs_fine=\n",
      " -0.002643782878294587\n",
      "\n",
      "\n",
      "True token at position 111 : A\n",
      "log_probs_pre=\n",
      " -2.2081797122955322\n",
      "log_probs_fine=\n",
      " -0.007705488707870245\n",
      "\n",
      "\n",
      "True token at position 112 : T\n",
      "log_probs_pre=\n",
      " -0.23410563170909882\n",
      "log_probs_fine=\n",
      " -0.006322974804788828\n",
      "\n",
      "\n",
      "True token at position 113 : E\n",
      "log_probs_pre=\n",
      " -0.011608080007135868\n",
      "log_probs_fine=\n",
      " -0.00032586511224508286\n",
      "\n",
      "\n",
      "True token at position 114 : A\n",
      "log_probs_pre=\n",
      " -0.3729158341884613\n",
      "log_probs_fine=\n",
      " -0.0032477036584168673\n",
      "\n",
      "\n",
      "True token at position 115 : F\n",
      "log_probs_pre=\n",
      " -1.322670578956604\n",
      "log_probs_fine=\n",
      " -0.0008467426523566246\n",
      "\n",
      "\n",
      "True token at position 116 : T\n",
      "log_probs_pre=\n",
      " -1.560603141784668\n",
      "log_probs_fine=\n",
      " -0.0048294090665876865\n",
      "\n",
      "\n",
      "True token at position 117 : A\n",
      "log_probs_pre=\n",
      " -0.0534282810986042\n",
      "log_probs_fine=\n",
      " -0.005489513278007507\n",
      "\n",
      "\n",
      "True token at position 118 : L\n",
      "log_probs_pre=\n",
      " -1.2095627784729004\n",
      "log_probs_fine=\n",
      " -0.0037087483797222376\n",
      "\n",
      "\n",
      "True token at position 119 : E\n",
      "log_probs_pre=\n",
      " -1.3367156982421875\n",
      "log_probs_fine=\n",
      " -0.003905527526512742\n",
      "\n",
      "\n",
      "True token at position 120 : A\n",
      "log_probs_pre=\n",
      " -1.636820912361145\n",
      "log_probs_fine=\n",
      " -0.00719491858035326\n",
      "\n",
      "\n",
      "True token at position 121 : G\n",
      "log_probs_pre=\n",
      " -0.004148212261497974\n",
      "log_probs_fine=\n",
      " -0.0008064831490628421\n",
      "\n",
      "\n",
      "True token at position 122 : A\n",
      "log_probs_pre=\n",
      " -0.046831123530864716\n",
      "log_probs_fine=\n",
      " -0.009227486327290535\n",
      "\n",
      "\n",
      "True token at position 123 : Q\n",
      "log_probs_pre=\n",
      " -2.514209747314453\n",
      "log_probs_fine=\n",
      " -0.001611959422007203\n",
      "\n",
      "\n",
      "True token at position 124 : A\n",
      "log_probs_pre=\n",
      " -1.2380024194717407\n",
      "log_probs_fine=\n",
      " -0.010770758613944054\n",
      "\n",
      "\n",
      "True token at position 125 : L\n",
      "log_probs_pre=\n",
      " -0.6813086867332458\n",
      "log_probs_fine=\n",
      " -0.0004892344586551189\n",
      "\n",
      "\n",
      "True token at position 126 : K\n",
      "log_probs_pre=\n",
      " -0.0006173135479912162\n",
      "log_probs_fine=\n",
      " -5.876845170860179e-05\n",
      "\n",
      "\n",
      "True token at position 127 : I\n",
      "log_probs_pre=\n",
      " -1.449833631515503\n",
      "log_probs_fine=\n",
      " -0.000834236154332757\n",
      "\n",
      "\n",
      "True token at position 128 : F\n",
      "log_probs_pre=\n",
      " -0.018948260694742203\n",
      "log_probs_fine=\n",
      " -0.0004119024670217186\n",
      "\n",
      "\n",
      "True token at position 129 : P\n",
      "log_probs_pre=\n",
      " -0.002459954936057329\n",
      "log_probs_fine=\n",
      " -0.005612567532807589\n",
      "\n",
      "\n",
      "True token at position 130 : S\n",
      "log_probs_pre=\n",
      " -2.7201309204101562\n",
      "log_probs_fine=\n",
      " -0.0036684852093458176\n",
      "\n",
      "\n",
      "True token at position 131 : S\n",
      "log_probs_pre=\n",
      " -2.24586820602417\n",
      "log_probs_fine=\n",
      " -0.002072688192129135\n",
      "\n",
      "\n",
      "True token at position 132 : A\n",
      "log_probs_pre=\n",
      " -1.546441912651062\n",
      "log_probs_fine=\n",
      " -0.013489289209246635\n",
      "\n",
      "\n",
      "True token at position 133 : F\n",
      "log_probs_pre=\n",
      " -3.473270893096924\n",
      "log_probs_fine=\n",
      " -0.000573351513594389\n",
      "\n",
      "\n",
      "True token at position 134 : G\n",
      "log_probs_pre=\n",
      " -1.5261995792388916\n",
      "log_probs_fine=\n",
      " -0.006067784037441015\n",
      "\n",
      "\n",
      "True token at position 135 : P\n",
      "log_probs_pre=\n",
      " -0.16632650792598724\n",
      "log_probs_fine=\n",
      " -0.0045343199744820595\n",
      "\n",
      "\n",
      "True token at position 136 : Q\n",
      "log_probs_pre=\n",
      " -3.2664434909820557\n",
      "log_probs_fine=\n",
      " -0.000179036331246607\n",
      "\n",
      "\n",
      "True token at position 137 : Y\n",
      "log_probs_pre=\n",
      " -0.3795355260372162\n",
      "log_probs_fine=\n",
      " -0.0016991952434182167\n",
      "\n",
      "\n",
      "True token at position 138 : I\n",
      "log_probs_pre=\n",
      " -1.382094383239746\n",
      "log_probs_fine=\n",
      " -0.0009149893885478377\n",
      "\n",
      "\n",
      "True token at position 139 : K\n",
      "log_probs_pre=\n",
      " -0.5035221576690674\n",
      "log_probs_fine=\n",
      " -0.0029538117814809084\n",
      "\n",
      "\n",
      "True token at position 140 : A\n",
      "log_probs_pre=\n",
      " -0.8134943246841431\n",
      "log_probs_fine=\n",
      " -0.008581665344536304\n",
      "\n",
      "\n",
      "True token at position 141 : L\n",
      "log_probs_pre=\n",
      " -1.2259843349456787\n",
      "log_probs_fine=\n",
      " -0.0003634030872490257\n",
      "\n",
      "\n",
      "True token at position 142 : K\n",
      "log_probs_pre=\n",
      " -1.9057261943817139\n",
      "log_probs_fine=\n",
      " -0.0009028411004692316\n",
      "\n",
      "\n",
      "True token at position 143 : A\n",
      "log_probs_pre=\n",
      " -0.28248631954193115\n",
      "log_probs_fine=\n",
      " -0.010128293186426163\n",
      "\n",
      "\n",
      "True token at position 144 : V\n",
      "log_probs_pre=\n",
      " -1.055379867553711\n",
      "log_probs_fine=\n",
      " -0.0022130541037768126\n",
      "\n",
      "\n",
      "True token at position 145 : L\n",
      "log_probs_pre=\n",
      " -0.21302950382232666\n",
      "log_probs_fine=\n",
      " -0.0018353299237787724\n",
      "\n",
      "\n",
      "True token at position 146 : P\n",
      "log_probs_pre=\n",
      " -0.08944626152515411\n",
      "log_probs_fine=\n",
      " -0.0077192108146846294\n",
      "\n",
      "\n",
      "True token at position 147 : S\n",
      "log_probs_pre=\n",
      " -2.518026351928711\n",
      "log_probs_fine=\n",
      " -0.5312889218330383\n",
      "\n",
      "\n",
      "True token at position 148 : D\n",
      "log_probs_pre=\n",
      " -1.0561652183532715\n",
      "log_probs_fine=\n",
      " -0.005765711888670921\n",
      "\n",
      "\n",
      "True token at position 149 : I\n",
      "log_probs_pre=\n",
      " -1.6826996803283691\n",
      "log_probs_fine=\n",
      " -0.0017711918335407972\n",
      "\n",
      "\n",
      "True token at position 150 : A\n",
      "log_probs_pre=\n",
      " -3.075082778930664\n",
      "log_probs_fine=\n",
      " -0.007459169719368219\n",
      "\n",
      "\n",
      "True token at position 151 : V\n",
      "log_probs_pre=\n",
      " -1.9874037504196167\n",
      "log_probs_fine=\n",
      " -0.0014692475087940693\n",
      "\n",
      "\n",
      "True token at position 152 : F\n",
      "log_probs_pre=\n",
      " -1.675069808959961\n",
      "log_probs_fine=\n",
      " -0.000501030299346894\n",
      "\n",
      "\n",
      "True token at position 153 : A\n",
      "log_probs_pre=\n",
      " -0.524933397769928\n",
      "log_probs_fine=\n",
      " -0.01072629727423191\n",
      "\n",
      "\n",
      "True token at position 154 : V\n",
      "log_probs_pre=\n",
      " -0.20116643607616425\n",
      "log_probs_fine=\n",
      " -0.48848530650138855\n",
      "\n",
      "\n",
      "True token at position 155 : G\n",
      "log_probs_pre=\n",
      " -0.020239029079675674\n",
      "log_probs_fine=\n",
      " -0.0012566297082230449\n",
      "\n",
      "\n",
      "True token at position 156 : G\n",
      "log_probs_pre=\n",
      " -0.01228315569460392\n",
      "log_probs_fine=\n",
      " -0.004790733102709055\n",
      "\n",
      "\n",
      "True token at position 157 : V\n",
      "log_probs_pre=\n",
      " -0.5213152766227722\n",
      "log_probs_fine=\n",
      " -0.000819347333163023\n",
      "\n",
      "\n",
      "True token at position 158 : T\n",
      "log_probs_pre=\n",
      " -1.236641526222229\n",
      "log_probs_fine=\n",
      " -0.0061243013478815556\n",
      "\n",
      "\n",
      "True token at position 159 : P\n",
      "log_probs_pre=\n",
      " -1.0612176656723022\n",
      "log_probs_fine=\n",
      " -0.005817150231450796\n",
      "\n",
      "\n",
      "True token at position 160 : E\n",
      "log_probs_pre=\n",
      " -1.2721972465515137\n",
      "log_probs_fine=\n",
      " -0.006685035303235054\n",
      "\n",
      "\n",
      "True token at position 161 : N\n",
      "log_probs_pre=\n",
      " -0.5243430733680725\n",
      "log_probs_fine=\n",
      " -0.005314270965754986\n",
      "\n",
      "\n",
      "True token at position 162 : L\n",
      "log_probs_pre=\n",
      " -1.5030912160873413\n",
      "log_probs_fine=\n",
      " -0.004696528892964125\n",
      "\n",
      "\n",
      "True token at position 163 : A\n",
      "log_probs_pre=\n",
      " -2.031522750854492\n",
      "log_probs_fine=\n",
      " -0.009726867079734802\n",
      "\n",
      "\n",
      "True token at position 164 : Q\n",
      "log_probs_pre=\n",
      " -2.655606269836426\n",
      "log_probs_fine=\n",
      " -0.001370206126011908\n",
      "\n",
      "\n",
      "True token at position 165 : W\n",
      "log_probs_pre=\n",
      " -1.2909893989562988\n",
      "log_probs_fine=\n",
      " -0.0013933007139712572\n",
      "\n",
      "\n",
      "True token at position 166 : I\n",
      "log_probs_pre=\n",
      " -2.8458755016326904\n",
      "log_probs_fine=\n",
      " -0.005732287652790546\n",
      "\n",
      "\n",
      "True token at position 167 : D\n",
      "log_probs_pre=\n",
      " -2.7021803855895996\n",
      "log_probs_fine=\n",
      " -0.0032873896416276693\n",
      "\n",
      "\n",
      "True token at position 168 : A\n",
      "log_probs_pre=\n",
      " -0.18502211570739746\n",
      "log_probs_fine=\n",
      " -0.0089979637414217\n",
      "\n",
      "\n",
      "True token at position 169 : G\n",
      "log_probs_pre=\n",
      " -0.002830429933965206\n",
      "log_probs_fine=\n",
      " -0.0009673921740613878\n",
      "\n",
      "\n",
      "True token at position 170 : C\n",
      "log_probs_pre=\n",
      " -3.362790584564209\n",
      "log_probs_fine=\n",
      " -0.00275827175937593\n",
      "\n",
      "\n",
      "True token at position 171 : A\n",
      "log_probs_pre=\n",
      " -1.7217663526535034\n",
      "log_probs_fine=\n",
      " -0.00783655047416687\n",
      "\n",
      "\n",
      "True token at position 172 : G\n",
      "log_probs_pre=\n",
      " -0.3164607286453247\n",
      "log_probs_fine=\n",
      " -0.0011364913079887629\n",
      "\n",
      "\n",
      "True token at position 173 : A\n",
      "log_probs_pre=\n",
      " -1.9707298278808594\n",
      "log_probs_fine=\n",
      " -0.026500841602683067\n",
      "\n",
      "\n",
      "True token at position 174 : G\n",
      "log_probs_pre=\n",
      " -0.009837951511144638\n",
      "log_probs_fine=\n",
      " -0.0028183048125356436\n",
      "\n",
      "\n",
      "True token at position 175 : L\n",
      "log_probs_pre=\n",
      " -2.260732650756836\n",
      "log_probs_fine=\n",
      " -0.0012062662281095982\n",
      "\n",
      "\n",
      "True token at position 176 : G\n",
      "log_probs_pre=\n",
      " -0.0409359335899353\n",
      "log_probs_fine=\n",
      " -0.0009036748087964952\n",
      "\n",
      "\n",
      "True token at position 177 : S\n",
      "log_probs_pre=\n",
      " -0.4204927086830139\n",
      "log_probs_fine=\n",
      " -0.016835978254675865\n",
      "\n",
      "\n",
      "True token at position 178 : D\n",
      "log_probs_pre=\n",
      " -2.5657308101654053\n",
      "log_probs_fine=\n",
      " -0.005537171848118305\n",
      "\n",
      "\n",
      "True token at position 179 : L\n",
      "log_probs_pre=\n",
      " -0.2074296921491623\n",
      "log_probs_fine=\n",
      " -0.00303082843311131\n",
      "\n",
      "\n",
      "True token at position 180 : Y\n",
      "log_probs_pre=\n",
      " -0.4074752628803253\n",
      "log_probs_fine=\n",
      " -0.7536547780036926\n",
      "\n",
      "\n",
      "True token at position 181 : R\n",
      "log_probs_pre=\n",
      " -1.1324576139450073\n",
      "log_probs_fine=\n",
      " -0.00292397802695632\n",
      "\n",
      "\n",
      "True token at position 182 : A\n",
      "log_probs_pre=\n",
      " -1.768548607826233\n",
      "log_probs_fine=\n",
      " -0.0160087738186121\n",
      "\n",
      "\n",
      "True token at position 183 : G\n",
      "log_probs_pre=\n",
      " -0.052633073180913925\n",
      "log_probs_fine=\n",
      " -0.0024180954787880182\n",
      "\n",
      "\n",
      "True token at position 184 : Q\n",
      "log_probs_pre=\n",
      " -2.3973228931427\n",
      "log_probs_fine=\n",
      " -0.005366206634789705\n",
      "\n",
      "\n",
      "True token at position 185 : S\n",
      "log_probs_pre=\n",
      " -0.7660987973213196\n",
      "log_probs_fine=\n",
      " -0.008208349347114563\n",
      "\n",
      "\n",
      "True token at position 186 : V\n",
      "log_probs_pre=\n",
      " -1.6673463582992554\n",
      "log_probs_fine=\n",
      " -0.002009992953389883\n",
      "\n",
      "\n",
      "True token at position 187 : E\n",
      "log_probs_pre=\n",
      " -0.44070929288864136\n",
      "log_probs_fine=\n",
      " -0.003106294432654977\n",
      "\n",
      "\n",
      "True token at position 188 : R\n",
      "log_probs_pre=\n",
      " -3.805788278579712\n",
      "log_probs_fine=\n",
      " -0.024506721645593643\n",
      "\n",
      "\n",
      "True token at position 189 : T\n",
      "log_probs_pre=\n",
      " -3.506119728088379\n",
      "log_probs_fine=\n",
      " -0.0006594866863451898\n",
      "\n",
      "\n",
      "True token at position 190 : A\n",
      "log_probs_pre=\n",
      " -2.4313738346099854\n",
      "log_probs_fine=\n",
      " -0.009077706374228\n",
      "\n",
      "\n",
      "True token at position 191 : Q\n",
      "log_probs_pre=\n",
      " -1.6553514003753662\n",
      "log_probs_fine=\n",
      " -0.0036351096350699663\n",
      "\n",
      "\n",
      "True token at position 192 : Q\n",
      "log_probs_pre=\n",
      " -2.247697591781616\n",
      "log_probs_fine=\n",
      " -0.0029910134617239237\n",
      "\n",
      "\n",
      "True token at position 193 : A\n",
      "log_probs_pre=\n",
      " -0.1555330902338028\n",
      "log_probs_fine=\n",
      " -0.0034389898646622896\n",
      "\n",
      "\n",
      "True token at position 194 : A\n",
      "log_probs_pre=\n",
      " -0.8052599430084229\n",
      "log_probs_fine=\n",
      " -0.006018136162310839\n",
      "\n",
      "\n",
      "True token at position 195 : A\n",
      "log_probs_pre=\n",
      " -0.6329491138458252\n",
      "log_probs_fine=\n",
      " -0.004160677082836628\n",
      "\n",
      "\n",
      "True token at position 196 : F\n",
      "log_probs_pre=\n",
      " -1.3160383701324463\n",
      "log_probs_fine=\n",
      " -0.00020358874462544918\n",
      "\n",
      "\n",
      "True token at position 197 : V\n",
      "log_probs_pre=\n",
      " -0.5291956663131714\n",
      "log_probs_fine=\n",
      " -0.002888675546273589\n",
      "\n",
      "\n",
      "True token at position 198 : K\n",
      "log_probs_pre=\n",
      " -1.6854549646377563\n",
      "log_probs_fine=\n",
      " -0.0011876918142661452\n",
      "\n",
      "\n",
      "True token at position 199 : A\n",
      "log_probs_pre=\n",
      " -0.2249497026205063\n",
      "log_probs_fine=\n",
      " -0.005199007224291563\n",
      "\n",
      "\n",
      "True token at position 200 : Y\n",
      "log_probs_pre=\n",
      " -1.3915860652923584\n",
      "log_probs_fine=\n",
      " -0.00023958197562023997\n",
      "\n",
      "\n",
      "True token at position 201 : R\n",
      "log_probs_pre=\n",
      " -0.7121734619140625\n",
      "log_probs_fine=\n",
      " -0.001447225920855999\n",
      "\n",
      "\n",
      "True token at position 202 : E\n",
      "log_probs_pre=\n",
      " -1.5494072437286377\n",
      "log_probs_fine=\n",
      " -0.0006820021662861109\n",
      "\n",
      "\n",
      "True token at position 203 : A\n",
      "log_probs_pre=\n",
      " -1.0278819799423218\n",
      "log_probs_fine=\n",
      " -0.0031755755189806223\n",
      "\n",
      "\n",
      "True token at position 204 : V\n",
      "log_probs_pre=\n",
      " -2.4903998374938965\n",
      "log_probs_fine=\n",
      " -0.004835815168917179\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LLR_scores = heatmap_generator.compute_llr(DgoA_seq, 0, len(DgoA_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d20f371a-a0bc-44fe-bf62-56741092516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapGenerator:\n",
    "    def __init__(self, model=None, tokenizer=None, model_name=None, is_progen=False):\n",
    "        \"\"\"\n",
    "        Initialize the HeatmapGenerator with either:\n",
    "        1. A pre-loaded model and tokenizer\n",
    "        2. A model_name to load from HuggingFace\n",
    "        \n",
    "        Args:\n",
    "            model: A pre-loaded model instance\n",
    "            tokenizer: A pre-loaded tokenizer\n",
    "            model_name: HuggingFace model name (used only if model and tokenizer are None)\n",
    "            is_progen: Set to True when using ProGen models that don't support MLM\n",
    "        \"\"\"\n",
    "        self.is_progen = is_progen\n",
    "        \n",
    "        if model is not None and tokenizer is not None:\n",
    "            # Use provided model and tokenizer\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "        elif model_name is not None:\n",
    "            # Load model and tokenizer from HuggingFace\n",
    "            if \"progen\" in model_name.lower():\n",
    "                self.is_progen = True\n",
    "                from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "            else:\n",
    "                from transformers import EsmForMaskedLM, EsmTokenizer\n",
    "                self.tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "                self.model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "            \n",
    "            # Put model in evaluation mode if it has the method\n",
    "            if hasattr(self.model, 'eval') and callable(self.model.eval):\n",
    "                self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(\"Either provide a model and tokenizer, or a model_name\")\n",
    "\n",
    "        # Print model type for debugging\n",
    "        print(f\"Using model type: {'ProGen' if self.is_progen else 'ESM'}\")\n",
    "\n",
    "    def _get_logits(self, input_ids, position=None):\n",
    "        \"\"\"\n",
    "        Runs the model and extracts logits, handling different model interfaces.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Tensor with input IDs or BatchEncoding\n",
    "            position: Position in the original sequence (for error reporting)\n",
    "            \n",
    "        Returns:\n",
    "            logits: Tensor of logits\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Try different methods of running the model\n",
    "                if hasattr(self.model, '__call__') and callable(self.model.__call__):\n",
    "                    outputs = self.model(input_ids)\n",
    "                elif hasattr(self.model, 'forward') and callable(self.model.forward):\n",
    "                    outputs = self.model.forward(input_ids)\n",
    "                elif hasattr(self.model, 'predict') and callable(self.model.predict):\n",
    "                    outputs = self.model.predict(input_ids)\n",
    "                elif hasattr(self.model, 'infer') and callable(self.model.infer):\n",
    "                    outputs = self.model.infer(input_ids)\n",
    "                else:\n",
    "                    raise ValueError(\"Could not find a suitable method to run the model\")\n",
    "                \n",
    "                # Extract logits from the model output\n",
    "                if isinstance(outputs, torch.Tensor):\n",
    "                    return outputs\n",
    "                elif hasattr(outputs, 'logits'):\n",
    "                    return outputs.logits\n",
    "                elif isinstance(outputs, dict) and 'logits' in outputs:\n",
    "                    return outputs['logits']\n",
    "                elif isinstance(outputs, tuple) and len(outputs) > 0:\n",
    "                    if isinstance(outputs[0], torch.Tensor):\n",
    "                        return outputs[0]\n",
    "                    elif hasattr(outputs[0], 'logits'):\n",
    "                        return outputs[0].logits\n",
    "                \n",
    "                # If we get here, we couldn't identify the logits\n",
    "                raise ValueError(f\"Could not extract logits from model output: {type(outputs)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error running model at position {position}: {str(e)}\")\n",
    "\n",
    "    def llrData(self, protein_sequence, start_pos=1, end_pos=None):\n",
    "        \"\"\"\n",
    "        Computes log likelihood ratio data for mutations at each position.\n",
    "        Modified to work with both ESM (MLM) and ProGen (CLM) models.\n",
    "        \n",
    "        Args:\n",
    "            protein_sequence: The protein sequence to analyze\n",
    "            start_pos: The starting position (1-indexed) to analyze\n",
    "            end_pos: The ending position (1-indexed) to analyze\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with LLR values for each amino acid at each position\n",
    "        \"\"\"\n",
    "        # Calculate sequence length\n",
    "        sequence_length = len(protein_sequence)\n",
    "    \n",
    "        # Adjust end_pos if not specified or if it exceeds the actual sequence length\n",
    "        if end_pos is None:\n",
    "            end_pos = sequence_length\n",
    "        end_pos = min(end_pos, sequence_length)\n",
    "    \n",
    "        # List of amino acids\n",
    "        amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    \n",
    "        # Initialize heatmap array: 20 rows (AAs) x (end_pos - start_pos + 1) columns\n",
    "        heatmap = np.zeros((20, end_pos - start_pos + 1))\n",
    "    \n",
    "        # Process each position\n",
    "        for position in range(start_pos, end_pos + 1):\n",
    "            try:\n",
    "                if not self.is_progen:\n",
    "                    # ESM uses masked language modeling\n",
    "                    # Tokenize the sequence\n",
    "                    tokens = self.tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "                    input_ids = tokens[\"input_ids\"].to(self.model.device)\n",
    "                    \n",
    "                    # Clone the input_ids to mask one position\n",
    "                    masked_input_ids = input_ids.clone().to(self.model.device)\n",
    "                    \n",
    "                    # ESM position offset: Adjust if needed\n",
    "                    token_position = position\n",
    "                    \n",
    "                    # Get the token ID for the mask token\n",
    "                    mask_token_id = self.tokenizer.mask_token_id\n",
    "                    if mask_token_id is None:\n",
    "                        raise ValueError(\"Tokenizer doesn't have a mask token\")\n",
    "                    \n",
    "                    # Mask the target position\n",
    "                    masked_input_ids[0, token_position] = mask_token_id\n",
    "                    \n",
    "                    # Get logits from the model for masked position\n",
    "                    logits = self._get_logits(masked_input_ids, position=position)\n",
    "                    \n",
    "                    # Get logits for the masked position\n",
    "                    masked_position_logits = logits[0, token_position]\n",
    "                    \n",
    "                    # Get the wildtype residue ID from input_ids\n",
    "                    wt_residue_id = input_ids[0, token_position].item()\n",
    "                    \n",
    "                else:  # ProGen or other causal models\n",
    "                    # Use causal language modeling (predict next token)\n",
    "                    \n",
    "                    # For ProGen, we need special handling for the first position\n",
    "                    if position <= 1:\n",
    "                        # For the first position, use a single letter to avoid empty tensor\n",
    "                        # This is just to get the vocabulary distribution\n",
    "                        prefix = \"A\"  # Use any amino acid here\n",
    "                    else:\n",
    "                        # Extract the sequence up to the position we want to predict\n",
    "                        prefix = protein_sequence[:position-1]  # -1 for 0-indexing\n",
    "                    \n",
    "                    # Tokenize just the prefix\n",
    "                    prefix_tokens = self.tokenizer(prefix, return_tensors=\"pt\")\n",
    "                    \n",
    "                    # Debug output to see what's happening\n",
    "                    #print(f\"Position {position}: Prefix '{prefix}', Prefix shape after tokenization: {prefix_tokens['input_ids'].shape}\")\n",
    "                    \n",
    "                    # Run the model to get the next token prediction\n",
    "                    logits = self._get_logits(prefix_tokens, position=position)\n",
    "                    \n",
    "                    # Get the last position logits (predicting the next token)\n",
    "                    if logits.size(1) > 0:\n",
    "                        masked_position_logits = logits[0, -1]\n",
    "                    else:\n",
    "                        # If we still get an empty tensor, use a default distribution\n",
    "                        # For safety, create an array of zeros with the correct vocab size\n",
    "                        vocab_size = len(self.tokenizer.get_vocab())\n",
    "                        masked_position_logits = torch.zeros(vocab_size, device=logits.device)\n",
    "                        print(f\"Warning: Empty logits at position {position}, using zeros with shape {masked_position_logits.shape}\")\n",
    "                    \n",
    "                    # For ProGen, get the wildtype residue directly from the sequence\n",
    "                    wt_residue = protein_sequence[position-1]  # 0-indexed\n",
    "                    # Map the character to a token ID\n",
    "                    wt_residue_id = None\n",
    "                    \n",
    "                    # Try different ways to get the token ID\n",
    "                    try:\n",
    "                        # Method 1: Direct conversion\n",
    "                        wt_residue_id = self.tokenizer.convert_tokens_to_ids(wt_residue)\n",
    "                        \n",
    "                        # Method 2: Try as a list\n",
    "                        if wt_residue_id is None or wt_residue_id == 0:\n",
    "                            wt_residue_id = self.tokenizer.convert_tokens_to_ids([wt_residue])[0]\n",
    "                            \n",
    "                        # Method 3: Use encode\n",
    "                        if wt_residue_id is None or wt_residue_id == 0:\n",
    "                            encoded = self.tokenizer.encode(wt_residue, add_special_tokens=False)\n",
    "                            if len(encoded) > 0:\n",
    "                                wt_residue_id = encoded[0]\n",
    "                        \n",
    "                        print(f\"Wildtype residue '{wt_residue}' at position {position} mapped to token ID {wt_residue_id}\")\n",
    "                    except Exception as e:\n",
    "                        # If all methods fail, print a warning\n",
    "                        print(f\"Warning: Could not convert '{wt_residue}' to a token ID: {str(e)}\")\n",
    "                \n",
    "                # Convert to probabilities and log probabilities\n",
    "                probs = torch.nn.functional.softmax(masked_position_logits, dim=0)\n",
    "                log_probs = torch.log(probs)\n",
    "    \n",
    "                # Set up wildtype log probability\n",
    "                log_prob_wt = 0.0\n",
    "                if wt_residue_id is not None:\n",
    "                    try:\n",
    "                        log_prob_wt = log_probs[wt_residue_id].item()\n",
    "                    except (IndexError, TypeError) as e:\n",
    "                        # If the ID is out of range, use 0.0\n",
    "                        print(f\"Warning: Token ID {wt_residue_id} for wildtype at position {position} is out of range: {str(e)}\")\n",
    "                        log_prob_wt = 0.0\n",
    "    \n",
    "                # Compute LLRs for all 20 amino acids\n",
    "                for i, aa in enumerate(amino_acids):\n",
    "                    try:\n",
    "                        # Get token ID for the amino acid - try multiple methods\n",
    "                        aa_id = None\n",
    "                        \n",
    "                        # Method 1: Direct conversion\n",
    "                        aa_id = self.tokenizer.convert_tokens_to_ids(aa)\n",
    "                        \n",
    "                        # Method 2: Try as a list\n",
    "                        if aa_id is None or aa_id == 0:\n",
    "                            aa_id = self.tokenizer.convert_tokens_to_ids([aa])[0]\n",
    "                            \n",
    "                        # Method 3: Use encode\n",
    "                        if aa_id is None or aa_id == 0:\n",
    "                            # Skip special tokens when encoding\n",
    "                            encoded = self.tokenizer.encode(aa, add_special_tokens=False)\n",
    "                            if len(encoded) > 0:\n",
    "                                aa_id = encoded[0]\n",
    "                        \n",
    "                        # If token ID exists, compute log likelihood ratio\n",
    "                        if aa_id is not None and aa_id > 0:\n",
    "                            try:\n",
    "                                log_prob_mut = log_probs[aa_id].item()\n",
    "                                heatmap[i, position - start_pos] = log_prob_mut - log_prob_wt\n",
    "                            except IndexError as e:\n",
    "                                # If the ID is out of range, use 0.0\n",
    "                                print(f\"Warning: Token ID {aa_id} for '{aa}' is out of range: {str(e)}\")\n",
    "                                heatmap[i, position - start_pos] = 0.0\n",
    "                        else:\n",
    "                            # Skip tokens not in the vocabulary\n",
    "                            heatmap[i, position - start_pos] = 0.0\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing amino acid '{aa}' at position {position}: {str(e)}\")\n",
    "                        heatmap[i, position - start_pos] = 0.0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing position {position}: {str(e)}\")\n",
    "                # Fill with zeros for this position\n",
    "                heatmap[:, position - start_pos] = 0\n",
    "    \n",
    "        # Create a DataFrame for readability\n",
    "        columns = [f\"{pos}\" for pos in range(start_pos, end_pos + 1)]\n",
    "        df = pd.DataFrame(heatmap, index=amino_acids, columns=columns)\n",
    "        return df\n",
    "\n",
    "        \n",
    "\n",
    "    def generate_heatmap(self, protein_sequence, start_pos=1, end_pos=None, figsize=(10, 5), \n",
    "                        cmap=\"viridis\", tick_interval=5, title=None):\n",
    "        \"\"\"\n",
    "        Plots the heatmap of log_prob_mutant - log_prob_wildtype.\n",
    "        \n",
    "        Args:\n",
    "            protein_sequence: The protein sequence to analyze\n",
    "            start_pos: The starting position (1-indexed) to analyze\n",
    "            end_pos: The ending position (1-indexed) to analyze\n",
    "            figsize: Figure size (width, height) in inches\n",
    "            cmap: Colormap to use for the heatmap\n",
    "            tick_interval: Label x-axis ticks at this interval\n",
    "            title: Custom title for the plot (if None, uses default)\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib figure object\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        heatmap = df.values  # shape [20, num_positions]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(heatmap, cmap=cmap, aspect=\"auto\")\n",
    "\n",
    "        ax.set_xticks(range(heatmap.shape[1]))  # Set ticks for every position\n",
    "        ax.set_xticklabels(\n",
    "            [df.columns[i] if i % tick_interval == 0 else \"\" for i in range(len(df.columns))],\n",
    "            rotation=90,\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "        ax.set_yticks(range(20))\n",
    "        ax.set_yticklabels(df.index)\n",
    "\n",
    "        ax.set_xlabel(\"Position in Protein Sequence\")\n",
    "        ax.set_ylabel(\"Amino Acid Mutations\")\n",
    "        \n",
    "        model_type = \"ProGen2\" if self.is_progen else \"ESM\"\n",
    "        if title is None:\n",
    "            title = f\"Predicted Effects of Mutations ({model_type} Model)\"\n",
    "        ax.set_title(title)\n",
    "\n",
    "        cbar = fig.colorbar(cax, ax=ax)\n",
    "        cbar.set_label(\"Log Likelihood Ratio (mutant vs. wild-type)\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def save_heatmap_data(self, protein_sequence, filename, start_pos=1, end_pos=None):\n",
    "        \"\"\"\n",
    "        Compute the LLR data and save it to a CSV file\n",
    "        \n",
    "        Args:\n",
    "            protein_sequence: The protein sequence to analyze\n",
    "            filename: Path to save the CSV file\n",
    "            start_pos: The starting position (1-indexed) to analyze\n",
    "            end_pos: The ending position (1-indexed) to analyze\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame containing the LLR data\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        df.to_csv(filename)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10d5f83a-b0ae-4352-9fcd-fcf1c3c58bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HeatmapGenerator...\n",
      "Using model type: ESM\n",
      "Testing with a short sequence segment...\n",
      "MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIP\n"
     ]
    }
   ],
   "source": [
    "wildtype_protein = DgoA_seq\n",
    "\n",
    "# Test small sequence first\n",
    "print(\"Creating HeatmapGenerator...\")\n",
    "heatmap_generator = HeatmapGenerator(model=model_finetuned, tokenizer=tokenizer)\n",
    "\n",
    "# Testing with a shorter sequence segment\n",
    "print(\"Testing with a short sequence segment...\")\n",
    "short_sequence = wildtype_protein[:50]  # First 50 amino acids\n",
    "print(short_sequence)\n",
    "try:\n",
    "    # Generate heatmap for a short segment to test functionality\n",
    "    test_fig = heatmap_generator.generate_heatmap(\n",
    "        protein_sequence=short_sequence,\n",
    "        start_pos=1,\n",
    "        end_pos=10,  # Just analyze the first 10 positions for quick testing\n",
    "        title=\"Test Heatmap (First 10 Positions)\",\n",
    "        figsize=(8, 4)\n",
    "    )\n",
    "    print(\"Short test successful!\")\n",
    "    \n",
    "    # If short test is successful, analyze a larger segment\n",
    "    print(\"Generating complete heatmap...\")\n",
    "    fig = heatmap_generator.generate_heatmap(\n",
    "        protein_sequence=wildtype_protein,\n",
    "        start_pos=1,\n",
    "        end_pos=len(wildtype_protein),  # First 50 positions\n",
    "        title=f\"Predicted Effects of Mutations (LoRA Fine-tuned Model)\",\n",
    "        figsize=(12, 6),\n",
    "        tick_interval=50\n",
    "    )\n",
    "  \n",
    "    # Save the results\n",
    "    print(\"Saving outputs...\")\n",
    "    fig.savefig(\"dgoa_esm_finetuned_llr_data.png\", dpi=300)\n",
    "    \n",
    "    # Save data to CSV\n",
    "    df = heatmap_generator.save_heatmap_data(\n",
    "        protein_sequence=wildtype_protein,\n",
    "        filename=\"dgoa_esm_finetuned_llr_data.csv\",\n",
    "        start_pos=1,\n",
    "        end_pos=len(wildtype_protein)\n",
    "    )\n",
    "    print(\"Analysis complete!\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d1fd4-91f2-42cc-a969-7a032ef6408a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

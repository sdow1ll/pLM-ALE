{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54e2764-b2bb-4132-be9d-16b7453f8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, re\n",
    "import numpy as np, pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# -------------------- Helper -------------------- #\n",
    "def get_token_index_from_sequence(input_ids_cpu, special_ids, residue_pos):\n",
    "    \"\"\"\n",
    "    Map 1-based residue_pos → token index by skipping special tokens.\n",
    "    input_ids_cpu: 1D torch.LongTensor on CPU.\n",
    "    special_ids: set of token IDs treated as 'special' (CLS, SEP, PAD, etc).\n",
    "    \"\"\"\n",
    "    seq_counter = 0\n",
    "    for idx, tid in enumerate(input_ids_cpu.tolist()):\n",
    "        if tid not in special_ids:\n",
    "            seq_counter += 1\n",
    "            if seq_counter == residue_pos:\n",
    "                return idx\n",
    "    raise ValueError(f\"Couldn't map residue {residue_pos} to a token index\")\n",
    "\n",
    "# -------------------- HeatmapGenerator -------------------- #\n",
    "class HeatmapGenerator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        # pick device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model  = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.special_ids = set(tokenizer.all_special_ids)\n",
    "        self.mask_id = tokenizer.mask_token_id\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_logits(self, ids, mask=None):\n",
    "        # ids, mask are already on self.device\n",
    "        with torch.no_grad():\n",
    "            if mask is not None:\n",
    "                return self.model(ids, attention_mask=mask).logits\n",
    "            else:\n",
    "                return self.model(ids).logits\n",
    "\n",
    "    def llrData(self, sequence, start_pos=1, end_pos=None):\n",
    "        if end_pos is None:\n",
    "            end_pos = len(sequence)\n",
    "        aas = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "        heatmap = np.zeros((20, end_pos - start_pos + 1), dtype=float)\n",
    "\n",
    "        # 1) Tokenize on CPU\n",
    "        enc_cpu = self.tokenizer(sequence, return_tensors=\"pt\")\n",
    "        input_ids_cpu = enc_cpu[\"input_ids\"][0]                       # 1D CPU\n",
    "        attention_cpu = enc_cpu.get(\"attention_mask\", None)          # 2D CPU or None\n",
    "\n",
    "        # 2) Move to device\n",
    "        input_ids = enc_cpu[\"input_ids\"].to(self.device)             # [1, L] GPU/CPU\n",
    "        attention_mask = (attention_cpu.to(self.device)\n",
    "                          if attention_cpu is not None else None)\n",
    "\n",
    "        for col, pos in enumerate(range(start_pos, end_pos+1)):\n",
    "            # a) map residue → token index\n",
    "            tok_i = get_token_index_from_sequence(input_ids_cpu,\n",
    "                                                  self.special_ids,\n",
    "                                                  pos)\n",
    "            # b) wild‑type token ID BEFORE masking\n",
    "            wt_id = input_ids[0, tok_i].item()\n",
    "\n",
    "            # c) mask and forward\n",
    "            masked = input_ids.clone()\n",
    "            masked[0, tok_i] = self.mask_id\n",
    "\n",
    "            logits = self._get_logits(masked, attention_mask)\n",
    "            log_probs = torch.log_softmax(logits[0, tok_i], dim=-1)\n",
    "\n",
    "            lp_wt = log_probs[wt_id].item()\n",
    "            for i, aa in enumerate(aas):\n",
    "                aa_id = self.tokenizer.convert_tokens_to_ids(aa)\n",
    "                hm_val = (log_probs[aa_id].item() - lp_wt\n",
    "                          if aa_id is not None and aa_id < log_probs.size(0)\n",
    "                          else 0.0)\n",
    "                heatmap[i, col] = hm_val\n",
    "\n",
    "        cols = [str(p) for p in range(start_pos, end_pos+1)]\n",
    "        return pd.DataFrame(heatmap, index=aas, columns=cols)\n",
    "\n",
    "# -------------------- Single‑mutation Function -------------------- #\n",
    "def compute_mutation_llr(model, tokenizer, sequence, mutation, device):\n",
    "    wt, pos, mut = re.match(r\"([A-Z])(\\d+)([A-Z])\", mutation).groups()\n",
    "    pos = int(pos)\n",
    "    if sequence[pos-1] != wt:\n",
    "        print(f\"[!] expected '{wt}' at {pos}, found '{sequence[pos-1]}'\")\n",
    "\n",
    "    # 1) Tokenize on CPU → for mapping\n",
    "    enc_cpu = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids_cpu = enc_cpu[\"input_ids\"][0]\n",
    "    special_ids = set(tokenizer.all_special_ids)\n",
    "    tok_i = get_token_index_from_sequence(input_ids_cpu, special_ids, pos)\n",
    "\n",
    "    # 2) Move inputs to device\n",
    "    enc = {k: v.to(device) for k, v in enc_cpu.items()}\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    attention_mask = enc.get(\"attention_mask\", None)\n",
    "\n",
    "    # 3) Mask & forward\n",
    "    masked = input_ids.clone()\n",
    "    masked[0, tok_i] = tokenizer.mask_token_id\n",
    "\n",
    "    model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        out = (model(masked, attention_mask=attention_mask)\n",
    "               if attention_mask is not None else model(masked))\n",
    "        log_probs = torch.log_softmax(out.logits[0, tok_i], dim=-1)\n",
    "\n",
    "    wt_id  = tokenizer.convert_tokens_to_ids(wt)\n",
    "    mut_id = tokenizer.convert_tokens_to_ids(mut)\n",
    "    lp_wt  = log_probs[wt_id].item()\n",
    "    lp_mut = log_probs[mut_id].item()\n",
    "    return lp_mut - lp_wt, lp_wt, lp_mut\n",
    "\n",
    "# -------------------- Comparison Runner -------------------- #\n",
    "def compare_llr_methods(model, tokenizer, sequence, mutations):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    generator = HeatmapGenerator(model, tokenizer)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    for mut in mutations:\n",
    "        llr_fn, lp_wt, lp_mut = compute_mutation_llr(\n",
    "            model, tokenizer, sequence, mut, device\n",
    "        )\n",
    "        pos = int(re.match(r\".(\\d+).\", mut).group(1))\n",
    "        df = generator.llrData(sequence, start_pos=pos, end_pos=pos)\n",
    "        llr_hm = df.loc[mut[-1], str(pos)]\n",
    "\n",
    "        print(f\"Mutation {mut}:\")\n",
    "        print(f\"  [Function]     LLR: {llr_fn:.6f}, wt: {lp_wt:.6f}, mut: {lp_mut:.6f}\")\n",
    "        print(f\"  [HeatmapData]  LLR: {llr_hm:.6f}\")\n",
    "        print(f\"  ΔLLR = {abs(llr_fn - llr_hm):.6f}\")\n",
    "        print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99362a-ee19-41bb-a6c3-7855c0bd7c36",
   "metadata": {},
   "source": [
    "# ESM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1125df-42dc-42eb-80bb-8978489b9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETRAINED ESM2\n",
      "============================================================\n",
      "Mutation F33I:\n",
      "  [Function]     LLR: 1.238841, wt: -2.160510, mut: -0.921669\n",
      "  [HeatmapData]  LLR: 1.238841\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation D58N:\n",
      "  [Function]     LLR: -3.135317, wt: -0.580529, mut: -3.715846\n",
      "  [HeatmapData]  LLR: -3.135317\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation A75V:\n",
      "  [Function]     LLR: -1.263491, wt: -1.756014, mut: -3.019505\n",
      "  [HeatmapData]  LLR: -1.263491\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation Q72H:\n",
      "  [Function]     LLR: -1.220701, wt: -0.489650, mut: -1.710351\n",
      "  [HeatmapData]  LLR: -1.220701\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation V85A:\n",
      "  [Function]     LLR: -5.180855, wt: -0.158098, mut: -5.338953\n",
      "  [HeatmapData]  LLR: -5.180855\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation V154F:\n",
      "  [Function]     LLR: -9.205742, wt: -0.201166, mut: -9.406908\n",
      "  [HeatmapData]  LLR: -9.205742\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation Y180F:\n",
      "  [Function]     LLR: -1.292190, wt: -0.407475, mut: -1.699665\n",
      "  [HeatmapData]  LLR: -1.292190\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "FINETUNED ESM2\n",
      "============================================================\n",
      "Mutation F33I:\n",
      "  [Function]     LLR: -1.033145, wt: -0.312933, mut: -1.346078\n",
      "  [HeatmapData]  LLR: -1.033145\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation D58N:\n",
      "  [Function]     LLR: -1.446954, wt: -0.217822, mut: -1.664777\n",
      "  [HeatmapData]  LLR: -1.446954\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation A75V:\n",
      "  [Function]     LLR: -1.586547, wt: -0.190193, mut: -1.776740\n",
      "  [HeatmapData]  LLR: -1.586547\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation Q72H:\n",
      "  [Function]     LLR: -0.842001, wt: -0.361280, mut: -1.203281\n",
      "  [HeatmapData]  LLR: -0.842001\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation V85A:\n",
      "  [Function]     LLR: -1.129199, wt: -0.282068, mut: -1.411267\n",
      "  [HeatmapData]  LLR: -1.129199\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation V154F:\n",
      "  [Function]     LLR: -0.474573, wt: -0.488485, mut: -0.963058\n",
      "  [HeatmapData]  LLR: -0.474573\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "Mutation Y180F:\n",
      "  [Function]     LLR: 0.116453, wt: -0.753655, mut: -0.637202\n",
      "  [HeatmapData]  LLR: 0.116453\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
    "\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "DgoA_seq = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "# List of mutations provided as strings\n",
    "mutations = ['F33I','D58N','A75V','Q72H','V85A','V154F','Y180F']\n",
    "\n",
    "print(\"PRETRAINED ESM2\")\n",
    "compare_llr_methods(model_pretrained, tokenizer, DgoA_seq, mutations)\n",
    "print(\"FINETUNED ESM2\")\n",
    "compare_llr_methods(model_finetuned, tokenizer, DgoA_seq, mutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b06ef-cf51-4d67-a9ec-e7920a125bc8",
   "metadata": {},
   "source": [
    "# ProGen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7deadb73-663d-4628-af19-1b49112c3d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for hugohrban/progen2-small contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/hugohrban/progen2-small.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for hugohrban/progen2-small contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/hugohrban/progen2-small.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProGenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for hugohrban/progen2-small contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/hugohrban/progen2-small.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRETRAINED PROGEN2\n",
      "Using model type: ESM on cuda\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't assign a NoneType to a torch.cuda.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m mutations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF33I\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD58N\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA75V\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ72H\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV85A\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV154F\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY180F\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRETRAINED PROGEN2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mcompare_llr_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDgoA_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINETUNED PROGEN2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m compare_llr_methods(model_finetuned, tokenizer, DgoA_seq, mutations)\n",
      "Cell \u001b[0;32mIn[3], line 123\u001b[0m, in \u001b[0;36mcompare_llr_methods\u001b[0;34m(model, tokenizer, sequence, mutations)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mut \u001b[38;5;129;01min\u001b[39;00m mutations:\n\u001b[0;32m--> 123\u001b[0m     llr_fn, lp_wt, lp_mut \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mutation_llr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+).\u001b[39m\u001b[38;5;124m\"\u001b[39m, mut)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    127\u001b[0m     df \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mllrData(sequence, start_pos\u001b[38;5;241m=\u001b[39mpos, end_pos\u001b[38;5;241m=\u001b[39mpos)\n",
      "Cell \u001b[0;32mIn[3], line 101\u001b[0m, in \u001b[0;36mcompute_mutation_llr\u001b[0;34m(model, tokenizer, sequence, mutation, device)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 3) Mask & forward\u001b[39;00m\n\u001b[1;32m    100\u001b[0m masked \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m--> 101\u001b[0m \u001b[43mmasked\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok_i\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id\n\u001b[1;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a NoneType to a torch.cuda.LongTensor"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"hugohrban/progen2-small\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
    "\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/progen2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "DgoA_seq = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "# List of mutations provided as strings\n",
    "mutations = ['F33I','D58N','A75V','Q72H','V85A','V154F','Y180F']\n",
    "\n",
    "print(\"PRETRAINED PROGEN2\")\n",
    "compare_llr_methods(model_pretrained, tokenizer, DgoA_seq, mutations)\n",
    "print(\"FINETUNED PROGEN2\")\n",
    "compare_llr_methods(model_finetuned, tokenizer, DgoA_seq, mutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2228dae4-bed1-49d6-8440-5d89028ec644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class HeatmapGenerator:\n",
    "    def __init__(self, model=None, tokenizer=None, model_name=None, is_progen=False):\n",
    "        \"\"\"\n",
    "        Initialize the HeatmapGenerator with either:\n",
    "        1. A pre-loaded model and tokenizer\n",
    "        2. A model_name to load from HuggingFace\n",
    "        \n",
    "        Args:\n",
    "            model: A pre-loaded model instance\n",
    "            tokenizer: A pre-loaded tokenizer\n",
    "            model_name: HuggingFace model name (used only if model and tokenizer are None)\n",
    "            is_progen: Set to True when using ProGen models that don't support MLM\n",
    "        \"\"\"\n",
    "        self.is_progen = is_progen\n",
    "        \n",
    "        if model is not None and tokenizer is not None:\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "        elif model_name is not None:\n",
    "            if \"progen\" in model_name.lower():\n",
    "                self.is_progen = True\n",
    "                from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "            else:\n",
    "                from transformers import EsmForMaskedLM, EsmTokenizer\n",
    "                self.tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "                self.model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "            if hasattr(self.model, 'eval'):\n",
    "                self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(\"Either provide a model and tokenizer, or a model_name\")\n",
    "\n",
    "        # device & specials\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.special_ids = set(self.tokenizer.all_special_ids)\n",
    "        self.mask_token_id = self.tokenizer.mask_token_id\n",
    "\n",
    "        print(f\"Using model type: {'ProGen' if self.is_progen else 'ESM'} on {self.device}\")\n",
    "\n",
    "    def _get_logits(self, input_ids, attention_mask=None):\n",
    "        with torch.no_grad():\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.device)\n",
    "                out = self.model(input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                out = self.model(input_ids)\n",
    "            # extract logits regardless of output type\n",
    "            return getattr(out, \"logits\", out[0] if isinstance(out, tuple) else out)\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_residue_to_token(input_ids, special_ids, residue_pos):\n",
    "        \"\"\"\n",
    "        Given a 1D tensor of input_ids on CPU and set of special_ids,\n",
    "        return the token index corresponding to the residue_pos (1-based),\n",
    "        skipping any special tokens.\n",
    "        \"\"\"\n",
    "        seq_count = 0\n",
    "        for idx, tid in enumerate(input_ids.tolist()):\n",
    "            if tid not in special_ids:\n",
    "                seq_count += 1\n",
    "                if seq_count == residue_pos:\n",
    "                    return idx\n",
    "        raise ValueError(f\"Could not map residue {residue_pos} to a token index\")\n",
    "\n",
    "    def llrData(self, protein_sequence, start_pos=1, end_pos=None):\n",
    "        seq_len = len(protein_sequence)\n",
    "        if end_pos is None or end_pos > seq_len:\n",
    "            end_pos = seq_len\n",
    "\n",
    "        aas = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "        heatmap = np.zeros((20, end_pos - start_pos + 1), dtype=float)\n",
    "\n",
    "        # tokenize once if MLM, or nothing if CLM\n",
    "        if not self.is_progen:\n",
    "            # Masked LM: we need full sequence tokenization\n",
    "            enc = self.tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "            input_ids_cpu = enc[\"input_ids\"][0]\n",
    "            attention_cpu = enc.get(\"attention_mask\", None)\n",
    "            input_ids = enc[\"input_ids\"].to(self.device)\n",
    "            attention_mask = attention_cpu.to(self.device) if attention_cpu is not None else None\n",
    "        else:\n",
    "            # For ProGen, we will retokenize prefixes each time\n",
    "            special = None  # unused\n",
    "\n",
    "        for col, pos in enumerate(range(start_pos, end_pos+1)):\n",
    "            try:\n",
    "                if not self.is_progen:\n",
    "                    # ESM path (masked LM)\n",
    "                    tok_idx = self._map_residue_to_token(input_ids_cpu,\n",
    "                                                         self.special_ids, pos)\n",
    "                    wt_id   = input_ids[0, tok_idx].item()\n",
    "                    masked  = input_ids.clone()\n",
    "                    masked[0, tok_idx] = self.mask_token_id\n",
    "                    logits  = self._get_logits(masked, attention_mask)\n",
    "                    log_probs = torch.log_softmax(logits[0, tok_idx], dim=-1)\n",
    "                else:\n",
    "                    # ProGen path (causal LM)\n",
    "                    # Build prefix up to pos-1\n",
    "                    prefix = protein_sequence[: pos-1] if pos>1 else \"\"\n",
    "                    enc_prefix = self.tokenizer(prefix, return_tensors=\"pt\")\n",
    "                    ids_cpu = enc_prefix[\"input_ids\"][0]\n",
    "                    # map pos-> next-token index = len(ids_cpu)-1\n",
    "                    wt_res = protein_sequence[pos-1]\n",
    "                    # wildtype token ID\n",
    "                    wt_id = self.tokenizer.convert_tokens_to_ids(wt_res)\n",
    "                    # move to device\n",
    "                    ids = enc_prefix[\"input_ids\"].to(self.device)\n",
    "                    mask = enc_prefix.get(\"attention_mask\", None)\n",
    "                    logits_full = self._get_logits(ids, mask)\n",
    "                    # next-token logits are at last position\n",
    "                    log_probs = torch.log_softmax(logits_full[0, -1], dim=-1)\n",
    "\n",
    "                log_wt = log_probs[wt_id].item()\n",
    "                for i, aa in enumerate(aas):\n",
    "                    aa_id = self.tokenizer.convert_tokens_to_ids(aa)\n",
    "                    heatmap[i, col] = (\n",
    "                        log_probs[aa_id].item() - log_wt\n",
    "                        if aa_id is not None and aa_id < log_probs.size(0)\n",
    "                        else 0.0\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at pos {pos}: {e}\")\n",
    "                heatmap[:, col] = 0.0\n",
    "\n",
    "        cols = [str(p) for p in range(start_pos, end_pos+1)]\n",
    "        return pd.DataFrame(heatmap, index=aas, columns=cols)\n",
    "\n",
    "\n",
    "    def generate_heatmap(self, protein_sequence, start_pos=1, end_pos=None,\n",
    "                         figsize=(10, 5), cmap=\"viridis\", tick_interval=5, title=None):\n",
    "        \"\"\"\n",
    "        Plots the heatmap of log_prob_mutant - log_prob_wildtype.\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(df.values, cmap=cmap, aspect=\"auto\")\n",
    "\n",
    "        ax.set_xticks(range(df.shape[1]))\n",
    "        ax.set_xticklabels(\n",
    "            [col if i % tick_interval == 0 else \"\" for i, col in enumerate(df.columns)],\n",
    "            rotation=90, fontsize=8\n",
    "        )\n",
    "        ax.set_yticks(range(len(df.index)))\n",
    "        ax.set_yticklabels(df.index)\n",
    "\n",
    "        ax.set_xlabel(\"Position\")\n",
    "        ax.set_ylabel(\"Amino Acid\")\n",
    "        model_type = \"ProGen\" if self.is_progen else \"ESM\"\n",
    "        ax.set_title(title or f\"LLR Heatmap ({model_type})\")\n",
    "        plt.colorbar(cax, ax=ax, label=\"LLR (mutant vs WT)\")\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def save_heatmap_data(self, protein_sequence, filename, start_pos=1, end_pos=None):\n",
    "        \"\"\"\n",
    "        Compute the LLR data and save it to CSV.\n",
    "        Returns the DataFrame for further use.\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        df.to_csv(filename)\n",
    "        print(f\"Saved LLR data to {filename}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d48382-e518-42b8-a0f5-8854e3acc654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sdowell/scratch/Thesis/ADP1/results'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b260b976-25ce-49cd-b707-3468f3ed8501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model type: ESM on cuda\n",
      "Using model type: ESM on cuda\n",
      "Using model type: ProGen on cuda\n",
      "Using model type: ProGen on cuda\n",
      "Saving full‐sequence LLRs to DgoA_esm_pretrained_full_sequence.csv ...\n",
      "Saved LLR data to DgoA_esm_pretrained_full_sequence.csv\n",
      "           1         2         3         4         5         6         7  \\\n",
      "A  -8.607591 -0.254883  0.598394 -1.159047 -0.223114 -1.143873  0.027223   \n",
      "C -11.709294 -4.224277 -2.918569 -3.368530 -3.439480 -4.316178 -2.283886   \n",
      "D -10.143125  0.022967 -0.416268 -1.278674  0.414902 -1.062482 -3.250323   \n",
      "E  -9.222314 -0.473002  0.438081 -0.981727  0.850894 -1.277184 -2.456563   \n",
      "F -10.871401 -2.358262 -0.152274 -0.042889 -2.167486 -4.575073 -1.300926   \n",
      "\n",
      "          8         9         10  ...       196       197       198       199  \\\n",
      "A -4.770309 -2.947813  -4.447848  ... -2.200759 -1.534761  0.464405  0.000000   \n",
      "C -7.264739 -3.379026  -4.942312  ... -3.540176 -2.611518 -4.847313 -5.613589   \n",
      "D -5.378175 -8.959444 -10.487106  ... -7.156523 -8.360392 -2.553880 -6.656273   \n",
      "E -5.836324 -8.666253  -9.409790  ... -7.367263 -7.040764 -2.019430 -5.973988   \n",
      "F -8.657617 -3.303360  -4.787302  ...  0.000000 -5.673549 -7.325736 -6.835136   \n",
      "\n",
      "        200       201       202       203       204       205  \n",
      "A -0.383986 -2.814820 -0.080003  0.000000  0.219545  0.668908  \n",
      "C -2.068042 -5.957041 -4.927659 -3.654232 -0.776892 -1.997691  \n",
      "D -9.362067 -4.946256 -1.043150 -4.785273 -3.690399 -1.993154  \n",
      "E -8.186059 -3.407352  0.000000 -2.935026 -2.452832 -1.353118  \n",
      "F -1.361941 -6.468616 -6.397621 -4.545381 -1.540697 -3.627534  \n",
      "\n",
      "[5 rows x 205 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to DgoA_esm_finetuned_full_sequence.csv ...\n",
      "Saved LLR data to DgoA_esm_finetuned_full_sequence.csv\n",
      "           1          2          3          4          5          6  \\\n",
      "A -11.923925 -13.404985 -11.860491 -13.090266  -6.350736 -12.021936   \n",
      "C -13.666243 -14.582540  -8.731630 -12.153505  -9.810660 -10.228265   \n",
      "D -14.319969 -12.607118 -11.756643 -15.419014 -10.280095 -10.742782   \n",
      "E -10.700240  -6.526724 -10.881938  -9.189344 -12.270050  -8.765331   \n",
      "F -13.360702 -12.329042 -11.547424 -12.008196  -9.363769 -12.891775   \n",
      "\n",
      "           7          8          9         10  ...        196        197  \\\n",
      "A -11.102415  -9.664465 -14.970751 -10.408276  ... -14.917285  -7.415428   \n",
      "C -10.612203 -12.432815 -15.117861 -12.060001  ... -11.053324 -14.864614   \n",
      "D -11.130565 -13.151043 -15.411837  -8.618205  ... -14.513806 -11.856850   \n",
      "E -11.654017 -13.650591 -15.534349  -9.913861  ... -16.360978 -10.613215   \n",
      "F  -5.872671 -13.331984  -8.911570  -6.854224  ...   0.000000  -6.443262   \n",
      "\n",
      "         198        199        200        201        202        203  \\\n",
      "A -12.511578   0.000000 -14.501762 -16.688199  -9.918474   0.000000   \n",
      "C -12.689119 -14.232523  -9.825922  -9.789171 -16.141263 -13.947304   \n",
      "D -13.237483 -11.117279 -13.960357 -15.456512  -9.249293 -12.441715   \n",
      "E -11.459760  -8.338202 -16.164441 -12.765090   0.000000  -9.215451   \n",
      "F -11.733683 -11.759133  -9.419037 -14.214200 -14.394170 -13.349625   \n",
      "\n",
      "         204        205  \n",
      "A  -9.886892 -16.100383  \n",
      "C -13.449488 -12.531767  \n",
      "D -11.754801 -14.164400  \n",
      "E  -6.403519 -12.513513  \n",
      "F  -9.917694 -12.645809  \n",
      "\n",
      "[5 rows x 205 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to DgoA_progen_pretrained_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to DgoA_progen_pretrained_full_sequence.csv\n",
      "     1         2         3         4         5         6         7         8  \\\n",
      "A  0.0  0.753872  1.840446  0.686455  0.534218  0.941437 -0.302948  0.568085   \n",
      "C  0.0 -1.047844 -0.060020 -0.928467 -1.570236 -1.083893 -2.276421 -1.484497   \n",
      "D  0.0  0.259064  1.309250  0.201775 -0.073410  0.249756 -0.557747  0.205177   \n",
      "E  0.0  0.387566  1.619431  0.295166  0.114479  0.398460 -0.362427  0.382950   \n",
      "F  0.0 -0.176250  0.943329  0.133224 -0.547737  0.150459 -1.048019 -0.244087   \n",
      "\n",
      "          9        10  ...       196       197       198       199       200  \\\n",
      "A -0.128639  0.267860  ... -0.358177 -1.718609  0.934082  0.000000 -1.391663   \n",
      "C -2.110290 -1.543076  ... -3.289150 -2.342365 -4.462913 -6.221222 -2.456852   \n",
      "D -0.445145 -0.096054  ... -4.664429 -8.736931 -1.509323 -4.635101 -8.625172   \n",
      "E -0.240715 -0.020462  ... -5.268749 -6.977883 -0.715927 -3.975845 -7.963463   \n",
      "F -1.102234 -0.281967  ...  0.000000 -5.097389 -5.931770 -4.326134 -0.615517   \n",
      "\n",
      "        201       202       203       204       205  \n",
      "A -0.871895  0.627525  0.000000  0.482132  1.616020  \n",
      "C -3.648415 -4.896027 -4.932877 -2.014206 -3.293045  \n",
      "D -3.420571 -0.498787 -6.354691 -3.116867 -0.688454  \n",
      "E -2.258781  0.000000 -5.250648 -2.105164 -0.025528  \n",
      "F -6.819954 -5.699738 -4.173523 -3.816963 -3.588089  \n",
      "\n",
      "[5 rows x 205 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to DgoA_progen_finetuned_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to DgoA_progen_finetuned_full_sequence.csv\n",
      "     1          2          3          4          5         6          7  \\\n",
      "A  0.0  -8.139176  -9.742505  -7.515557  -7.793045 -8.251640 -14.024440   \n",
      "C  0.0 -10.760725  -9.807160 -11.295498 -11.507592 -8.352432 -10.960064   \n",
      "D  0.0  -9.288586 -12.361981  -8.351783  -8.169648 -1.956032 -12.199655   \n",
      "E  0.0  -7.678551 -12.571473  -6.890209 -11.399094 -2.207577 -13.685547   \n",
      "F  0.0 -10.501278  -9.245186 -10.037502  -9.745571 -9.013397  -5.734282   \n",
      "\n",
      "           8          9         10  ...        196        197        198  \\\n",
      "A  -9.550309 -12.451075 -10.516666  ... -14.365066  -8.273266  -8.121437   \n",
      "C -12.272931 -12.589950 -10.102504  ... -12.631483 -10.783787 -10.968757   \n",
      "D -11.032372 -14.490683 -11.804214  ... -15.509523 -16.538154 -11.650310   \n",
      "E -11.255322 -14.481592 -10.903598  ... -13.561697 -14.491257  -8.236007   \n",
      "F -10.507840  -8.626011  -7.317925  ...   0.000000  -9.074173 -13.166107   \n",
      "\n",
      "         199        200        201        202        203        204        205  \n",
      "A   0.000000 -16.692616 -13.293609  -7.844929   0.000000  -6.956368 -11.509823  \n",
      "C -11.746044 -14.136266 -12.287922 -14.402931 -11.424179  -9.685226 -13.132233  \n",
      "D -11.435585 -17.192420 -14.324886  -9.301765 -14.432400  -9.724380 -14.777184  \n",
      "E  -9.160682 -16.303788 -10.283668   0.000000 -12.979286  -6.151989 -11.797692  \n",
      "F -12.543876 -12.277246 -16.191475 -13.153106 -11.114796 -10.374267 -14.134526  \n",
      "\n",
      "[5 rows x 205 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# ─── Your existing imports & data ─────────────────────────────────────\n",
    "# (you already ran these)\n",
    "from autoamp.evolveFinetune import *  # if needed\n",
    "from Bio import SeqIO\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "DgoA_seq = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "# List of mutations provided as strings\n",
    "mutations = ['F33I','D58N','A75V','Q72H','V85A','V154F','Y180F']\n",
    "\n",
    "# ─── 1. ESM‑2 models (masked LM) ────────────────────────────────────────\n",
    "esm_name = \"facebook/esm2_t30_150M_UR50D\"\n",
    "esm_tok  = AutoTokenizer.from_pretrained(esm_name, use_fast=True)\n",
    "\n",
    "# pretrained\n",
    "esm_pre   = AutoModelForMaskedLM.from_pretrained(esm_name)\n",
    "esm_pre_gen = HeatmapGenerator(model=esm_pre, tokenizer=esm_tok)\n",
    "\n",
    "# finetuned via PEFT adapter\n",
    "esm_base   = AutoModelForMaskedLM.from_pretrained(esm_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "esm_ft     = PeftModel.from_pretrained(esm_base, adapter_checkpoint)\n",
    "esm_ft_gen = HeatmapGenerator(model=esm_ft, tokenizer=esm_tok)\n",
    "\n",
    "# ─── 2. ProGen2 models (causal LM) ─────────────────────────────────────\n",
    "progen_name = \"hugohrban/progen2-small\"\n",
    "progen_tok  = AutoTokenizer.from_pretrained(progen_name, trust_remote_code=True)\n",
    "\n",
    "# pretrained ProGen2\n",
    "progen_pre   = AutoModelForCausalLM.from_pretrained(progen_name, trust_remote_code=True)\n",
    "progen_pre_gen = HeatmapGenerator(model=progen_pre, tokenizer=progen_tok, is_progen=True)\n",
    "\n",
    "# finetuned ProGen2 via PEFT\n",
    "progen_adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/progen2_dgoa_finetune_1/checkpoint-3000\"\n",
    "progen_base = AutoModelForCausalLM.from_pretrained(progen_name, trust_remote_code=True)\n",
    "progen_ft   = PeftModel.from_pretrained(progen_base, progen_adapter_checkpoint)\n",
    "progen_ft_gen = HeatmapGenerator(model=progen_ft, tokenizer=progen_tok, is_progen=True)\n",
    "\n",
    "gene_seq = {\n",
    "    \"DgoA\": DgoA_seq\n",
    "}\n",
    "\n",
    "# ── Loop & save CSVs across the full sequence ─────────────────────────\n",
    "for model_label, gen in [\n",
    "    (\"esm_pretrained\",    esm_pre_gen),\n",
    "    (\"esm_finetuned\",     esm_ft_gen),\n",
    "    (\"progen_pretrained\", progen_pre_gen),\n",
    "    (\"progen_finetuned\",  progen_ft_gen),\n",
    "]:\n",
    "    for gene, seq in gene_seq.items():\n",
    "        filename = f\"{gene}_{model_label}_full_sequence.csv\"\n",
    "        print(f\"Saving full‐sequence LLRs to {filename} ...\")\n",
    "        # omit start_pos/end_pos → defaults to entire sequence\n",
    "        df = gen.save_heatmap_data(\n",
    "            protein_sequence=DgoA_seq,\n",
    "            filename=filename\n",
    "        )\n",
    "        # (optional) inspect the head\n",
    "        print(df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f2ce3-96af-4278-b400-faee057e1630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

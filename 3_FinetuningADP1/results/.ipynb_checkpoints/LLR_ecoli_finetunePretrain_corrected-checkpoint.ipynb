{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f54e2764-b2bb-4132-be9d-16b7453f8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, re\n",
    "import numpy as np, pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# -------------------- Helper -------------------- #\n",
    "def get_token_index_from_sequence(input_ids_cpu, special_ids, residue_pos):\n",
    "    \"\"\"\n",
    "    Map 1-based residue_pos → token index by skipping special tokens.\n",
    "    input_ids_cpu: 1D torch.LongTensor on CPU.\n",
    "    special_ids: set of token IDs treated as 'special' (CLS, SEP, PAD, etc).\n",
    "    \"\"\"\n",
    "    seq_counter = 0\n",
    "    for idx, tid in enumerate(input_ids_cpu.tolist()):\n",
    "        if tid not in special_ids:\n",
    "            seq_counter += 1\n",
    "            if seq_counter == residue_pos:\n",
    "                return idx\n",
    "    raise ValueError(f\"Couldn't map residue {residue_pos} to a token index\")\n",
    "\n",
    "# -------------------- HeatmapGenerator -------------------- #\n",
    "class HeatmapGenerator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        # pick device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model  = model.to(self.device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.special_ids = set(tokenizer.all_special_ids)\n",
    "        self.mask_id = tokenizer.mask_token_id\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_logits(self, ids, mask=None):\n",
    "        # ids, mask are already on self.device\n",
    "        with torch.no_grad():\n",
    "            if mask is not None:\n",
    "                return self.model(ids, attention_mask=mask).logits\n",
    "            else:\n",
    "                return self.model(ids).logits\n",
    "\n",
    "    def llrData(self, sequence, start_pos=1, end_pos=None):\n",
    "        if end_pos is None:\n",
    "            end_pos = len(sequence)\n",
    "        aas = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "        heatmap = np.zeros((20, end_pos - start_pos + 1), dtype=float)\n",
    "\n",
    "        # 1) Tokenize on CPU\n",
    "        enc_cpu = self.tokenizer(sequence, return_tensors=\"pt\")\n",
    "        input_ids_cpu = enc_cpu[\"input_ids\"][0]                       # 1D CPU\n",
    "        attention_cpu = enc_cpu.get(\"attention_mask\", None)          # 2D CPU or None\n",
    "\n",
    "        # 2) Move to device\n",
    "        input_ids = enc_cpu[\"input_ids\"].to(self.device)             # [1, L] GPU/CPU\n",
    "        attention_mask = (attention_cpu.to(self.device)\n",
    "                          if attention_cpu is not None else None)\n",
    "\n",
    "        for col, pos in enumerate(range(start_pos, end_pos+1)):\n",
    "            # a) map residue → token index\n",
    "            tok_i = get_token_index_from_sequence(input_ids_cpu,\n",
    "                                                  self.special_ids,\n",
    "                                                  pos)\n",
    "            # b) wild‑type token ID BEFORE masking\n",
    "            wt_id = input_ids[0, tok_i].item()\n",
    "\n",
    "            # c) mask and forward\n",
    "            masked = input_ids.clone()\n",
    "            masked[0, tok_i] = self.mask_id\n",
    "\n",
    "            logits = self._get_logits(masked, attention_mask)\n",
    "            log_probs = torch.log_softmax(logits[0, tok_i], dim=-1)\n",
    "\n",
    "            lp_wt = log_probs[wt_id].item()\n",
    "            for i, aa in enumerate(aas):\n",
    "                aa_id = self.tokenizer.convert_tokens_to_ids(aa)\n",
    "                hm_val = (log_probs[aa_id].item() - lp_wt\n",
    "                          if aa_id is not None and aa_id < log_probs.size(0)\n",
    "                          else 0.0)\n",
    "                heatmap[i, col] = hm_val\n",
    "\n",
    "        cols = [str(p) for p in range(start_pos, end_pos+1)]\n",
    "        return pd.DataFrame(heatmap, index=aas, columns=cols)\n",
    "\n",
    "# -------------------- Single‑mutation Function -------------------- #\n",
    "def compute_mutation_llr(model, tokenizer, sequence, mutation, device):\n",
    "    wt, pos, mut = re.match(r\"([A-Z])(\\d+)([A-Z])\", mutation).groups()\n",
    "    pos = int(pos)\n",
    "    if sequence[pos-1] != wt:\n",
    "        print(f\"[!] expected '{wt}' at {pos}, found '{sequence[pos-1]}'\")\n",
    "\n",
    "    # 1) Tokenize on CPU → for mapping\n",
    "    enc_cpu = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids_cpu = enc_cpu[\"input_ids\"][0]\n",
    "    special_ids = set(tokenizer.all_special_ids)\n",
    "    tok_i = get_token_index_from_sequence(input_ids_cpu, special_ids, pos)\n",
    "\n",
    "    # 2) Move inputs to device\n",
    "    enc = {k: v.to(device) for k, v in enc_cpu.items()}\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    attention_mask = enc.get(\"attention_mask\", None)\n",
    "\n",
    "    # 3) Mask & forward\n",
    "    masked = input_ids.clone()\n",
    "    masked[0, tok_i] = tokenizer.mask_token_id\n",
    "\n",
    "    model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        out = (model(masked, attention_mask=attention_mask)\n",
    "               if attention_mask is not None else model(masked))\n",
    "        log_probs = torch.log_softmax(out.logits[0, tok_i], dim=-1)\n",
    "\n",
    "    wt_id  = tokenizer.convert_tokens_to_ids(wt)\n",
    "    mut_id = tokenizer.convert_tokens_to_ids(mut)\n",
    "    lp_wt  = log_probs[wt_id].item()\n",
    "    lp_mut = log_probs[mut_id].item()\n",
    "    return lp_mut - lp_wt, lp_wt, lp_mut\n",
    "\n",
    "# -------------------- Comparison Runner -------------------- #\n",
    "def compare_llr_methods(model, tokenizer, sequence, mutations):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    generator = HeatmapGenerator(model, tokenizer)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    for mut in mutations:\n",
    "        llr_fn, lp_wt, lp_mut = compute_mutation_llr(\n",
    "            model, tokenizer, sequence, mut, device\n",
    "        )\n",
    "        pos = int(re.match(r\".(\\d+).\", mut).group(1))\n",
    "        df = generator.llrData(sequence, start_pos=pos, end_pos=pos)\n",
    "        llr_hm = df.loc[mut[-1], str(pos)]\n",
    "\n",
    "        print(f\"Mutation {mut}:\")\n",
    "        print(f\"  [Function]     LLR: {llr_fn:.6f}, wt: {lp_wt:.6f}, mut: {lp_mut:.6f}\")\n",
    "        print(f\"  [HeatmapData]  LLR: {llr_hm:.6f}\")\n",
    "        print(f\"  ΔLLR = {abs(llr_fn - llr_hm):.6f}\")\n",
    "        print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb1125df-42dc-42eb-80bb-8978489b9e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Mutation H33Y:\n",
      "  [Function]     LLR: -7.873776, wt: -0.001575, mut: -7.875352\n",
      "  [HeatmapData]  LLR: -7.873776\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "Mutation H33Y:\n",
      "  [Function]     LLR: 10.498736, wt: -10.498798, mut: -0.000062\n",
      "  [HeatmapData]  LLR: 10.498736\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "Mutation K662I:\n",
      "  [Function]     LLR: -0.623509, wt: -1.979034, mut: -2.602542\n",
      "  [HeatmapData]  LLR: -0.623509\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "Mutation K662I:\n",
      "  [Function]     LLR: 8.221296, wt: -8.222016, mut: -0.000720\n",
      "  [HeatmapData]  LLR: 8.221296\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "Mutation L143I:\n",
      "  [Function]     LLR: -0.479110, wt: -2.057928, mut: -2.537038\n",
      "  [HeatmapData]  LLR: -0.479110\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n",
      "============================================================\n",
      "Mutation L143I:\n",
      "  [Function]     LLR: 6.995316, wt: -6.996365, mut: -0.001049\n",
      "  [HeatmapData]  LLR: 6.995316\n",
      "  ΔLLR = 0.000000\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
    "\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "# The protein sequence (concatenated)\n",
    "topA_seq = (\n",
    "\"MGKALVIVESPAKAKTINKYLGSDYVVKSSVGHIRDLPTSGSAAKKSADSTSTKTAKKPK\" \n",
    "    \"KDERGALVNRMGVDPWHNWEAHYEVLPGKEKVVSELKQLAEKADHIYLATDLDREGEAIA\" \n",
    "    \"WHLREVIGGDDARYSRVVFNEITKNAIRQAFNKPGELNIDRVNAQQARRFMDRVVGYMVS\" \n",
    "    \"PLLWKKIARGLSAGRVQSVAVRLVVEREREIKAFVPEEFWEVDASTTTPSGEALALQVTH\" \n",
    "    \"QNDKPFRPVNKEQTQAAVSLLEKARYSVLEREDKPTTSKPGAPFITSTLQQAASTRLGFG\" \n",
    "    \"VKKTMMMAQRLYEAGYITYMRTDSTNLSQDAVNMVRGYISDNFGKKYLPESPNQYASKEN\" \n",
    "    \"SQEAHEAIRPSDVNVMAESLKDMEADAQKLYQLIWRQFVACQMTPAKYDSTTLTVGAGDF\" \n",
    "    \"RLKARGRILRFDGWTKVMPALRKGDEDRILPAVNKGDALTLVELTPAQHFTKPPARFSEA\" \n",
    "    \"SLVKELEKRGIGRPSTYASIISTIQDRGYVRVENRRFYAEKMGEIVTDRLEENFRELMNY\" \n",
    "    \"DFTAQMENSLDQVANHEAEWKAVLDHFFSDFTQQLDKAEKDPEEGGMRPNQMVLTSIDCP\" \n",
    "    \"TCGRKMGIRTASTGVFLGCSGYALPPKERCKTTINLVPENEVLNVLEGEDAETNALRAKR\" \n",
    "    \"RCPKCGTAMDSYLIDPKRKLHVCGNNPTCDGYEIEEGEFRIKGYDGPIVECEKCGSEMHL\" \n",
    "    \"KMGRFGKYMACTNEECKNTRKILRNGEVAPPKEDPVPLPELPCEKSDAYFVLRDGAAGVF\" \n",
    "    \"LAANTFPKSRETRAPLVEELYRFRDRLPEKLRYLADAPQQDPEGNKTMVRFSRKTKQQYV\" \n",
    "    \"SSEKDGKATGWSAFYVDGKWVEGKK\" \n",
    ")\n",
    "spoT_seq = (\"MYLFESLNQLIQTYLPEDQIKRLRQAYLVARDAHEGQTRSSGEPYITHPVAVACILAEMK\"\n",
    "                    \"LDYETLMAALLHDVIEDTPATYQDMEQLFGKSVAELVEGVSKLDKLKFRDKKEAQAENFR\"\n",
    "                    \"KMIMAMVQDIRVILIKLADRTHNMRTLGSLRPDKRRRIARETLEIYSPLAHRLGIHHIKT\"\n",
    "                    \"ELEELGFEALYPNRYRVIKEVVKAARGNRKEMIQKILSEIEGRLQEAGIPCRVSGREKHL\"\n",
    "                    \"YSIYCKMVLKEQRFHSIMDIYAFRVIVNDSDTCYRVLGQMHSLYKPRPGRVKDYIAIPKA\"\n",
    "                    \"NGYQSLHTSMIGPHGVPVEVQIRTEDMDQMAEMGVAAHWAYKEHGETSTTAQIRAQRWMQ\"\n",
    "                    \"SLLELQQSAGSSFEFIESVKSDLFPDEIYVFTPEGRIVELPAGATPVDFAYAVHTDIGHA\"\n",
    "                    \"CVGARVDRQPYPLSQPLTSGQTVEIITAPGARPNAAWLNFVVSSKARAKIRQLLKNLKRD\"\n",
    "                    \"DSVSLGRRLLNHALGGSRKLNEIPQENIQRELDRMKLATLDDLLAEIGLGNAMSVVVAKN\"\n",
    "                    \"LQHGDASIPPATQSHGHLPIKGADGVLITFAKCCRPIPGDPIIAHVSPGKGLVIHHESCR\"\n",
    "                    \"NIRGYQKEPEKFMAVEWDKETAQEFITEIKVEMFNHQGALANLTAAINTTTSNIQSLNTE\"\n",
    "                    \"EKDGRVYSAFIRLTARDRVHLANIMRKIRVMPDVIKVTRNRN\")\n",
    "\n",
    "yeiB_seq = (\"MERNVTLDFVRGVAILGILLLNISAFGLPKAAYLNPAWYGAITPRDAWTWAFLDLIGQVK\"\n",
    "\"FLTLFALLFGAGLQMLLPRGRRWIQSRLTLLVLLGFIHGLLFWDGDILLAYGLVGLICWR\"\n",
    "\"LVRDAPSVKSLFNTGVMLYLVGLGVLLLLGLISDSQTSRAWTPDASAILYEKYWKLHGGV\"\n",
    "\"EAISNRADGVGNSLLALGAQYGWQLAGMMLIGAALMRSGWLKGQFSLRHYRRTGFVLVAI\"\n",
    "\"GVTINLPAIALQWQLDWAYRWCAFLLQMPRELSAPFQAIGYASLFYGFWPQLSRFKLVLA\"\n",
    "\"IACVGRMALTNYLLQTLICTTLFYHLGLFMHFDRLELLAFVIPVWLANILFSVIWLRYFR\"\n",
    "\"QGPVEWLWRQLTLRAAGPAISKTSR\")\n",
    "\n",
    "# List of mutations provided as strings\n",
    "gene_mutation = {\"topA\":\"H33Y\", \"spoT\":\"K662I\", \"yeiB\":\"L143I\"}\n",
    "\n",
    "mutations = [[gene_mutation[\"topA\"]], [gene_mutation[\"spoT\"]], [gene_mutation[\"yeiB\"]]]\n",
    "\n",
    "compare_llr_methods(model, tokenizer, topA_seq, mutations[0])\n",
    "compare_llr_methods(model_finetuned, tokenizer, topA_seq, mutations[0])\n",
    "compare_llr_methods(model, tokenizer, spoT_seq, mutations[1])\n",
    "compare_llr_methods(model_finetuned, tokenizer, spoT_seq, mutations[1])\n",
    "compare_llr_methods(model, tokenizer, yeiB_seq, mutations[2])\n",
    "compare_llr_methods(model_finetuned, tokenizer, yeiB_seq, mutations[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2228dae4-bed1-49d6-8440-5d89028ec644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class HeatmapGenerator:\n",
    "    def __init__(self, model=None, tokenizer=None, model_name=None, is_progen=False):\n",
    "        \"\"\"\n",
    "        Initialize the HeatmapGenerator with either:\n",
    "        1. A pre-loaded model and tokenizer\n",
    "        2. A model_name to load from HuggingFace\n",
    "        \n",
    "        Args:\n",
    "            model: A pre-loaded model instance\n",
    "            tokenizer: A pre-loaded tokenizer\n",
    "            model_name: HuggingFace model name (used only if model and tokenizer are None)\n",
    "            is_progen: Set to True when using ProGen models that don't support MLM\n",
    "        \"\"\"\n",
    "        self.is_progen = is_progen\n",
    "        \n",
    "        if model is not None and tokenizer is not None:\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "        elif model_name is not None:\n",
    "            if \"progen\" in model_name.lower():\n",
    "                self.is_progen = True\n",
    "                from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "            else:\n",
    "                from transformers import EsmForMaskedLM, EsmTokenizer\n",
    "                self.tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "                self.model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "            if hasattr(self.model, 'eval'):\n",
    "                self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(\"Either provide a model and tokenizer, or a model_name\")\n",
    "\n",
    "        # device & specials\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.special_ids = set(self.tokenizer.all_special_ids)\n",
    "        self.mask_token_id = self.tokenizer.mask_token_id\n",
    "\n",
    "        print(f\"Using model type: {'ProGen' if self.is_progen else 'ESM'} on {self.device}\")\n",
    "\n",
    "    def _get_logits(self, input_ids, attention_mask=None):\n",
    "        with torch.no_grad():\n",
    "            input_ids = input_ids.to(self.device)\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(self.device)\n",
    "                out = self.model(input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                out = self.model(input_ids)\n",
    "            # extract logits regardless of output type\n",
    "            return getattr(out, \"logits\", out[0] if isinstance(out, tuple) else out)\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_residue_to_token(input_ids, special_ids, residue_pos):\n",
    "        \"\"\"\n",
    "        Given a 1D tensor of input_ids on CPU and set of special_ids,\n",
    "        return the token index corresponding to the residue_pos (1-based),\n",
    "        skipping any special tokens.\n",
    "        \"\"\"\n",
    "        seq_count = 0\n",
    "        for idx, tid in enumerate(input_ids.tolist()):\n",
    "            if tid not in special_ids:\n",
    "                seq_count += 1\n",
    "                if seq_count == residue_pos:\n",
    "                    return idx\n",
    "        raise ValueError(f\"Could not map residue {residue_pos} to a token index\")\n",
    "\n",
    "    def llrData(self, protein_sequence, start_pos=1, end_pos=None):\n",
    "        seq_len = len(protein_sequence)\n",
    "        if end_pos is None or end_pos > seq_len:\n",
    "            end_pos = seq_len\n",
    "\n",
    "        aas = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "        heatmap = np.zeros((20, end_pos - start_pos + 1), dtype=float)\n",
    "\n",
    "        # tokenize once if MLM, or nothing if CLM\n",
    "        if not self.is_progen:\n",
    "            # Masked LM: we need full sequence tokenization\n",
    "            enc = self.tokenizer(protein_sequence, return_tensors=\"pt\")\n",
    "            input_ids_cpu = enc[\"input_ids\"][0]\n",
    "            attention_cpu = enc.get(\"attention_mask\", None)\n",
    "            input_ids = enc[\"input_ids\"].to(self.device)\n",
    "            attention_mask = attention_cpu.to(self.device) if attention_cpu is not None else None\n",
    "        else:\n",
    "            # For ProGen, we will retokenize prefixes each time\n",
    "            special = None  # unused\n",
    "\n",
    "        for col, pos in enumerate(range(start_pos, end_pos+1)):\n",
    "            try:\n",
    "                if not self.is_progen:\n",
    "                    # ESM path (masked LM)\n",
    "                    tok_idx = self._map_residue_to_token(input_ids_cpu,\n",
    "                                                         self.special_ids, pos)\n",
    "                    wt_id   = input_ids[0, tok_idx].item()\n",
    "                    masked  = input_ids.clone()\n",
    "                    masked[0, tok_idx] = self.mask_token_id\n",
    "                    logits  = self._get_logits(masked, attention_mask)\n",
    "                    log_probs = torch.log_softmax(logits[0, tok_idx], dim=-1)\n",
    "                else:\n",
    "                    # ProGen path (causal LM)\n",
    "                    # Build prefix up to pos-1\n",
    "                    prefix = protein_sequence[: pos-1] if pos>1 else \"\"\n",
    "                    enc_prefix = self.tokenizer(prefix, return_tensors=\"pt\")\n",
    "                    ids_cpu = enc_prefix[\"input_ids\"][0]\n",
    "                    # map pos-> next-token index = len(ids_cpu)-1\n",
    "                    wt_res = protein_sequence[pos-1]\n",
    "                    # wildtype token ID\n",
    "                    wt_id = self.tokenizer.convert_tokens_to_ids(wt_res)\n",
    "                    # move to device\n",
    "                    ids = enc_prefix[\"input_ids\"].to(self.device)\n",
    "                    mask = enc_prefix.get(\"attention_mask\", None)\n",
    "                    logits_full = self._get_logits(ids, mask)\n",
    "                    # next-token logits are at last position\n",
    "                    log_probs = torch.log_softmax(logits_full[0, -1], dim=-1)\n",
    "\n",
    "                log_wt = log_probs[wt_id].item()\n",
    "                for i, aa in enumerate(aas):\n",
    "                    aa_id = self.tokenizer.convert_tokens_to_ids(aa)\n",
    "                    heatmap[i, col] = (\n",
    "                        log_probs[aa_id].item() - log_wt\n",
    "                        if aa_id is not None and aa_id < log_probs.size(0)\n",
    "                        else 0.0\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at pos {pos}: {e}\")\n",
    "                heatmap[:, col] = 0.0\n",
    "\n",
    "        cols = [str(p) for p in range(start_pos, end_pos+1)]\n",
    "        return pd.DataFrame(heatmap, index=aas, columns=cols)\n",
    "\n",
    "\n",
    "    def generate_heatmap(self, protein_sequence, start_pos=1, end_pos=None,\n",
    "                         figsize=(10, 5), cmap=\"viridis\", tick_interval=5, title=None):\n",
    "        \"\"\"\n",
    "        Plots the heatmap of log_prob_mutant - log_prob_wildtype.\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        cax = ax.imshow(df.values, cmap=cmap, aspect=\"auto\")\n",
    "\n",
    "        ax.set_xticks(range(df.shape[1]))\n",
    "        ax.set_xticklabels(\n",
    "            [col if i % tick_interval == 0 else \"\" for i, col in enumerate(df.columns)],\n",
    "            rotation=90, fontsize=8\n",
    "        )\n",
    "        ax.set_yticks(range(len(df.index)))\n",
    "        ax.set_yticklabels(df.index)\n",
    "\n",
    "        ax.set_xlabel(\"Position\")\n",
    "        ax.set_ylabel(\"Amino Acid\")\n",
    "        model_type = \"ProGen\" if self.is_progen else \"ESM\"\n",
    "        ax.set_title(title or f\"LLR Heatmap ({model_type})\")\n",
    "        plt.colorbar(cax, ax=ax, label=\"LLR (mutant vs WT)\")\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def save_heatmap_data(self, protein_sequence, filename, start_pos=1, end_pos=None):\n",
    "        \"\"\"\n",
    "        Compute the LLR data and save it to CSV.\n",
    "        Returns the DataFrame for further use.\n",
    "        \"\"\"\n",
    "        df = self.llrData(protein_sequence, start_pos, end_pos)\n",
    "        df.to_csv(filename)\n",
    "        print(f\"Saved LLR data to {filename}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34d48382-e518-42b8-a0f5-8854e3acc654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sdowell/scratch/Thesis'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b260b976-25ce-49cd-b707-3468f3ed8501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model type: ESM on cuda\n",
      "Using model type: ESM on cuda\n",
      "Using model type: ProGen on cuda\n",
      "Using model type: ProGen on cuda\n",
      "Saving full‐sequence LLRs to topA_esm_pretrained_full_sequence.csv ...\n",
      "Saved LLR data to topA_esm_pretrained_full_sequence.csv\n",
      "           1         2         3         4          5          6          7  \\\n",
      "A  -7.860852  0.898954 -4.338935  0.000000  -6.224969  -4.939164  -9.669122   \n",
      "C -10.719876 -5.129097 -7.209413 -3.161048  -7.122352  -3.875269 -10.084961   \n",
      "D  -8.410981 -1.875968 -4.203128 -3.031852 -13.915084 -13.362583 -14.192044   \n",
      "E  -8.406668 -1.254799 -3.644945 -1.901605 -12.897623 -12.520166 -13.429348   \n",
      "F  -9.296198 -4.304859 -6.416578 -4.530185  -6.975884  -3.493910  -6.829610   \n",
      "\n",
      "           8          9         10  ...       856       857       858  \\\n",
      "A  -1.537119 -11.029301  -4.220697  ... -0.929391 -2.887781 -5.453708   \n",
      "C  -3.873991 -12.514373  -8.771953  ... -4.318420 -6.371966 -7.500591   \n",
      "D -11.427444  -8.036858 -10.795480  ... -2.040178  0.000000 -3.469801   \n",
      "E -10.112083   0.000000 -10.359556  ...  0.524019 -2.163144 -3.941964   \n",
      "F  -8.963152 -14.925805 -14.277381  ... -3.608596 -6.208986 -6.902865   \n",
      "\n",
      "        859       860       861       862       863       864       865  \n",
      "A -2.378516 -0.375736 -2.505758 -0.441612  0.615026 -1.743647 -1.417799  \n",
      "C -5.808377 -3.270630 -4.542050 -3.367354 -3.313016 -5.907842 -5.809529  \n",
      "D -3.681221 -2.808914 -5.517426 -1.218301 -0.195907 -2.258045 -1.936707  \n",
      "E -1.504390 -0.541521 -2.553097  0.000000  0.677130 -1.048066 -1.376098  \n",
      "F -5.440666 -0.296993 -3.915599 -0.821040 -1.189007 -4.281812 -3.722228  \n",
      "\n",
      "[5 rows x 865 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to spoT_esm_pretrained_full_sequence.csv ...\n",
      "Saved LLR data to spoT_esm_pretrained_full_sequence.csv\n",
      "          1         2         3         4         5         6         7  \\\n",
      "A -7.071325  1.298436 -0.437790 -0.703475 -2.756383 -0.896469 -4.639564   \n",
      "C -9.678347 -1.882905 -1.829826 -3.162426 -6.395757 -3.641036 -5.410970   \n",
      "D -6.285937  3.165690  2.017573 -0.433372  0.045849  1.266514 -6.841100   \n",
      "E -6.644764  3.145636  0.851217  0.612965  0.000000  2.088969 -6.007716   \n",
      "F -6.518476  0.092474 -1.704769  0.000000 -4.925043 -2.830400 -2.273196   \n",
      "\n",
      "          8         9        10  ...       693        694       695       696  \\\n",
      "A  0.266142 -0.821609 -0.906204  ... -1.596622  -5.328964 -2.251070 -0.332710   \n",
      "C  0.438208 -4.444022 -2.838225  ... -4.404253  -6.598033 -4.960967 -3.721815   \n",
      "D  0.595683 -0.387810 -1.172096  ...  0.000000 -10.504554 -3.549587 -1.320283   \n",
      "E  2.400365  0.006282 -0.085817  ...  0.190063 -10.129332 -2.494253 -0.135783   \n",
      "F  0.995242 -4.234338 -2.377209  ... -3.279641  -6.687070 -0.960530 -3.381772   \n",
      "\n",
      "         697       698       699       700       701       702  \n",
      "A  -1.709652 -1.831633 -7.546413  2.096215 -1.039896  0.716513  \n",
      "C  -5.060508 -4.093297 -7.956755  0.524546 -4.179888 -3.122739  \n",
      "D -10.920817 -1.948341 -9.387957  0.141513 -1.143880  0.352430  \n",
      "E  -9.724214 -0.012853 -8.698035  1.549025 -0.676902  0.593359  \n",
      "F  -7.039269 -1.022200 -8.246769  1.200990 -2.149496 -1.117896  \n",
      "\n",
      "[5 rows x 702 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to yeiB_esm_pretrained_full_sequence.csv ...\n",
      "Saved LLR data to yeiB_esm_pretrained_full_sequence.csv\n",
      "           1         2          3         4         5         6          7  \\\n",
      "A  -9.166728 -2.097215  -9.920280 -0.575669  0.667777  0.296466  -5.930836   \n",
      "C -13.100713 -6.827145 -10.550521 -3.247884 -4.429783 -3.224222  -9.379293   \n",
      "D -10.480780 -1.117165 -10.560699 -0.848824 -0.862386 -1.562334 -11.379516   \n",
      "E -10.749167  0.000000 -10.289966 -1.193693  0.031768 -0.759001 -11.397455   \n",
      "F -11.877597 -5.429612 -11.692527  0.090257 -1.318970 -1.398810  -2.411075   \n",
      "\n",
      "           8         9         10  ...       376       377       378  \\\n",
      "A  -9.246613  1.057769  -0.702628  ...  0.000000  1.194720 -0.372994   \n",
      "C -11.634896 -2.564589  -3.731435  ... -2.737524 -2.253685 -3.570410   \n",
      "D   0.000000 -4.889423 -10.776357  ... -1.297799 -0.828794 -1.876800   \n",
      "E  -6.784249 -4.383034  -9.082147  ... -1.619041 -0.281448 -1.800139   \n",
      "F -11.947284  0.000000  -2.607516  ... -1.018799  0.078803 -1.486748   \n",
      "\n",
      "        379       380       381       382       383       384       385  \n",
      "A  0.000000  1.183373  0.117575  0.705800  0.272296  0.122079  0.693832  \n",
      "C -3.396112 -2.563263 -3.550587 -3.154248 -3.261396 -3.144457 -2.149773  \n",
      "D -1.728592 -0.603946 -1.625650 -1.077209 -1.274096 -1.079665 -0.794661  \n",
      "E -1.532764 -0.423668 -0.955200 -0.614634 -0.810930 -0.921540 -0.813040  \n",
      "F -0.737671 -0.445406 -2.126858 -2.062745 -1.821646 -2.330093 -1.717226  \n",
      "\n",
      "[5 rows x 385 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to topA_esm_finetuned_full_sequence.csv ...\n",
      "Saved LLR data to topA_esm_finetuned_full_sequence.csv\n",
      "           1          2          3          4          5          6  \\\n",
      "A -12.779844  -7.298015 -14.929070   0.000000 -14.545280 -11.582582   \n",
      "C -14.761004  -7.038578 -14.890189 -16.924612 -15.846489 -17.139296   \n",
      "D -13.032188  -9.205082 -15.501353 -13.328996 -14.657400 -16.272467   \n",
      "E -13.902396 -11.525482 -10.351316 -12.801946 -16.209209 -15.983733   \n",
      "F -13.901359 -10.212706 -15.445551 -15.218396  -7.583661 -11.925360   \n",
      "\n",
      "           7          8          9         10  ...        856        857  \\\n",
      "A -16.895714  -9.230031 -13.020461 -11.233185  ... -10.906979 -11.134296   \n",
      "C -18.132303 -15.303146 -16.916100 -12.112461  ... -16.059731 -13.621986   \n",
      "D -15.768726 -12.461180 -12.744906 -13.917280  ... -13.334499   0.000000   \n",
      "E -18.150625 -12.136307   0.000000 -15.687500  ... -13.513004  -9.496721   \n",
      "F -11.974319 -10.521366 -18.716763 -11.475351  ... -11.086101 -14.784973   \n",
      "\n",
      "         858        859        860        861        862        863  \\\n",
      "A -11.732432 -10.885887 -11.136165  -7.564677  -9.355779 -12.038752   \n",
      "C -10.321706 -12.853507 -12.308384 -12.194142 -14.499911  -8.344733   \n",
      "D  -8.524298 -11.518671 -14.688570  -8.676602  -4.800743  -7.994583   \n",
      "E -10.283512  -7.404202 -12.746516  -9.534725   0.000000  -7.854495   \n",
      "F -14.223430 -13.542183 -13.524865  -7.988207 -13.086505  -9.145913   \n",
      "\n",
      "         864        865  \n",
      "A -12.086009 -12.881261  \n",
      "C -13.884915 -12.225576  \n",
      "D -12.811740 -12.656313  \n",
      "E  -7.186748  -7.758637  \n",
      "F -12.236419 -13.017844  \n",
      "\n",
      "[5 rows x 865 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to spoT_esm_finetuned_full_sequence.csv ...\n",
      "Saved LLR data to spoT_esm_finetuned_full_sequence.csv\n",
      "           1          2          3          4          5          6  \\\n",
      "A -11.381110 -14.865465 -13.987061 -14.248776  -8.213952 -11.167278   \n",
      "C -11.275842  -9.968287 -13.466621  -9.718686 -15.474810  -9.526658   \n",
      "D -11.320039  -9.651106 -13.207526 -14.064723  -8.942080 -12.448570   \n",
      "E -12.256473 -13.758333 -14.435894 -15.059281   0.000000 -14.948088   \n",
      "F -10.537523 -11.189932  -9.440500   0.000000 -15.119246 -11.133164   \n",
      "\n",
      "           7          8          9         10  ...        693        694  \\\n",
      "A -13.696785 -10.813976 -10.624034 -17.371183  ...  -8.851665 -10.435548   \n",
      "C -14.170267 -13.445740 -13.590317 -15.773581  ... -11.045583 -16.991594   \n",
      "D -13.731606  -9.495364 -11.281090 -17.858093  ...   0.000000 -14.140461   \n",
      "E -14.040543 -12.807199  -7.318578 -18.237171  ...  -9.066815 -13.237170   \n",
      "F  -8.102594 -14.346256 -13.659965  -9.822090  ... -11.881155 -12.646526   \n",
      "\n",
      "         695        696        697        698        699        700  \\\n",
      "A -14.811543 -14.035637  -9.876108  -9.451729 -13.318530 -13.603023   \n",
      "C -14.871760 -13.139368 -17.615010 -17.129220 -10.455067 -17.578514   \n",
      "D -13.730220 -13.255928 -13.163082 -15.365293 -14.838955 -14.300183   \n",
      "E -17.403282  -8.073571 -11.527223 -16.973073 -16.233876 -17.537361   \n",
      "F -10.523916 -12.296329 -14.210328 -14.605951 -14.803939 -15.542646   \n",
      "\n",
      "         701        702  \n",
      "A -13.409424 -11.947318  \n",
      "C  -9.510030 -14.160453  \n",
      "D -12.534059 -13.484105  \n",
      "E -11.880670 -14.449263  \n",
      "F -13.448242 -13.815375  \n",
      "\n",
      "[5 rows x 702 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to yeiB_esm_finetuned_full_sequence.csv ...\n",
      "Saved LLR data to yeiB_esm_finetuned_full_sequence.csv\n",
      "           1          2          3          4          5          6  \\\n",
      "A -12.084817  -8.626104 -12.447071 -12.835382  -8.386174 -10.810058   \n",
      "C -14.429861 -15.409209  -6.573661 -14.914764 -14.330826 -14.275341   \n",
      "D -13.324515  -7.886947 -12.752005 -10.812071 -12.155093 -12.045685   \n",
      "E -13.964245   0.000000 -13.687492 -14.672333 -10.971349 -13.478183   \n",
      "F -13.674183 -15.806313 -12.658273 -11.665919  -7.280460 -11.816262   \n",
      "\n",
      "           7          8          9         10  ...        376        377  \\\n",
      "A -13.399755 -10.012448  -9.145188 -10.100629  ...   0.000000 -10.137898   \n",
      "C -15.435852 -12.754211  -7.137099 -13.575201  ... -13.252833 -13.017585   \n",
      "D -14.271572   0.000000 -10.998780 -14.684176  ... -11.366223  -8.366792   \n",
      "E -15.056403  -9.707147 -10.630239 -12.086983  ... -10.171751  -6.359147   \n",
      "F  -7.120547 -11.814632   0.000000  -7.538529  ... -12.436873 -13.495989   \n",
      "\n",
      "         378        379        380        381        382        383  \\\n",
      "A  -9.272586   0.000000  -9.582837  -9.346233 -13.055247  -7.141409   \n",
      "C -10.712273 -13.933790 -11.789280 -12.812573 -14.690169 -12.828452   \n",
      "D -12.097689 -10.695050 -11.053711 -12.267319 -14.302419 -11.779069   \n",
      "E -13.133704 -10.295036 -12.332330 -16.291291  -8.750082 -13.545672   \n",
      "F -11.501078 -12.757359 -10.622139  -6.406653 -14.691776 -13.685283   \n",
      "\n",
      "         384        385  \n",
      "A  -9.081897 -10.379475  \n",
      "C  -9.200259  -9.362874  \n",
      "D -14.099932 -11.465207  \n",
      "E -13.401034 -10.829326  \n",
      "F  -9.267947 -13.028050  \n",
      "\n",
      "[5 rows x 385 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to topA_progen_pretrained_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to topA_progen_pretrained_full_sequence.csv\n",
      "     1         2         3         4         5         6         7         8  \\\n",
      "A  0.0  0.162033  0.865013  0.000000 -0.063782  0.653915  0.805450  0.628319   \n",
      "C  0.0 -1.639683 -0.967049 -1.819008 -2.251579 -1.683861 -1.524506 -1.458672   \n",
      "D  0.0 -0.332775  0.311302 -0.395699 -0.909370 -0.136307  0.353058  0.045341   \n",
      "E  0.0 -0.204273  0.363480 -0.107841 -0.656586  0.190094  0.532310 -0.069893   \n",
      "F  0.0 -0.768089  0.053177 -0.776680 -0.995384 -0.720718 -0.407455 -0.847054   \n",
      "\n",
      "          9        10  ...       856       857       858       859       860  \\\n",
      "A  0.568733  0.094154  ... -0.023674 -2.231262 -2.319237 -5.333511 -3.570091   \n",
      "C -1.566101 -2.287689  ... -2.956917 -5.911057 -8.756405 -8.939987 -6.749634   \n",
      "D  0.442558 -0.013115  ... -1.569420  0.000000 -3.703781 -6.085426 -5.431808   \n",
      "E  0.000000 -0.263962  ... -0.656273 -0.917030 -2.755089 -3.496079 -3.830383   \n",
      "F -0.719597 -1.373283  ... -2.144150 -2.764732 -6.936173 -8.422768 -2.852158   \n",
      "\n",
      "        861       862       863       864       865  \n",
      "A -1.396027  2.173645 -0.103783 -1.134193  0.535019  \n",
      "C -4.613655 -4.463600 -3.841469 -5.311081 -3.711868  \n",
      "D -3.232277 -0.718651 -0.610741 -1.390472  2.040550  \n",
      "E -0.642616  0.000000  0.403389 -0.430069  1.377441  \n",
      "F -1.500412 -5.599533 -1.836083 -4.470688 -3.020271  \n",
      "\n",
      "[5 rows x 865 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to spoT_progen_pretrained_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to spoT_progen_pretrained_full_sequence.csv\n",
      "     1         2         3         4         5         6         7         8  \\\n",
      "A  0.0  1.233402 -0.265938  0.764915  0.396950  0.108307 -0.453125  0.459778   \n",
      "C  0.0 -0.568313 -1.620514 -1.068218 -1.042923 -1.763169 -2.111443 -1.242622   \n",
      "D  0.0  0.738594 -0.513771  0.278503  0.047195 -0.124107 -0.765297  0.230621   \n",
      "E  0.0  0.867096 -0.289658  0.362305  0.000000  0.015808 -0.692017  0.231796   \n",
      "F  0.0  0.303280 -0.607788  0.000000  0.073517 -0.661858 -0.792824 -0.049385   \n",
      "\n",
      "          9        10  ...       693        694       695       696  \\\n",
      "A  0.451233 -0.403908  ... -3.179615  -5.862175 -3.632198 -2.464977   \n",
      "C -1.389724 -2.102058  ... -6.050728  -5.432465 -6.548557 -4.810413   \n",
      "D  0.307007 -0.816437  ...  0.000000 -10.117122 -7.195232 -4.430420   \n",
      "E  0.589661 -0.563194  ... -0.671764 -10.781766 -4.615055 -2.847164   \n",
      "F  0.039040 -0.829666  ... -7.530109  -8.982940 -2.853268 -5.860744   \n",
      "\n",
      "         697       698        699       700       701       702  \n",
      "A  -2.976074 -2.436920 -12.392780 -0.808372 -3.508507  0.056290  \n",
      "C  -5.076073 -4.485275 -12.925186 -2.084343 -8.608025 -5.408775  \n",
      "D -12.283683 -4.298020 -14.266388 -2.035919 -5.513405  0.737144  \n",
      "E  -9.776348 -2.892338 -12.828690 -0.779137 -4.510185  0.949501  \n",
      "F  -6.412613 -3.425938 -14.923962 -0.207027 -6.614014 -2.228134  \n",
      "\n",
      "[5 rows x 702 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to yeiB_progen_pretrained_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to yeiB_progen_pretrained_full_sequence.csv\n",
      "     1         2         3         4         5         6         7         8  \\\n",
      "A  0.0  0.366306  0.188721  0.900917  0.138046  0.454865 -0.131882  0.480278   \n",
      "C  0.0 -1.435409 -1.776050 -0.922676 -1.735283 -1.594337 -2.105972 -1.459045   \n",
      "D  0.0 -0.128502 -0.196175  0.399284 -0.381645 -0.028679 -0.589867  0.000000   \n",
      "E  0.0  0.000000  0.133690  0.692352 -0.018745  0.248070 -0.331383  0.134644   \n",
      "F  0.0 -0.563816 -0.728523  0.171730 -0.407036 -0.220871 -0.828316 -0.380844   \n",
      "\n",
      "          9        10  ...       376       377       378       379       380  \\\n",
      "A  0.758049  0.201233  ...  0.000000 -0.110245  0.355965  0.000000  2.332077   \n",
      "C -1.253380 -1.782982  ... -3.642471 -3.220299 -2.839043 -4.554871 -2.015877   \n",
      "D  0.484756  0.041954  ... -1.643654 -1.571472 -1.549858 -2.442154  0.276619   \n",
      "E  0.644112 -0.086548  ... -0.585411 -1.056007 -0.435928 -1.253693  1.369362   \n",
      "F  0.000000 -0.669952  ... -1.868225 -2.284859 -1.462181 -2.401604 -0.431053   \n",
      "\n",
      "        381       382       383       384       385  \n",
      "A  0.516953  0.787331  0.699699  0.299110  1.023735  \n",
      "C -3.740189 -3.091652 -3.384468 -3.686592 -2.961921  \n",
      "D -0.858635 -1.109840 -0.739700 -1.196884 -0.812141  \n",
      "E -0.314224 -0.308655 -0.244263 -0.520424 -0.259346  \n",
      "F -3.110390 -2.028839 -2.379257 -2.735588 -2.191757  \n",
      "\n",
      "[5 rows x 385 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to topA_progen_finetuned_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to topA_progen_finetuned_full_sequence.csv\n",
      "     1         2          3          4          5          6          7  \\\n",
      "A  0.0 -8.090754 -12.177975   0.000000 -12.397565 -11.312858 -14.786577   \n",
      "C  0.0 -7.856789 -15.083452 -10.153616 -13.745123 -12.139774 -18.146718   \n",
      "D  0.0 -6.714582 -20.137194 -11.781612 -13.427336 -13.287996 -16.430695   \n",
      "E  0.0 -0.195053 -18.521841 -10.028303 -14.604472 -14.793973 -16.891739   \n",
      "F  0.0 -8.055485 -13.203379 -11.845667  -7.344830 -11.631298 -11.400804   \n",
      "\n",
      "           8          9         10  ...        856        857        858  \\\n",
      "A  -8.132900 -13.369621 -11.855781  ...  -8.558590 -10.059429  -9.947719   \n",
      "C -14.055206 -17.909549 -13.530254  ... -12.658306  -9.813446  -9.047848   \n",
      "D -12.273172 -11.088882 -14.817512  ... -13.893154   0.000000  -7.573093   \n",
      "E -13.352261   0.000000 -16.085682  ... -13.976376  -8.885616  -9.562324   \n",
      "F -13.275312 -14.787956 -13.483707  ... -13.160648 -10.622768 -12.084587   \n",
      "\n",
      "         859        860       861        862        863        864        865  \n",
      "A -12.201435 -11.430386 -8.243412 -11.650638 -10.928387 -12.606480 -11.266961  \n",
      "C -14.305260 -11.830231 -9.784603 -18.925110  -9.720646 -12.624581 -12.524739  \n",
      "D -11.891659 -13.058167 -9.065159 -10.617012  -8.247666 -12.470562  -9.344597  \n",
      "E  -7.774414 -11.016743 -9.633148   0.000000  -7.211349  -8.236176  -7.769119  \n",
      "F -11.910614 -10.253201 -8.484623 -17.896105 -11.890755 -11.737801 -11.533062  \n",
      "\n",
      "[5 rows x 865 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to spoT_progen_finetuned_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to spoT_progen_finetuned_full_sequence.csv\n",
      "     1         2          3          4          5          6          7  \\\n",
      "A  0.0 -7.308589  -8.794033 -10.508235  -6.578985 -10.952065 -11.345878   \n",
      "C  0.0 -7.074624 -10.259121 -10.203530 -10.728209  -7.325447 -15.549507   \n",
      "D  0.0 -5.932417 -10.766289 -16.823017  -6.972574 -12.536770 -12.974895   \n",
      "E  0.0  0.587112  -5.941482 -12.084822   0.000000 -13.991086 -11.115474   \n",
      "F  0.0 -7.273320  -7.382503   0.000000  -7.862202  -8.830960  -9.775127   \n",
      "\n",
      "           8          9         10  ...        693        694        695  \\\n",
      "A -10.746350  -8.960247 -12.557201  ...  -9.959335 -10.243809 -15.803295   \n",
      "C -11.744889 -10.568627 -12.920849  ... -12.635368 -13.360207 -19.964801   \n",
      "D -10.800789 -10.455769 -14.333847  ...   0.000000 -12.386112 -18.256759   \n",
      "E -11.999699  -7.678974 -15.727577  ...  -5.700012 -15.481575 -19.461929   \n",
      "F -10.996796  -9.099881  -9.779606  ... -11.278064 -14.077850 -15.550979   \n",
      "\n",
      "         696        697        698        699        700        701        702  \n",
      "A -11.580410  -9.869789 -10.472732 -16.173138 -13.418534 -14.630970 -10.600130  \n",
      "C -14.687015 -12.147895 -11.807217 -14.721874  -9.704524 -11.380901 -11.409561  \n",
      "D -14.077808 -17.777798 -15.301739 -15.658413 -12.061765 -17.851806 -10.378803  \n",
      "E  -9.468513 -15.685440 -17.877090 -14.366192 -16.380889 -18.755296 -16.572134  \n",
      "F -15.151508 -17.264943 -14.480179 -15.181252 -18.236806 -13.037700 -13.884689  \n",
      "\n",
      "[5 rows x 702 columns] \n",
      "\n",
      "Saving full‐sequence LLRs to yeiB_progen_finetuned_full_sequence.csv ...\n",
      "Error at pos 1: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "Saved LLR data to yeiB_progen_finetuned_full_sequence.csv\n",
      "     1         2          3          4          5          6          7  \\\n",
      "A  0.0 -7.895701  -8.584389 -11.623505  -8.013333  -9.767327 -11.884884   \n",
      "C  0.0 -7.661736  -8.266407 -12.559153 -10.111142 -13.563233 -12.265636   \n",
      "D  0.0 -6.519529 -14.593344 -12.212040 -12.309672 -13.070095 -13.309170   \n",
      "E  0.0  0.000000 -10.506176 -12.036656 -12.558667 -13.616898 -14.266998   \n",
      "F  0.0 -7.860432  -9.237028 -10.504538  -6.944252 -11.839489  -6.662373   \n",
      "\n",
      "           8          9         10  ...        376        377        378  \\\n",
      "A  -9.413433 -10.453308  -8.870953  ...   0.000000 -10.090042  -8.111687   \n",
      "C -13.251171  -7.401895 -12.898876  ... -10.858757 -12.052467 -12.076141   \n",
      "D   0.000000 -12.182541 -13.410290  ... -12.997925  -9.139881 -12.550056   \n",
      "E -10.452484 -13.099697 -13.109043  ... -11.353019  -7.650005 -10.434586   \n",
      "F  -8.909386   0.000000  -7.579422  ... -14.045196 -15.083054 -11.596443   \n",
      "\n",
      "         379        380        381        382        383        384        385  \n",
      "A   0.000000 -12.953569 -10.808837 -13.027960  -8.317607 -10.217248 -10.176865  \n",
      "C -13.088249 -13.134195 -12.821714 -18.944666 -13.943946 -10.678373  -8.456230  \n",
      "D -10.216972 -16.842862  -9.125161 -13.550587 -11.751510 -12.426084 -10.437374  \n",
      "E  -9.711624 -16.701632 -12.586515 -11.821685 -13.129188 -13.553486 -11.964703  \n",
      "F -12.125351 -10.935493  -7.691729 -15.161084 -12.463272 -12.635182 -11.661583  \n",
      "\n",
      "[5 rows x 385 columns] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForMaskedLM, \n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# ─── Your existing imports & data ─────────────────────────────────────\n",
    "# (you already ran these)\n",
    "from autoamp.evolveFinetune import *  # if needed\n",
    "from Bio import SeqIO\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Sequences & mutations\n",
    "topA_seq = (\n",
    "    \"MGKALVIVESPAKAKTINKYLGSDYVVKSSVGHIRDLPTSGSAAKKSADSTSTKTAKKPK\" \n",
    "    \"KDERGALVNRMGVDPWHNWEAHYEVLPGKEKVVSELKQLAEKADHIYLATDLDREGEAIA\" \n",
    "    \"WHLREVIGGDDARYSRVVFNEITKNAIRQAFNKPGELNIDRVNAQQARRFMDRVVGYMVS\" \n",
    "    \"PLLWKKIARGLSAGRVQSVAVRLVVEREREIKAFVPEEFWEVDASTTTPSGEALALQVTH\" \n",
    "    \"QNDKPFRPVNKEQTQAAVSLLEKARYSVLEREDKPTTSKPGAPFITSTLQQAASTRLGFG\" \n",
    "    \"VKKTMMMAQRLYEAGYITYMRTDSTNLSQDAVNMVRGYISDNFGKKYLPESPNQYASKEN\" \n",
    "    \"SQEAHEAIRPSDVNVMAESLKDMEADAQKLYQLIWRQFVACQMTPAKYDSTTLTVGAGDF\" \n",
    "    \"RLKARGRILRFDGWTKVMPALRKGDEDRILPAVNKGDALTLVELTPAQHFTKPPARFSEA\" \n",
    "    \"SLVKELEKRGIGRPSTYASIISTIQDRGYVRVENRRFYAEKMGEIVTDRLEENFRELMNY\" \n",
    "    \"DFTAQMENSLDQVANHEAEWKAVLDHFFSDFTQQLDKAEKDPEEGGMRPNQMVLTSIDCP\" \n",
    "    \"TCGRKMGIRTASTGVFLGCSGYALPPKERCKTTINLVPENEVLNVLEGEDAETNALRAKR\" \n",
    "    \"RCPKCGTAMDSYLIDPKRKLHVCGNNPTCDGYEIEEGEFRIKGYDGPIVECEKCGSEMHL\" \n",
    "    \"KMGRFGKYMACTNEECKNTRKILRNGEVAPPKEDPVPLPELPCEKSDAYFVLRDGAAGVF\" \n",
    "    \"LAANTFPKSRETRAPLVEELYRFRDRLPEKLRYLADAPQQDPEGNKTMVRFSRKTKQQYV\" \n",
    "    \"SSEKDGKATGWSAFYVDGKWVEGKK\" \n",
    ")\n",
    "\n",
    "spoT_seq = (\n",
    "        \"MYLFESLNQLIQTYLPEDQIKRLRQAYLVARDAHEGQTRSSGEPYITHPVAVACILAEMK\"\n",
    "        \"LDYETLMAALLHDVIEDTPATYQDMEQLFGKSVAELVEGVSKLDKLKFRDKKEAQAENFR\"\n",
    "        \"KMIMAMVQDIRVILIKLADRTHNMRTLGSLRPDKRRRIARETLEIYSPLAHRLGIHHIKT\"\n",
    "        \"ELEELGFEALYPNRYRVIKEVVKAARGNRKEMIQKILSEIEGRLQEAGIPCRVSGREKHL\"\n",
    "        \"YSIYCKMVLKEQRFHSIMDIYAFRVIVNDSDTCYRVLGQMHSLYKPRPGRVKDYIAIPKA\"\n",
    "        \"NGYQSLHTSMIGPHGVPVEVQIRTEDMDQMAEMGVAAHWAYKEHGETSTTAQIRAQRWMQ\"\n",
    "        \"SLLELQQSAGSSFEFIESVKSDLFPDEIYVFTPEGRIVELPAGATPVDFAYAVHTDIGHA\"\n",
    "        \"CVGARVDRQPYPLSQPLTSGQTVEIITAPGARPNAAWLNFVVSSKARAKIRQLLKNLKRD\"\n",
    "        \"DSVSLGRRLLNHALGGSRKLNEIPQENIQRELDRMKLATLDDLLAEIGLGNAMSVVVAKN\"\n",
    "        \"LQHGDASIPPATQSHGHLPIKGADGVLITFAKCCRPIPGDPIIAHVSPGKGLVIHHESCR\"\n",
    "        \"NIRGYQKEPEKFMAVEWDKETAQEFITEIKVEMFNHQGALANLTAAINTTTSNIQSLNTE\"\n",
    "        \"EKDGRVYSAFIRLTARDRVHLANIMRKIRVMPDVIKVTRNRN\")\n",
    "\n",
    "yeiB_seq = (\"MERNVTLDFVRGVAILGILLLNISAFGLPKAAYLNPAWYGAITPRDAWTWAFLDLIGQVK\"\n",
    "        \"FLTLFALLFGAGLQMLLPRGRRWIQSRLTLLVLLGFIHGLLFWDGDILLAYGLVGLICWR\"\n",
    "        \"LVRDAPSVKSLFNTGVMLYLVGLGVLLLLGLISDSQTSRAWTPDASAILYEKYWKLHGGV\"\n",
    "        \"EAISNRADGVGNSLLALGAQYGWQLAGMMLIGAALMRSGWLKGQFSLRHYRRTGFVLVAI\"\n",
    "        \"GVTINLPAIALQWQLDWAYRWCAFLLQMPRELSAPFQAIGYASLFYGFWPQLSRFKLVLA\"\n",
    "        \"IACVGRMALTNYLLQTLICTTLFYHLGLFMHFDRLELLAFVIPVWLANILFSVIWLRYFR\"\n",
    "        \"QGPVEWLWRQLTLRAAGPAISKTSR\")\n",
    "\n",
    "gene_seq = {\n",
    "    \"topA\": topA_seq,\n",
    "    \"spoT\": spoT_seq,\n",
    "    \"yeiB\": yeiB_seq\n",
    "}\n",
    "gene_mutation = {\"topA\":\"H33Y\", \"spoT\":\"K662I\", \"yeiB\":\"L143I\"}\n",
    "\n",
    "# ─── 1. ESM‑2 models (masked LM) ────────────────────────────────────────\n",
    "esm_name = \"facebook/esm2_t30_150M_UR50D\"\n",
    "esm_tok  = AutoTokenizer.from_pretrained(esm_name, use_fast=True)\n",
    "\n",
    "# pretrained\n",
    "esm_pre   = AutoModelForMaskedLM.from_pretrained(esm_name)\n",
    "esm_pre_gen = HeatmapGenerator(model=esm_pre, tokenizer=esm_tok)\n",
    "\n",
    "# finetuned via PEFT adapter\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "esm_base   = AutoModelForMaskedLM.from_pretrained(esm_name)\n",
    "esm_ft     = PeftModel.from_pretrained(esm_base, adapter_checkpoint)\n",
    "esm_ft_gen = HeatmapGenerator(model=esm_ft, tokenizer=esm_tok)\n",
    "\n",
    "# ─── 2. ProGen2 models (causal LM) ─────────────────────────────────────\n",
    "progen_name = \"hugohrban/progen2-small\"\n",
    "progen_tok  = AutoTokenizer.from_pretrained(progen_name, trust_remote_code=True)\n",
    "\n",
    "# pretrained ProGen2\n",
    "progen_pre   = AutoModelForCausalLM.from_pretrained(progen_name, trust_remote_code=True)\n",
    "progen_pre_gen = HeatmapGenerator(model=progen_pre, tokenizer=progen_tok, is_progen=True)\n",
    "\n",
    "# finetuned ProGen2 via PEFT (replace with your path)\n",
    "progen_adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/progen2_151m_ecoli_finetuning_1/checkpoint-11500/\"\n",
    "progen_base = AutoModelForCausalLM.from_pretrained(progen_name, trust_remote_code=True)\n",
    "progen_ft   = PeftModel.from_pretrained(progen_base, progen_adapter_checkpoint)\n",
    "progen_ft_gen = HeatmapGenerator(model=progen_ft, tokenizer=progen_tok, is_progen=True)\n",
    "\n",
    "# ── Loop & save CSVs across the full sequence ─────────────────────────\n",
    "for model_label, gen in [\n",
    "    (\"esm_pretrained\",    esm_pre_gen),\n",
    "    (\"esm_finetuned\",     esm_ft_gen),\n",
    "    (\"progen_pretrained\", progen_pre_gen),\n",
    "    (\"progen_finetuned\",  progen_ft_gen),\n",
    "]:\n",
    "    for gene, seq in gene_seq.items():\n",
    "        filename = f\"{gene}_{model_label}_full_sequence.csv\"\n",
    "        print(f\"Saving full‐sequence LLRs to {filename} ...\")\n",
    "        # omit start_pos/end_pos → defaults to entire sequence\n",
    "        df = gen.save_heatmap_data(\n",
    "            protein_sequence=seq,\n",
    "            filename=filename\n",
    "        )\n",
    "        # (optional) inspect the head\n",
    "        print(df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f2ce3-96af-4278-b400-faee057e1630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

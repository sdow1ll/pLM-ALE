{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df7daa2-bea0-4953-943b-9c82ab30922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sdowell/scratch/Thesis/ADP1\n"
     ]
    }
   ],
   "source": [
    "cd /home/sdowell/scratch/Thesis/ADP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba1885f9-0df5-4358-a611-0ecd256d98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained MLM Cross Entropy Loss: 1.6485\n",
      "Finetuned MLM Cross Entropy Loss: 0.0446\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_mlm_loss(model, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes the MLM (masked language model) cross entropy loss for a given sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence as a string.\n",
    "        mask_prob: The probability of masking a token.\n",
    "        device: torch.device to run the computation.\n",
    "    \n",
    "    Returns:\n",
    "        loss: The MLM cross entropy loss.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()  # or model.eval() if you don't want dropout, etc.\n",
    "    \n",
    "    # Tokenize the sequence\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    \n",
    "    # Create labels as a copy of input_ids.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a mask for positions to replace.\n",
    "    # Generate random values in [0, 1) for each token.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    # Create a boolean mask for tokens to mask.\n",
    "    mask = probability_matrix < mask_prob\n",
    "    \n",
    "    # For positions NOT selected for masking, set the corresponding label to -100 so they are ignored.\n",
    "    labels[~mask] = -100\n",
    "    \n",
    "    # Replace the selected input positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    input_ids[mask] = mask_token_id\n",
    "    \n",
    "    # Forward pass: the model automatically computes the loss when labels are provided.\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sequence = \"MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKALIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEAGAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLYRAGQSVERTAQQAAAFVKAYREAVQ\"\n",
    "loss = compute_mlm_loss(model_pretrained, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Pretrained MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "loss = compute_mlm_loss(model_finetuned, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Finetuned MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26a30a0f-9316-407f-91e6-f4449249036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained Token Prediction Accuracy: 0.5000 (10/20)\n",
      "finetuned Token Prediction Accuracy: 1.0000 (27/27)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability with which to mask tokens (e.g., 0.15 for 15%).\n",
    "        device: torch.device to run the computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (accuracy, correct, total_masked) where:\n",
    "          - accuracy is the fraction of masked tokens correctly predicted.\n",
    "          - correct is the number of correct predictions.\n",
    "          - total_masked is the total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of the original tokens to serve as labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens according to mask_prob.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace the selected token positions in input_ids with the mask token ID.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    \n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch_size, sequence_length, vocab_size]\n",
    "    \n",
    "    # Get predictions (top candidate from the logits) using argmax.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Consider only the masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    # Calculate the number of correct predictions.\n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    accuracy = correct / total_masked if total_masked > 0 else 0.0\n",
    "    return accuracy, correct, total_masked\n",
    "\n",
    "sequence = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_pretrained, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"pretrained Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_finetuned, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"finetuned Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aea29c6-a2e4-4dbf-ac88-f3b2d5257ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [00:54<00:00, 35.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "pretrained Overall Token Prediction Accuracy: 0.5505 (33014/59970)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [01:12<00:00, 26.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "finetuned Overall Token Prediction Accuracy: 0.9834 (58786/59779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a given sequence from a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability of masking a token (default is 15%).\n",
    "        device: Torch device on which to run computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (correct, total_masked) where:\n",
    "          - correct: number of masked tokens correctly predicted.\n",
    "          - total_masked: total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of input_ids for ground-truth labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace tokens at masked positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch, seq_length, vocab_size]\n",
    "    \n",
    "    # Get predicted token IDs.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Evaluate only on masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    return correct, total_masked\n",
    "\n",
    "# ----- Main script to process FASTA file with a progress bar -----\n",
    "\n",
    "fasta_file = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/dataset_splits/finetuning_dataset/test.fasta\"\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"pretrained Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n",
    "\n",
    "\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"finetuned Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f78f8-00ae-4cbe-828d-ab1a936162aa",
   "metadata": {},
   "source": [
    "# ESM-2 Pretrained Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bb4e5b-2000-4c09-8e67-43ec29d09898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 1953it [00:55, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       318\n",
      "       <eos>  1.0000000000 1.0000000000 1.0000000000       271\n",
      "           A  0.5720624627 0.6826188779 0.6224698236      9821\n",
      "           C  0.0000000000 0.0000000000 0.0000000000      1119\n",
      "           D  0.3596673597 0.3502024291 0.3548717949      2470\n",
      "           E  0.5634062747 0.7229364005 0.6332790043      2956\n",
      "           F  0.3867791842 0.2958579882 0.3352636391      1859\n",
      "           G  0.9094237811 0.8600328048 0.8840389659      5487\n",
      "           H  0.0250000000 0.0073260073 0.0113314448       546\n",
      "           I  0.5677771396 0.4785766158 0.5193747537      4131\n",
      "           K  0.3681233933 0.3232505643 0.3442307692      2215\n",
      "           L  0.4487759239 0.6179944170 0.5199638663      4657\n",
      "           M  0.9002036660 0.4799131379 0.6260623229       921\n",
      "           N  0.6700167504 0.6768189509 0.6734006734      1182\n",
      "           P  0.6564500485 0.8521782926 0.7416173570      3971\n",
      "           Q  0.3435326843 0.0739520958 0.1217048534      3340\n",
      "           R  0.2833669645 0.4797462177 0.3562885103      2049\n",
      "           S  0.5803680982 0.4088159032 0.4797160243      2314\n",
      "           T  0.5168759812 0.4723816356 0.4936281859      2788\n",
      "           V  0.4588412089 0.5341107872 0.4936231363      5145\n",
      "           W  0.2736842105 0.0618311534 0.1008729389       841\n",
      "           X  0.0000000000 0.0000000000 0.0000000000        14\n",
      "           Y  0.5652936021 0.5226904376 0.5431578947      1234\n",
      "\n",
      "    accuracy                      0.5482573052     59649\n",
      "   macro avg  0.4978108145 0.4739667268 0.4719519982     59649\n",
      "weighted avg  0.5324963214 0.5482573052 0.5280149080     59649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_token_predictions_for_metrics(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Returns token-level predictions and labels for masked positions, to support precision/recall/F1.\n",
    "    \n",
    "    Returns:\n",
    "        Two lists: true token IDs and predicted token IDs at masked positions.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "\n",
    "    # Ensure mask token is defined\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    true_tokens = labels[mask_positions].tolist()\n",
    "    predicted_tokens = predictions[mask_positions].tolist()\n",
    "\n",
    "    return true_tokens, predicted_tokens\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55db1d-01aa-4414-8985-5cc05aee1d15",
   "metadata": {},
   "source": [
    "# ESM-2 fine-tuned Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b313046c-6cfc-44fa-86fa-4f760dce7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 1953it [01:16, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       286\n",
      "       <eos>  1.0000000000 0.9667774086 0.9831081081       301\n",
      "           A  0.9853016754 0.9881455520 0.9867215646      9701\n",
      "           C  0.9948186528 0.9956784788 0.9952483801      1157\n",
      "           D  0.9659591510 0.9824847251 0.9741518578      2455\n",
      "           E  0.9846461949 0.9836612204 0.9841534612      2999\n",
      "           F  0.9635274905 0.9440000000 0.9536637931      1875\n",
      "           G  0.9931072012 0.9969045885 0.9950022717      5492\n",
      "           H  0.9937888199 0.8695652174 0.9275362319       552\n",
      "           I  0.9922923918 0.9760332600 0.9840956725      4089\n",
      "           K  0.9804010939 0.9804010939 0.9804010939      2194\n",
      "           L  0.9963814389 0.9948990436 0.9956396895      4705\n",
      "           M  0.9893617021 0.9779179811 0.9836065574       951\n",
      "           N  0.9872122762 0.9460784314 0.9662077597      1224\n",
      "           P  0.9892583120 0.9933230611 0.9912865197      3894\n",
      "           Q  0.9685420448 0.9925604464 0.9804041641      3226\n",
      "           R  0.9860423233 0.9869310500 0.9864864865      2219\n",
      "           S  0.9788519637 0.9717223650 0.9752741346      2334\n",
      "           T  0.9866858582 0.9764957265 0.9815643458      2808\n",
      "           V  0.9654064272 0.9830606352 0.9741535527      5195\n",
      "           W  0.9977452086 0.9988713318 0.9983079526       886\n",
      "           X  0.8750000000 0.4666666667 0.6086956522        15\n",
      "           Y  0.9674981104 0.9846153846 0.9759817003      1300\n",
      "\n",
      "    accuracy                      0.9833606201     59858\n",
      "   macro avg  0.9800794929 0.9546432030 0.9644213457     59858\n",
      "weighted avg  0.9834266373 0.9833606201 0.9832991377     59858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2778a-5173-4c13-86e4-e6776eb2e411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe24e43-414d-415e-ae24-ff54a15af152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

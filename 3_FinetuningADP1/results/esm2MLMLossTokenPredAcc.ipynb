{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df7daa2-bea0-4953-943b-9c82ab30922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sdowell/scratch/Thesis/ADP1\n"
     ]
    }
   ],
   "source": [
    "cd /home/sdowell/scratch/Thesis/ADP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1885f9-0df5-4358-a611-0ecd256d98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 17:30:47.472310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-18 17:30:47.490832: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-18 17:30:47.490866: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-18 17:30:47.503816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-18 17:30:48.500100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained MLM Cross Entropy Loss: 1.3070\n",
      "Finetuned MLM Cross Entropy Loss: 0.0118\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_mlm_loss(model, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes the MLM (masked language model) cross entropy loss for a given sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence as a string.\n",
    "        mask_prob: The probability of masking a token.\n",
    "        device: torch.device to run the computation.\n",
    "    \n",
    "    Returns:\n",
    "        loss: The MLM cross entropy loss.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()  # or model.eval() if you don't want dropout, etc.\n",
    "    \n",
    "    # Tokenize the sequence\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    \n",
    "    # Create labels as a copy of input_ids.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a mask for positions to replace.\n",
    "    # Generate random values in [0, 1) for each token.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    # Create a boolean mask for tokens to mask.\n",
    "    mask = probability_matrix < mask_prob\n",
    "    \n",
    "    # For positions NOT selected for masking, set the corresponding label to -100 so they are ignored.\n",
    "    labels[~mask] = -100\n",
    "    \n",
    "    # Replace the selected input positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    input_ids[mask] = mask_token_id\n",
    "    \n",
    "    # Forward pass: the model automatically computes the loss when labels are provided.\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sequence = \"MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKALIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEAGAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLYRAGQSVERTAQQAAAFVKAYREAVQ\"\n",
    "loss = compute_mlm_loss(model_pretrained, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Pretrained MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "loss = compute_mlm_loss(model_finetuned, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Finetuned MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a30a0f-9316-407f-91e6-f4449249036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained Token Prediction Accuracy: 0.5000 (14/28)\n",
      "finetuned Token Prediction Accuracy: 0.9545 (21/22)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability with which to mask tokens (e.g., 0.15 for 15%).\n",
    "        device: torch.device to run the computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (accuracy, correct, total_masked) where:\n",
    "          - accuracy is the fraction of masked tokens correctly predicted.\n",
    "          - correct is the number of correct predictions.\n",
    "          - total_masked is the total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of the original tokens to serve as labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens according to mask_prob.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace the selected token positions in input_ids with the mask token ID.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    \n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch_size, sequence_length, vocab_size]\n",
    "    \n",
    "    # Get predictions (top candidate from the logits) using argmax.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Consider only the masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    # Calculate the number of correct predictions.\n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    accuracy = correct / total_masked if total_masked > 0 else 0.0\n",
    "    return accuracy, correct, total_masked\n",
    "\n",
    "sequence = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_pretrained, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"pretrained Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_finetuned, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"finetuned Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aea29c6-a2e4-4dbf-ac88-f3b2d5257ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [00:51<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "pretrained Overall Token Prediction Accuracy: 0.5534 (33101/59819)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [01:07<00:00, 28.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "finetuned Overall Token Prediction Accuracy: 0.9831 (58754/59761)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a given sequence from a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability of masking a token (default is 15%).\n",
    "        device: Torch device on which to run computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (correct, total_masked) where:\n",
    "          - correct: number of masked tokens correctly predicted.\n",
    "          - total_masked: total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of input_ids for ground-truth labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace tokens at masked positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch, seq_length, vocab_size]\n",
    "    \n",
    "    # Get predicted token IDs.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Evaluate only on masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    return correct, total_masked\n",
    "\n",
    "# ----- Main script to process FASTA file with a progress bar -----\n",
    "\n",
    "fasta_file = \"/home/sdowell/scratch/Thesis/ADP1/finetuning_data/test/dgoa_mutants_test.fasta\"\n",
    "\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"pretrained Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n",
    "\n",
    "\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"finetuned Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f78f8-00ae-4cbe-828d-ab1a936162aa",
   "metadata": {},
   "source": [
    "# ESM-2 Pretrained Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66bb4e5b-2000-4c09-8e67-43ec29d09898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 1953it [00:51, 38.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       290\n",
      "       <eos>  1.0000000000 1.0000000000 1.0000000000       296\n",
      "           A  0.5616942910 0.6894780107 0.6190607990      9732\n",
      "           C  0.0000000000 0.0000000000 0.0000000000      1166\n",
      "           D  0.3831498730 0.3662484824 0.3745085868      2471\n",
      "           E  0.5674479167 0.7239202658 0.6362043796      3010\n",
      "           F  0.3720298710 0.2870612886 0.3240685985      1909\n",
      "           G  0.9081651895 0.8520877565 0.8792332268      5652\n",
      "           H  0.0318471338 0.0097656250 0.0149476831       512\n",
      "           I  0.5869186047 0.4847539016 0.5309664694      4165\n",
      "           K  0.3742603550 0.3367346939 0.3545072396      2254\n",
      "           L  0.4505844846 0.6184621796 0.5213419989      4799\n",
      "           M  0.8672566372 0.5147058824 0.6460118655       952\n",
      "           N  0.6695880806 0.6430976431 0.6560755689      1188\n",
      "           P  0.6539114043 0.8523341523 0.7400533333      4070\n",
      "           Q  0.3581699346 0.0809453471 0.1320481928      3385\n",
      "           R  0.2778415614 0.4651609803 0.3478885894      2081\n",
      "           S  0.5485407981 0.4066225166 0.4670385396      2265\n",
      "           T  0.5221911236 0.4733599130 0.4965779468      2759\n",
      "           V  0.4659570873 0.5272414452 0.4947085202      5231\n",
      "           W  0.2037914692 0.0478309232 0.0774774775       899\n",
      "           X  0.0000000000 0.0000000000 0.0000000000        13\n",
      "           Y  0.5885328836 0.5050651230 0.5436137072      1382\n",
      "\n",
      "    accuracy                      0.5487012450     60481\n",
      "   macro avg  0.4952990739 0.4732554839 0.4720144662     60481\n",
      "weighted avg  0.5325204476 0.5487012450 0.5286501846     60481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_token_predictions_for_metrics(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Returns token-level predictions and labels for masked positions, to support precision/recall/F1.\n",
    "    \n",
    "    Returns:\n",
    "        Two lists: true token IDs and predicted token IDs at masked positions.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "\n",
    "    # Ensure mask token is defined\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    true_tokens = labels[mask_positions].tolist()\n",
    "    predicted_tokens = predictions[mask_positions].tolist()\n",
    "\n",
    "    return true_tokens, predicted_tokens\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55db1d-01aa-4414-8985-5cc05aee1d15",
   "metadata": {},
   "source": [
    "# ESM-2 fine-tuned Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b313046c-6cfc-44fa-86fa-4f760dce7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 1953it [01:07, 28.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       301\n",
      "       <eos>  1.0000000000 0.9393939394 0.9687500000       297\n",
      "       <unk>  1.0000000000 1.0000000000 1.0000000000         1\n",
      "           A  0.9831342235 0.9894917652 0.9863027495      9897\n",
      "           C  0.9927338783 0.9927338783 0.9927338783      1101\n",
      "           D  0.9618627057 0.9827727646 0.9722053155      2438\n",
      "           E  0.9808580858 0.9779532741 0.9794035261      3039\n",
      "           F  0.9685777288 0.9436090226 0.9559303591      1862\n",
      "           G  0.9952188305 0.9955849890 0.9954018760      5436\n",
      "           H  0.9910514541 0.8618677043 0.9219562955       514\n",
      "           I  0.9908844543 0.9816939224 0.9862677783      4097\n",
      "           K  0.9795454545 0.9755545496 0.9775459288      2209\n",
      "           L  0.9943181818 0.9913973982 0.9928556419      4766\n",
      "           M  0.9894291755 0.9801047120 0.9847448711       955\n",
      "           N  0.9803082192 0.9454995871 0.9625893232      1211\n",
      "           P  0.9867776690 0.9950617284 0.9909023851      4050\n",
      "           Q  0.9651128701 0.9927623643 0.9787423814      3316\n",
      "           R  0.9838945827 0.9834146341 0.9836545499      2050\n",
      "           S  0.9761698440 0.9657093871 0.9709114415      2333\n",
      "           T  0.9878408254 0.9777534646 0.9827712610      2742\n",
      "           V  0.9719539376 0.9770351008 0.9744878957      5356\n",
      "           W  0.9926470588 0.9987669544 0.9956976030       811\n",
      "           X  0.4285714286 0.3000000000 0.3529411765        10\n",
      "           Y  0.9664634146 0.9898516784 0.9780177401      1281\n",
      "\n",
      "    accuracy                      0.9824047409     60073\n",
      "   macro avg  0.9611397509 0.9474172008 0.9535339157     60073\n",
      "weighted avg  0.9824419816 0.9824047409 0.9823404513     60073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2778a-5173-4c13-86e4-e6776eb2e411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe24e43-414d-415e-ae24-ff54a15af152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

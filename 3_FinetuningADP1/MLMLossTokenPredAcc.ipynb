{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1885f9-0df5-4358-a611-0ecd256d98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained MLM Cross Entropy Loss: 1.1636\n",
      "Finetuned MLM Cross Entropy Loss: 0.0362\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_mlm_loss(model, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes the MLM (masked language model) cross entropy loss for a given sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence as a string.\n",
    "        mask_prob: The probability of masking a token.\n",
    "        device: torch.device to run the computation.\n",
    "    \n",
    "    Returns:\n",
    "        loss: The MLM cross entropy loss.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()  # or model.eval() if you don't want dropout, etc.\n",
    "    \n",
    "    # Tokenize the sequence\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    \n",
    "    # Create labels as a copy of input_ids.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a mask for positions to replace.\n",
    "    # Generate random values in [0, 1) for each token.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    # Create a boolean mask for tokens to mask.\n",
    "    mask = probability_matrix < mask_prob\n",
    "    \n",
    "    # For positions NOT selected for masking, set the corresponding label to -100 so they are ignored.\n",
    "    labels[~mask] = -100\n",
    "    \n",
    "    # Replace the selected input positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    input_ids[mask] = mask_token_id\n",
    "    \n",
    "    # Forward pass: the model automatically computes the loss when labels are provided.\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sequence = \"MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKALIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEAGAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLYRAGQSVERTAQQAAAFVKAYREAVQ\"\n",
    "loss = compute_mlm_loss(model_pretrained, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Pretrained MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "loss = compute_mlm_loss(model_finetuned, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Finetuned MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a30a0f-9316-407f-91e6-f4449249036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained Token Prediction Accuracy: 0.5161 (16/31)\n",
      "finetuned Token Prediction Accuracy: 1.0000 (32/32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability with which to mask tokens (e.g., 0.15 for 15%).\n",
    "        device: torch.device to run the computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (accuracy, correct, total_masked) where:\n",
    "          - accuracy is the fraction of masked tokens correctly predicted.\n",
    "          - correct is the number of correct predictions.\n",
    "          - total_masked is the total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of the original tokens to serve as labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens according to mask_prob.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace the selected token positions in input_ids with the mask token ID.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    \n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch_size, sequence_length, vocab_size]\n",
    "    \n",
    "    # Get predictions (top candidate from the logits) using argmax.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Consider only the masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    # Calculate the number of correct predictions.\n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    accuracy = correct / total_masked if total_masked > 0 else 0.0\n",
    "    return accuracy, correct, total_masked\n",
    "\n",
    "sequence = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_pretrained, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"pretrained Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_finetuned, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"finetuned Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70369247-35a0-426f-8f02-6624e11c6fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [01:18<00:00, 24.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "Overall Token Prediction Accuracy: 0.5515 (33062/59950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a given sequence from a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability of masking a token (default is 15%).\n",
    "        device: Torch device on which to run computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (correct, total_masked) where:\n",
    "          - correct: number of masked tokens correctly predicted.\n",
    "          - total_masked: total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of input_ids for ground-truth labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace tokens at masked positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch, seq_length, vocab_size]\n",
    "    \n",
    "    # Get predicted token IDs.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Evaluate only on masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    return correct, total_masked\n",
    "\n",
    "# ----- Main script to process FASTA file with a progress bar -----\n",
    "\n",
    "# File path to FASTA file (adjust as needed)\n",
    "fasta_file = \"finetuning_data/test/dgoa_mutants_test.fasta\"\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"pretrained Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "324168df-0540-422a-b847-7871f4599219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [01:49<00:00, 17.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "finetuned Overall Token Prediction Accuracy: 0.9833 (59056/60061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"finetuned Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb4e5b-2000-4c09-8e67-43ec29d09898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

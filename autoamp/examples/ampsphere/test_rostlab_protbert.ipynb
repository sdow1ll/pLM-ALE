{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26ce4cd7-062f-40a2-9e4a-b09675c09930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'tf_bert_model_4' (type TFBertModel).\n\nData of type <class 'torch.Tensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for token_type_ids.\n\nCall arguments received by layer 'tf_bert_model_4' (type TFBertModel):\n  • input_ids=tensor([[ 2, 21,  9, 17, 10, 14, 10, 17, 15, 18, 10, 22, 19,  8,  9,  8,  7,  5,\n         10, 13,  3]])\n  • attention_mask=tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n  • token_type_ids=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM E N S D S N T Q S H F V E V G L S R\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Get the model output\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_protbert_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(sequence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized input:\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m, in \u001b[0;36mget_protbert_embeddings\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get the model output\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# The hidden states are the last layer's embeddings\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# `outputs.last_hidden_state` has shape [batch_size, seq_length, hidden_size]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/miniconda3/envs/KE-default/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/KE-default/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/KE-default/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:513\u001b[0m, in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         output[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(main_input, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(main_input):\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;66;03m# EagerTensors don't allow to use the .name property so we check for a real Tensor\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_model_4' (type TFBertModel).\n\nData of type <class 'torch.Tensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for token_type_ids.\n\nCall arguments received by layer 'tf_bert_model_4' (type TFBertModel):\n  • input_ids=tensor([[ 2, 21,  9, 17, 10, 14, 10, 17, 15, 18, 10, 22, 19,  8,  9,  8,  7,  5,\n         10, 13,  3]])\n  • attention_mask=tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n  • token_type_ids=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from transformers import TFBertModel, BertTokenizer,BertConfig\n",
    "\n",
    "# Load the pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"Rostlab/prot_bert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "model = TFBertModel.from_pretrained(\"Rostlab/prot_bert\", from_pt=True)\n",
    "\n",
    "# Function to process a sequence with ProtBERT\n",
    "def get_protbert_embeddings(sequence):\n",
    "    # Tokenize the input sequence\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the model output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # The hidden states are the last layer's embeddings\n",
    "    # `outputs.last_hidden_state` has shape [batch_size, seq_length, hidden_size]\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Example sequence (input should be a single string of amino acids)\n",
    "sequence = \"M E N S D S N T Q S H F V E V G L S R\"\n",
    "\n",
    "# Get the model output\n",
    "embeddings = get_protbert_embeddings(sequence)\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(\"Tokenized input:\", inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdfbdb-0b41-4e31-946d-f2982277f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input: tensor([[2, 1, 3],\n",
      "        [2, 1, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Print the shape of the embeddings to verify\\nprint(\"Embeddings shape:\", embeddings.shape)\\nprint(\"Embeddings:\", embeddings)\\n\\n# You can also inspect individual embeddings\\nfor i, embedding in enumerate(embeddings[0]):\\n    print(f\"Residue {i}: {embedding.shape}\")\\n\\n# If you need to handle special cases (like shape (1, 1024)), you can include logic here\\nif embeddings.shape[1] == 1:\\n    print(\"Single sequence embedding detected\")\\n    # Handle the single vector case here\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Print the shape of the embeddings to verify\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Embeddings:\", embeddings)\n",
    "\n",
    "# You can also inspect individual embeddings\n",
    "for i, embedding in enumerate(embeddings[0]):\n",
    "    print(f\"Residue {i}: {embedding.shape}\")\n",
    "\n",
    "# If you need to handle special cases (like shape (1, 1024)), you can include logic here\n",
    "if embeddings.shape[1] == 1:\n",
    "    print(\"Single sequence embedding detected\")\n",
    "    # Handle the single vector case here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b76c2ab-d955-4b69-95a8-60e3b05a5202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded sequence for 'MENSDSNTQSHFVEVGLSR': [2, 1, 3]\n",
      "Encoded sequence for 'ENGLN': [2, 1, 3]\n",
      "Tokenized input (input_ids): tensor([[2, 1, 3],\n",
      "        [2, 1, 3]])\n",
      "Embeddings shape: torch.Size([2, 3, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the ProtBERT model and tokenizer\n",
    "model_name = \"Rostlab/prot_bert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Sequences to be processed\n",
    "sequence = [\"MENSDSNTQSHFVEVGLSR\", \"ENGLN\"]\n",
    "\n",
    "# Manually encode each sequence to debug tokenization\n",
    "for seq in sequence:\n",
    "    encoded_seq = tokenizer.encode(seq, add_special_tokens=True)\n",
    "    print(f\"Encoded sequence for '{seq}': {encoded_seq}\")\n",
    "\n",
    "# Tokenize the input sequences for the model\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Print the tokenized input to debug\n",
    "print(\"Tokenized input (input_ids):\", inputs['input_ids'])\n",
    "\n",
    "# Get the model output (embeddings)\n",
    "with torch.no_grad():\n",
    "    embeddings = model(**inputs).last_hidden_state\n",
    "\n",
    "# Print the shape of the embeddings\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc0ec34-4ac2-497a-86e4-764f98cc0a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 1, 1, 1, 1, 1, 1, 3]\n",
      "['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "sample = 'where is Himalayas in the world map?'\n",
    "encoding = tokenizer.encode(sample)\n",
    "print(encoding)\n",
    "print(tokenizer.convert_ids_to_tokens(encoding))\n",
    "#output 1: [101, 2073, 2003, 26779, 1999, 1996, 2088, 4949, 1029, 102]\n",
    "#output 2: ['[CLS]', 'where', 'is', 'himalayas', 'in', 'the', 'world', 'map', '?', '[SEP]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa16471-6829-452b-b67d-be112c7fc46b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

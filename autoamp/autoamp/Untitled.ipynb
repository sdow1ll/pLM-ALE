{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1f55b9-b0b6-4bd1-8b0c-0dd90c6371a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoamp.pLM import ProteinLanguageModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac3276a2-4b3d-420a-83bf-9fb04b629920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: EVQLVESGGGLVQPGGSLRLSCAASGFSVSTKYMTWVRQAPGKGLEWVSVLYSGGSDYYADSVKGRFTISRDNSKNALYLQMNSLRVEDTGVYYCARDSSEVRDHPGHPGRSVGAFDIWGQGTMVTVSS\n",
      "Generated sequence: EVQLVESGGGLVQPGGSLRLSCAASGFSVSTKYMTWVRQAPGKGLEWVSVLYSGGSDYYADSVKGRFTISRDNSKNALYLQMNSLRVEDTGVYYCARDSSEVRDHPGHPGRSVGAFDIWGQGTMVTVSS\n",
      "EFFCFVSWLFW\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nferruz/ProtGPT2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"nferruz/ProtGPT2\")\n",
    "\n",
    "# Example input sequence (partial protein sequence)\n",
    "input_sequence = \"EVQLVESGGGLVQPGGSLRLSCAASGFSVSTKYMTWVRQAPGKGLEWVSVLYSGGSDYYADSVKGRFTISRDNSKNALYLQMNSLRVEDTGVYYCARDSSEVRDHPGHPGRSVGAFDIWGQGTMVTVSS\"\n",
    "\n",
    "# Tokenize the input sequence\n",
    "inputs = tokenizer(input_sequence, return_tensors='pt')\n",
    "\n",
    "# Generate the protein sequence\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=50,  # Adjust max_length based on your needs\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,  # Sampling is needed for generation tasks\n",
    "        top_k=50,  # Adjust these parameters to control randomness\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "# Decode the generated token IDs to a protein sequence\n",
    "generated_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input sequence: {input_sequence}\")\n",
    "print(f\"Generated sequence: {generated_sequence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6691858-d90a-4630-81d1-96aa91178678",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Get predictions for single point mutations\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m mutations \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_mutations\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted mutations: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmutations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 16\u001b[0m, in \u001b[0;36mpredict_mutations\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sequence)):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Mask the i-th position\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     masked_sequence \u001b[38;5;241m=\u001b[39m \u001b[43msequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_token\u001b[49m \u001b[38;5;241m+\u001b[39m sequence[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     17\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(masked_sequence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nferruz/ProtGPT2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"nferruz/ProtGPT2\")\n",
    "\n",
    "# Example wild-type protein sequence\n",
    "sequence = \"EVQLVESGGGLVQPGGSLRLSCAASGFSVSTKYMTWVRQAPGKGLEWVSVLYSGGSDYYADSVKGRFTISRDNSKNALYLQMNSLRVEDTGVYYCARDSSEVRDHPGHPGRSVGAFDIWGQGTMVTVSS\"\n",
    "\n",
    "# Function to predict single point mutations\n",
    "def predict_mutations(sequence):\n",
    "    predictions = {}\n",
    "    for i in range(len(sequence)):\n",
    "        # Mask the i-th position\n",
    "        masked_sequence = sequence[:i] + tokenizer.mask_token + sequence[i+1:]\n",
    "        inputs = tokenizer(masked_sequence, return_tensors='pt')\n",
    "        \n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs['input_ids'], max_length=len(sequence), do_sample=True, top_k=50, top_p=0.95)\n",
    "        \n",
    "        # Decode predictions\n",
    "        predicted_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predicted_amino_acid = predicted_sequence[i]\n",
    "        \n",
    "        # Store the prediction\n",
    "        predictions[i] = predicted_amino_acid\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "# Get predictions for single point mutations\n",
    "mutations = predict_mutations(sequence)\n",
    "print(f\"Original sequence: {sequence}\")\n",
    "print(f\"Predicted mutations: {mutations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a0dffe5-6796-4cc5-9f0e-8dcdf06df272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tokenizer': <tokenizers.Tokenizer at 0x7f04f2a95430>,\n",
       " '_decode_use_source_tokenizer': False,\n",
       " 'init_inputs': (),\n",
       " 'init_kwargs': {'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "  'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "  'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "  'add_prefix_space': False,\n",
       "  'name_or_path': 'nferruz/ProtGPT2'},\n",
       " 'name_or_path': 'nferruz/ProtGPT2',\n",
       " '_processor_class': None,\n",
       " 'model_max_length': 1000000000000000019884624838656,\n",
       " 'padding_side': 'right',\n",
       " 'truncation_side': 'right',\n",
       " 'model_input_names': ['input_ids', 'attention_mask'],\n",
       " 'clean_up_tokenization_spaces': True,\n",
       " 'split_special_tokens': False,\n",
       " 'deprecation_warnings': {},\n",
       " '_in_target_context_manager': False,\n",
       " 'chat_template': None,\n",
       " '_bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " '_eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " '_unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " '_sep_token': None,\n",
       " '_pad_token': None,\n",
       " '_cls_token': None,\n",
       " '_mask_token': None,\n",
       " '_pad_token_type_id': 0,\n",
       " '_additional_special_tokens': [],\n",
       " 'verbose': False,\n",
       " 'add_bos_token': False,\n",
       " 'add_prefix_space': False}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc278cf3-b114-4ef8-bff8-cf053a8f823f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066e057-bf56-4ca2-9bee-e2e6c1ffe861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3c54f4-0170-43bf-8dbf-58bdbab0cbdb",
   "metadata": {},
   "source": [
    "# Distill a Finetuned ESM2 150M model into a ESM2 8M model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c490ca21-244c-49cd-bbdc-c6cb2e112fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sdowell/scratch/Thesis/distillation'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be18970-0b64-40dc-8a20-bb08427bf76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:54:21.538442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-11 16:54:21.730983: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-11 16:54:21.732418: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-11 16:54:22.014917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-11 16:54:32.695175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import sys\n",
    "import pLM_KD\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4812c-47f2-4cc7-8e7d-2a1fde1be64c",
   "metadata": {},
   "source": [
    "# Load teacher and student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7890de1d-13fc-4a87-9119-ca6cf8b03823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdowell/miniconda3/envs/KE-default/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msdowell\u001b[0m (\u001b[33msdowell1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>distilled_esm2_model/wandb/run-20250511_165500-00dws11e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sdowell1/esm2_knowledge_distillation/runs/00dws11e' target=\"_blank\">esm2_knowledge_distillation</a></strong> to <a href='https://wandb.ai/sdowell1/esm2_knowledge_distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sdowell1/esm2_knowledge_distillation' target=\"_blank\">https://wandb.ai/sdowell1/esm2_knowledge_distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sdowell1/esm2_knowledge_distillation/runs/00dws11e' target=\"_blank\">https://wandb.ai/sdowell1/esm2_knowledge_distillation/runs/00dws11e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 16:55:19,130 INFO: Distillation configuration saved to distilled_esm2_model/distillation_config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Teacher model device: cuda:0\n",
      "Student model device: cuda:0\n",
      "Using max_length: 1024, which is divisible by 8\n",
      "Loaded 7489 training and 1404 evaluation sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdowell/miniconda3/envs/KE-default/lib/python3.10/site-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Configuration dictionary (or loaded from YML)\n",
    "config_dict = {\n",
    "    \"train_path\": \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/train.fasta\",\n",
    "    \"eval_path\": \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/valid.fasta\",\n",
    "    \"base_model\": \"facebook/esm2_t6_8M_UR50D\",  # Student model\n",
    "    \"teacher_model_path\": \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\",\n",
    "    \"student_model_path\": \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_8m_ecoli_finetuning_2/checkpoint-11500\",\n",
    "    \"wandb_project\": \"esm2_knowledge_distillation\",\n",
    "    \"training_args\": {\n",
    "        \"output_dir\": \"distilled_esm2_model\",\n",
    "        \"per_device_train_batch_size\": 32,  \n",
    "        \"per_device_eval_batch_size\": 32,\n",
    "        \"num_train_epochs\": 100,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"alpha\": 0.5,\n",
    "        \"temperature\": 2.0,\n",
    "        \"fp16\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a DistillationConfig instance\n",
    "config = pLM_KD.DistillationConfig(**config_dict)\n",
    "\n",
    "# Choose the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Load teacher model using PEFT wrapper (LoRA)\n",
    "# ============================================\n",
    "from peft import PeftModel\n",
    "\n",
    "# Use the base model id for teacher (should match the one used during fine-tuning)\n",
    "teacher_base_model_id = \"facebook/esm2_t30_150M_UR50D\"\n",
    "# Load the base teacher model\n",
    "teacher_base = pLM_KD.EsmForMaskedLM.from_pretrained(teacher_base_model_id).to(device)\n",
    "# Wrap the base model with LoRA adapters using the previously fine-tuned checkpoint\n",
    "teacher_model = PeftModel.from_pretrained(teacher_base, config.teacher_model_path).to(device)\n",
    "teacher_model.eval()  # Set teacher to evaluation mode\n",
    "print(f\"Teacher model device: {next(teacher_model.parameters()).device}\")  # Debug print\n",
    "\n",
    "# Load tokenizer using the teacher base model identifier (assumes the tokenizer is shared)\n",
    "tokenizer = pLM_KD.EsmTokenizer.from_pretrained(teacher_base_model_id)\n",
    "\n",
    "# ============================================\n",
    "# Load student model\n",
    "# ============================================\n",
    "student_model = pLM_KD.EsmForMaskedLM.from_pretrained(config.base_model).to(device)\n",
    "# Wrap the student model with LoRA adpaters\n",
    "student_model = PeftModel.from_pretrained(student_model, config.student_model_path).to(device)\n",
    "student_model.train()\n",
    "print(f\"Student model device: {next(student_model.parameters()).device}\")  # Debug print\n",
    "\n",
    "# Set max length\n",
    "max_length = min(\n",
    "    getattr(teacher_model.config, \"max_position_embeddings\", 1024),\n",
    "    getattr(student_model.config, \"max_position_embeddings\", 1024)\n",
    ")\n",
    "\n",
    "# Ensure max_length is a multiple of pad_to_multiple_of\n",
    "pad_to_multiple_of = 8 if config.training_args.fp16 else None\n",
    "if pad_to_multiple_of:\n",
    "    max_length = (max_length // pad_to_multiple_of) * pad_to_multiple_of\n",
    "print(f\"Using max_length: {max_length}, which is divisible by {pad_to_multiple_of}\")\n",
    "\n",
    "tokenizer.model_max_length = max_length\n",
    "\n",
    "# Load sequences\n",
    "train_sequences = [seq.sequence for seq in pLM_KD.read_fasta(config.train_path)]\n",
    "eval_sequences = [seq.sequence for seq in pLM_KD.read_fasta(config.eval_path)]\n",
    "print(f\"Loaded {len(train_sequences)} training and {len(eval_sequences)} evaluation sequences\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = pLM_KD.SequenceDataset(train_sequences)\n",
    "eval_dataset = pLM_KD.SequenceDataset(eval_sequences)\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = pLM_KD.HybridDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    model_type=\"mlm\",\n",
    "    mlm_probability=0.15,\n",
    "    max_length=max_length,\n",
    "    pad_to_multiple_of=8 if config.training_args.fp16 else None\n",
    ")\n",
    "\n",
    "# Initialize distillation trainer\n",
    "trainer = pLM_KD.DistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    alpha=config.training_args.alpha,\n",
    "    temperature=config.training_args.temperature,\n",
    "    model=student_model,\n",
    "    args=config.training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "#train_result = trainer.train()\n",
    "\n",
    "# Save the model\n",
    "#trainer.save_model()\n",
    "\n",
    "# Evaluate the model\n",
    "#eval_metrics = trainer.evaluate()\n",
    "#print(\"Evaluation metrics:\", eval_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb42f3-36db-4d25-98c8-a3f1a3bde622",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a7fc9-59b5-485a-a35c-4737b729c7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd7456-d72c-4816-b8e4-a7dc234a89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a small DataLoader for the evaluation set using your data collator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For example, use batch_size=8 (or any small batch size)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Get one batch from the evaluation data\n",
    "batch = next(iter(eval_loader))\n",
    "\n",
    "# Ensure the batch is on the correct device\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        batch[key] = value.to(device)\n",
    "\n",
    "# Set models to evaluation mode (if not already)\n",
    "teacher_model.eval()\n",
    "student_model.eval()\n",
    "\n",
    "# Forward pass through both models (teacher: no grad needed)\n",
    "with torch.no_grad():\n",
    "    teacher_outputs = teacher_model(**batch)\n",
    "    student_outputs = student_model(**batch)\n",
    "\n",
    "# Extract logits\n",
    "teacher_logits = teacher_outputs.logits  # shape: [batch_size, seq_length, vocab_size]\n",
    "student_logits = student_outputs.logits\n",
    "\n",
    "# Retrieve the temperature from your configuration\n",
    "T = config.training_args.temperature\n",
    "\n",
    "# Compute softened probability distributions:\n",
    "# Teacher uses softmax, and student uses log_softmax for KL divergence stability\n",
    "teacher_probs = F.softmax(teacher_logits / T, dim=-1)\n",
    "student_log_probs = F.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "# Compute the KL divergence with batch mean reduction and scale by T^2\n",
    "kl_div = F.kl_div(student_log_probs, teacher_probs, reduction='mean') * (T ** 2)\n",
    "\n",
    "print(f\"KL divergence on one evaluation batch: {kl_div.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a7e4155-f2c4-4214-a6af-7da352a87dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7156, -4.9638, -4.8050, -4.9654, -3.8123, -3.6673, -3.7609, -3.6731,\n",
       "        -3.7292, -3.8123, -3.5947, -3.6705, -3.8625, -3.8000, -3.9839, -3.7046,\n",
       "        -3.7173, -3.9350, -3.8993, -3.7534, -3.8414, -3.9192, -4.2384, -3.9956,\n",
       "        -3.6264, -5.9818, -6.0206, -6.1081, -6.7169, -6.8043, -6.7890, -6.8330,\n",
       "        -4.9627], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_log_probs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b99c22b5-7486-460c-9e6c-9ef05ef5da7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3348, 0.0121, 0.0263, 0.0121, 0.0243, 0.0304, 0.0265, 0.0598, 0.0569,\n",
       "        0.0315, 0.0268, 0.0361, 0.0324, 0.0194, 0.0456, 0.0223, 0.0223, 0.0166,\n",
       "        0.0273, 0.0162, 0.0260, 0.0109, 0.0132, 0.0221, 0.0101, 0.0082, 0.0053,\n",
       "        0.0046, 0.0020, 0.0019, 0.0022, 0.0014, 0.0125], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_probs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca312ee6-3d7e-4988-ba57-7dce83348e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable LoRA parameters in student model: 30\n",
      "base_model.model.lm_head.modules_to_save.default.bias: requires_grad=True\n",
      "base_model.model.lm_head.modules_to_save.default.dense.weight: requires_grad=True\n",
      "base_model.model.lm_head.modules_to_save.default.dense.bias: requires_grad=True\n",
      "base_model.model.lm_head.modules_to_save.default.layer_norm.weight: requires_grad=True\n",
      "base_model.model.lm_head.modules_to_save.default.layer_norm.bias: requires_grad=True\n",
      "base_model.model.lm_head.modules_to_save.default.decoder.weight: requires_grad=True\n",
      "\n",
      "base_model.model.esm.encoder.layer.0.attention\n",
      "base_model.model.esm.encoder.layer.0.attention.self\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.base_layer\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_dropout\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.base_layer\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_dropout\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.0.attention.self.dropout\n",
      "base_model.model.esm.encoder.layer.0.attention.self.rotary_embeddings\n",
      "base_model.model.esm.encoder.layer.0.attention.output\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dropout\n",
      "base_model.model.esm.encoder.layer.0.attention.LayerNorm\n",
      "base_model.model.esm.encoder.layer.1.attention\n",
      "base_model.model.esm.encoder.layer.1.attention.self\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.base_layer\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_dropout\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.base_layer\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_dropout\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.1.attention.self.dropout\n",
      "base_model.model.esm.encoder.layer.1.attention.self.rotary_embeddings\n",
      "base_model.model.esm.encoder.layer.1.attention.output\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dropout\n",
      "base_model.model.esm.encoder.layer.1.attention.LayerNorm\n",
      "base_model.model.esm.encoder.layer.2.attention\n",
      "base_model.model.esm.encoder.layer.2.attention.self\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.base_layer\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_dropout\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.base_layer\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_dropout\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.2.attention.self.dropout\n",
      "base_model.model.esm.encoder.layer.2.attention.self.rotary_embeddings\n",
      "base_model.model.esm.encoder.layer.2.attention.output\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dropout\n",
      "base_model.model.esm.encoder.layer.2.attention.LayerNorm\n",
      "base_model.model.esm.encoder.layer.3.attention\n",
      "base_model.model.esm.encoder.layer.3.attention.self\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.base_layer\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_dropout\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.base_layer\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_dropout\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.3.attention.self.dropout\n",
      "base_model.model.esm.encoder.layer.3.attention.self.rotary_embeddings\n",
      "base_model.model.esm.encoder.layer.3.attention.output\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dropout\n",
      "base_model.model.esm.encoder.layer.3.attention.LayerNorm\n",
      "base_model.model.esm.encoder.layer.4.attention\n",
      "base_model.model.esm.encoder.layer.4.attention.self\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.base_layer\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_dropout\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.base_layer\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_dropout\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.4.attention.self.dropout\n",
      "base_model.model.esm.encoder.layer.4.attention.self.rotary_embeddings\n",
      "base_model.model.esm.encoder.layer.4.attention.output\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dropout\n",
      "base_model.model.esm.encoder.layer.4.attention.LayerNorm\n",
      "base_model.model.esm.encoder.layer.5.attention\n",
      "base_model.model.esm.encoder.layer.5.attention.self\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.base_layer\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_dropout\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.base_layer\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_dropout\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_dropout.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_embedding_A\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_embedding_B\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector\n",
      "base_model.model.esm.encoder.layer.5.attention.self.dropout\n",
      "base_model.model.esm.encoder.layer.5.attention.self.rotary_embeddings\n",
      "base_model.model.esm.encoder.layer.5.attention.output\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dropout\n",
      "base_model.model.esm.encoder.layer.5.attention.LayerNorm\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model_state_dict\n",
    "\n",
    "# This prints only LoRA-adapted parameters\n",
    "print(\"Trainable LoRA parameters in student model:\", len(get_peft_model_state_dict(student_model)))\n",
    "\n",
    "# Optional: print all trainable params by name\n",
    "for name, param in student_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "print(\"\")\n",
    "for name, _ in student_model.named_modules():\n",
    "    if \"attention\" in name or \"key\" in name or \"value\" in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c6e5a-2d36-42fe-8313-842877af7ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

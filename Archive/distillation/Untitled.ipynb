{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ab8c67-d4f2-45f7-9180-946fd77b0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 21:29:39.587331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-08 21:29:39.603972: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-08 21:29:39.604015: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-08 21:29:39.616609: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-08 21:29:40.976324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 149843867\n"
     ]
    }
   ],
   "source": [
    "from pLM_KD import EsmForMaskedLM  # or use the relevant import if different\n",
    "\n",
    "teacher_model_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "model = EsmForMaskedLM.from_pretrained(teacher_model_path)\n",
    "\n",
    "# Print total number of parameters (both trainable and non-trainable)\n",
    "total_parameters = model.num_parameters()\n",
    "print(f\"Total parameters: {total_parameters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c23e3e-b06c-4a36-a71e-734bfee69e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c4fcff-a83a-474f-af63-f2f58d8877d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 21:45:02,989 INFO: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "- base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.6.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.6.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.6.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.6.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.7.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.7.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.7.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.7.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.8.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.8.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.8.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.8.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.9.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.9.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.9.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.9.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.10.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.10.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.10.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.10.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.11.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.11.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.11.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.11.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.12.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.12.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.12.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.12.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.13.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.13.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.13.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.13.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.14.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.14.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.14.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.14.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.15.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.15.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.15.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.15.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.16.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.16.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.16.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.16.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.17.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.17.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.17.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.17.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.18.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.18.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.18.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.18.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.19.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.19.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.19.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.19.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.20.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.20.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.20.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.20.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.21.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.21.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.21.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.21.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.22.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.22.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.22.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.22.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.23.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.23.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.23.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.23.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.24.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.24.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.24.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.24.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.25.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.25.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.25.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.25.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.26.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.26.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.26.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.26.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.27.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.27.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.27.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.27.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.28.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.28.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.28.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.28.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.29.attention.self.key.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.29.attention.self.key.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.29.attention.self.value.lora_A.default.weight: torch.Size([8, 640]) (5120 parameters)\n",
      "- base_model.model.esm.encoder.layer.29.attention.self.value.lora_B.default.weight: torch.Size([640, 8]) (5120 parameters)\n",
      "- base_model.model.lm_head.original_module.bias: torch.Size([33]) (33 parameters)\n",
      "- base_model.model.lm_head.original_module.dense.weight: torch.Size([640, 640]) (409600 parameters)\n",
      "- base_model.model.lm_head.original_module.dense.bias: torch.Size([640]) (640 parameters)\n",
      "- base_model.model.lm_head.original_module.layer_norm.weight: torch.Size([640]) (640 parameters)\n",
      "- base_model.model.lm_head.original_module.layer_norm.bias: torch.Size([640]) (640 parameters)\n",
      "- base_model.model.lm_head.modules_to_save.default.bias: torch.Size([33]) (33 parameters)\n",
      "- base_model.model.lm_head.modules_to_save.default.dense.weight: torch.Size([640, 640]) (409600 parameters)\n",
      "- base_model.model.lm_head.modules_to_save.default.dense.bias: torch.Size([640]) (640 parameters)\n",
      "- base_model.model.lm_head.modules_to_save.default.layer_norm.weight: torch.Size([640]) (640 parameters)\n",
      "- base_model.model.lm_head.modules_to_save.default.layer_norm.bias: torch.Size([640]) (640 parameters)\n",
      "- base_model.model.lm_head.modules_to_save.default.decoder.weight: torch.Size([33, 640]) (21120 parameters)\n",
      "\n",
      "LoRA integration complete. Trainable parameters: 1458626 (0.97% of total 149843867)\n"
     ]
    }
   ],
   "source": [
    "from pLM_KD import EsmForMaskedLM, EsmTokenizer, get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "\n",
    "# Path to your fine-tuned teacher model checkpoint\n",
    "teacher_model_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "base_model = \"facebook/esm2_t30_150M_UR50D\"\n",
    "\n",
    "# Load your model and tokenizer. If you have a separate base model from which to load the tokenizer,\n",
    "# adjust accordingly (here I assume the tokenizer is in the same location for simplicity).\n",
    "model = EsmForMaskedLM.from_pretrained(teacher_model_path)\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
    "\n",
    "# Before applying LoRA, make sure the entire model requires gradients (optional if you want to inspect them)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Determine the target modules. Here we assume \"key\" and \"value\" layers are the targets.\n",
    "# You might want to include additional logic to determine these names dynamically.\n",
    "target_module_names = [\"key\", \"value\"]\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Adjust task type as needed, e.g., CAUSAL_LM for ESM models\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_module_names,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA integration. This wraps your model and adds extra trainable parameters.\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Set which parameters should be trainable:\n",
    "# For PEFT-style fine-tuning (e.g. LoRA), you typically set only the LoRA modules and sometimes the LM head to require gradients.\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name or \"lm_head\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Optionally, print details of each trainable parameter for debugging\n",
    "print(\"Trainable parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"- {name}: {param.shape} ({param.numel()} parameters)\")\n",
    "\n",
    "# Calculate total numbers and print summary\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nLoRA integration complete. Trainable parameters: {trainable_params} \"\n",
    "      f\"({100 * trainable_params / total_params:.2f}% of total {total_params})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aad177c-aa36-4ba8-8ab3-2226405ad284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from pLM_KD import EsmForMaskedLM, EsmTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Your teacher model checkpoint path (LoRA fine-tuned model checkpoint)\n",
    "teacher_model_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "\n",
    "# Instead of passing a string as base_model, load the base model object:\n",
    "base_model_obj = EsmForMaskedLM.from_pretrained(\"facebook/esm2_t30_150M_UR50D\").to(device)\n",
    "\n",
    "# Load the LoRA fine-tuned teacher model using the PEFT utility\n",
    "teacher_model = PeftModel.from_pretrained(base_model_obj, teacher_model_path)\n",
    "teacher_model.eval()  # Set teacher to evaluation mode\n",
    "\n",
    "# Optionally, load your tokenizer as needed.\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b303e526-0477-4ef4-8d1c-9b051ddb93a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForMaskedLM(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 640, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 640, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-29): 30 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=640, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=640, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=640, out_features=2560, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): EsmLMHead(\n",
       "          (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "          (decoder): Linear(in_features=640, out_features=33, bias=False)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): EsmLMHead(\n",
       "            (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "            (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "            (decoder): Linear(in_features=640, out_features=33, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6410d6-e4d4-456a-9e63-44d1648923ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

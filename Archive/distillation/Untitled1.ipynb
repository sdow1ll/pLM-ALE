{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5e0ce8-15db-4ed0-b567-2a096c000f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sdowell/scratch/Thesis/distillation'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6664d53c-7213-4c5f-bf0f-110ebbc32217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 15:53:36.202231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-13 15:53:36.346787: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-13 15:53:36.347584: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-13 15:53:36.597387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 15:53:48.261820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/sdowell/miniconda3/envs/KE-default/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2367575/1570354355.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'compute_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 91\u001b[0m\n\u001b[1;32m     75\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     76\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilled-esm2-8M\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 10. Trainer \u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# 12. Save back only the student LoRA weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/KE-default/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'compute_loss'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# —————————————————————————————\n",
    "# 1. Paths to your LoRA adapters\n",
    "teacher_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "student_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_8m_ecoli_finetuning_2/checkpoint-11500\"\n",
    "# —————————————————————————————\n",
    "\n",
    "# 2. Tokenizer (same for both)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\", do_lower_case=False)\n",
    "\n",
    "# 3. Load base models\n",
    "base_teacher = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t30_150M_UR50D\", output_hidden_states=True, return_dict=True\n",
    ")\n",
    "base_student = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\", output_hidden_states=True, return_dict=True\n",
    ")\n",
    "\n",
    "# 4. Attach your fine-tuned LoRA adapters\n",
    "teacher = PeftModel.from_pretrained(base_teacher, teacher_lora_path)\n",
    "student = PeftModel.from_pretrained(base_student, student_lora_path)\n",
    "\n",
    "# 5. Freeze everything except student’s LoRA\n",
    "for p in teacher.parameters():            \n",
    "    p.requires_grad = False\n",
    "for name, p in student.named_parameters():\n",
    "    if \"lora_\" not in name:\n",
    "        p.requires_grad = False\n",
    "\n",
    "teacher.eval()\n",
    "\n",
    "# —————————————————————————————\n",
    "# 6. Distillation loss\n",
    "def distill_loss(student_logits, teacher_logits, T=2.0):\n",
    "    \"\"\"KL(student||teacher) on softened logits.\"\"\"\n",
    "    s = student_logits / T\n",
    "    t = teacher_logits / T\n",
    "    kl = F.kl_div(\n",
    "        F.log_softmax(s, dim=-1),\n",
    "        F.softmax(t, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    )\n",
    "    return kl * (T * T)\n",
    "\n",
    "# 7. Hook into Trainer\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    # forward teacher\n",
    "    with torch.no_grad():\n",
    "        t_out = teacher(**inputs)\n",
    "    # forward student\n",
    "    s_out = model(**inputs)\n",
    "\n",
    "    loss = distill_loss(s_out.logits, t_out.logits, T=2.0)\n",
    "    # optional: combine with CE on masked labels\n",
    "    if inputs.get(\"labels\") is not None:\n",
    "        ce = F.cross_entropy(\n",
    "            s_out.logits.view(-1, s_out.logits.size(-1)),\n",
    "            inputs[\"labels\"].view(-1)\n",
    "        )\n",
    "        alpha = 0.5\n",
    "        loss = alpha * ce + (1 - alpha) * loss\n",
    "\n",
    "    return (loss, s_out) if return_outputs else loss\n",
    "# —————————————————————————————\n",
    "\n",
    "# 8. Your datasets\n",
    "train_dataset = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/train.fasta\"\n",
    "eval_dataset  = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/valid.fasta\"\n",
    "\n",
    "# 9. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilled-esm2-8M\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    save_steps=1,\n",
    "    logging_steps=1,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4)\n",
    "student.train()\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            teacher_out = teacher(**batch)\n",
    "        student_out = student(**batch)\n",
    "\n",
    "        loss = distill_loss(student_out.logits, teacher_out.logits, T=2.0)\n",
    "        if \"labels\" in batch:\n",
    "            ce = F.cross_entropy(\n",
    "                student_out.logits.view(-1, student_out.logits.size(-1)),\n",
    "                batch[\"labels\"].view(-1)\n",
    "            )\n",
    "            loss = 0.5 * ce + 0.5 * loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # log to W&B, checkpoint, etc.\n",
    "\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 12. Save back only the student LoRA weights\n",
    "student.save_pretrained(\"distilled-esm2-8M-lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51580815-42de-428f-a057-2c64a28cc5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k6icqegl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.027 MB of 0.027 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">manual-loop</strong> at: <a href='https://wandb.ai/sdowell1/esm2-distill/runs/k6icqegl' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill/runs/k6icqegl</a><br/> View project at: <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_161638-k6icqegl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k6icqegl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b0ce2bab1c49cfa07d23a68600644d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112385411332878, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sdowell/scratch/Thesis/distillation/wandb/run-20250513_161819-kr0bdi98</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sdowell1/esm2-distill/runs/kr0bdi98' target=\"_blank\">manual-loop</a></strong> to <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sdowell1/esm2-distill/runs/kr0bdi98' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill/runs/kr0bdi98</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 175\u001b[0m\n\u001b[1;32m    173\u001b[0m         s_eval \u001b[38;5;241m=\u001b[39m student(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mev_batch)\n\u001b[1;32m    174\u001b[0m         eval_loss \u001b[38;5;241m=\u001b[39m distill_loss(s_eval\u001b[38;5;241m.\u001b[39mlogits, t_eval\u001b[38;5;241m.\u001b[39mlogits, T)\n\u001b[0;32m--> 175\u001b[0m     total_eval_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43meval_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     eval_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    177\u001b[0m avg_eval_loss \u001b[38;5;241m=\u001b[39m total_eval_loss \u001b[38;5;241m/\u001b[39m eval_batches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ─── Hyper-parameters ─────────────────────────────────────\n",
    "teacher_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "student_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_8m_ecoli_finetuning_2/checkpoint-11500\"\n",
    "train_fasta         = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/train.fasta\"\n",
    "valid_fasta         = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/valid.fasta\"\n",
    "\n",
    "output_dir       = \"distilled-esm2-8M\"\n",
    "epochs           = 3\n",
    "train_bs         = 16\n",
    "eval_bs          = 32\n",
    "lr               = 1e-4\n",
    "mlm_prob         = 0.15\n",
    "T                = 2.0\n",
    "logging_steps    = 100\n",
    "eval_steps       = 500\n",
    "save_steps       = 500\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── Models & Tokenizer ────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/esm2_t30_150M_UR50D\", do_lower_case=False\n",
    ")\n",
    "base_teacher = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t30_150M_UR50D\", output_hidden_states=True, return_dict=True\n",
    ")\n",
    "base_student = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\", output_hidden_states=True, return_dict=True\n",
    ")\n",
    "teacher = PeftModel.from_pretrained(base_teacher, teacher_lora_path)\n",
    "student = PeftModel.from_pretrained(base_student, student_lora_path)\n",
    "\n",
    "# ─── Freeze parameters ────────────────────────────────────\n",
    "# Freeze all teacher params\n",
    "teacher.eval()\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "# Freeze student base model, leave only LoRA adapter params trainable\n",
    "for name, param in student.named_parameters():\n",
    "    if 'lora_' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "teacher.to(device)\n",
    "student.to(device).train()\n",
    "\n",
    "# ─── Data ─────────────────────────────────────────────────\n",
    "ds = load_dataset(\n",
    "    \"text\", data_files={\"train\": train_fasta, \"validation\": valid_fasta}\n",
    ")\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, max_length=1024\n",
    "    )\n",
    "ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds  = ds[\"validation\"]\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=True, mlm_probability=mlm_prob\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=train_bs, shuffle=True, collate_fn=collator\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds, batch_size=eval_bs, shuffle=False, collate_fn=collator\n",
    ")\n",
    "\n",
    "# ─── Optimizer & Scheduler ────────────────────────────────\n",
    "optimizer   = torch.optim.AdamW(student.parameters(), lr=lr)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler   = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# ─── Mixed-precision setup ─────────────────────────────────\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# ─── W&B setup ─────────────────────────────────────────────\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"esm2-distill\",\n",
    "    name=\"manual-loop\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"train_bs\": train_bs,\n",
    "        \"eval_bs\": eval_bs,\n",
    "        \"lr\": lr,\n",
    "        \"mlm_prob\": mlm_prob,\n",
    "        \"temperature\": T,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ─── Distillation loss ────────────────────────────────────\n",
    "def distill_loss(s_logits, t_logits, T):\n",
    "    s = s_logits / T\n",
    "    t = t_logits / T\n",
    "    kld = F.kl_div(\n",
    "        F.log_softmax(s, dim=-1),\n",
    "        F.softmax(t, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    )\n",
    "    return kld * (T * T)\n",
    "\n",
    "# ─── Training loop ────────────────────────────────────────\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Training batches\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Teacher forward\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(**batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Student forward + compute loss\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            s_out = student(**batch)\n",
    "            loss_kd = distill_loss(s_out.logits, t_out.logits, T)\n",
    "            if batch.get(\"labels\") is not None:\n",
    "                ce = F.cross_entropy(\n",
    "                    s_out.logits.view(-1, s_out.logits.size(-1)),\n",
    "                    batch[\"labels\"].view(-1)\n",
    "                )\n",
    "                loss = 0.5 * ce + 0.5 * loss_kd\n",
    "            else:\n",
    "                loss = loss_kd\n",
    "\n",
    "        # Backward + update\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        # Log training metrics\n",
    "        if global_step % logging_steps == 0:\n",
    "            wandb.log({\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/lr\": scheduler.get_last_lr()[0],\n",
    "                \"step\": global_step,\n",
    "                \"epoch\": epoch + global_step / total_steps\n",
    "            })\n",
    "\n",
    "        # Step-level evaluation\n",
    "        if global_step % eval_steps == 0:\n",
    "            student.eval()\n",
    "            total_eval_loss, eval_batches = 0.0, 0\n",
    "            for ev_batch in eval_loader:\n",
    "                ev_batch = {k: v.to(device) for k, v in ev_batch.items()}\n",
    "                with torch.no_grad():\n",
    "                    t_eval = teacher(**ev_batch)\n",
    "                    s_eval = student(**ev_batch)\n",
    "                    eval_loss = distill_loss(s_eval.logits, t_eval.logits, T)\n",
    "                total_eval_loss += eval_loss.item()\n",
    "                eval_batches += 1\n",
    "            avg_eval_loss = total_eval_loss / eval_batches\n",
    "            wandb.log({\"eval/loss\": avg_eval_loss, \"step\": global_step})\n",
    "            student.train()\n",
    "\n",
    "        # Checkpointing\n",
    "        if global_step % save_steps == 0:\n",
    "            ckpt_dir = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            student.save_pretrained(ckpt_dir)\n",
    "            wandb.save(f\"{ckpt_dir}/*\")\n",
    "\n",
    "    # End-of-epoch validation\n",
    "    student.eval()\n",
    "    total_val_loss, val_batches = 0.0, 0\n",
    "    for ev_batch in eval_loader:\n",
    "        ev_batch = {k: v.to(device) for k, v in ev_batch.items()}\n",
    "        with torch.no_grad():\n",
    "            t_eval = teacher(**ev_batch)\n",
    "            s_eval = student(**ev_batch)\n",
    "            val_loss = distill_loss(s_eval.logits, t_eval.logits, T)\n",
    "        total_val_loss += val_loss.item()\n",
    "        val_batches += 1\n",
    "    avg_val_loss = total_val_loss / val_batches\n",
    "    wandb.log({\"validation/loss\": avg_val_loss, \"epoch\": epoch})\n",
    "    print(f\"Finished epoch {epoch}/{epochs} - val_loss: {avg_val_loss:.4f}\")\n",
    "    student.train()\n",
    "\n",
    "# ─── Final save ────────────────────────────────────────────\n",
    "student.save_pretrained(output_dir)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba9d026-5fad-428a-b34c-598d5d0e8cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 17:07:05.580267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-13 17:07:05.602981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-13 17:07:05.603028: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-13 17:07:05.619235: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 17:07:06.670164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 84.99 GB\n",
      "Trainable params: 61,440 / 8,016,187 (0.77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2530463/2409737060.py:110: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msdowell\u001b[0m (\u001b[33msdowell1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sdowell/scratch/Thesis/distillation/wandb/run-20250513_170711-fzsjdf50</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sdowell1/esm2-distill/runs/fzsjdf50' target=\"_blank\">epoch-based-logging</a></strong> to <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sdowell1/esm2-distill/runs/fzsjdf50' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill/runs/fzsjdf50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Training..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2530463/2409737060.py:174: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................. Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 32.7651 | Val Loss: 30.2341 | ETA: 5.7h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 2/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 13.3446 | Val Loss: 19.9945 | ETA: 5.5h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 3/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 10.3499 | Val Loss: 16.5507 | ETA: 5.4h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 4/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 8.9819 | Val Loss: 14.4167 | ETA: 5.3h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 5/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 8.1924 | Val Loss: 12.9661 | ETA: 5.2h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 6/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.8m | Train Loss: 7.6486 | Val Loss: 12.0280 | ETA: 5.1h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 7/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.7m | Train Loss: 7.2768 | Val Loss: 11.4825 | ETA: 4.9h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 8/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 7.0145 | Val Loss: 11.0668 | ETA: 4.8h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 9/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 6.8453 | Val Loss: 10.8387 | ETA: 4.7h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 10/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.7m | Train Loss: 6.7105 | Val Loss: 10.4650 | ETA: 4.6h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved at epoch 10\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 11/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 6.5618 | Val Loss: 10.2540 | ETA: 4.5h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 12/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 6.4931 | Val Loss: 10.1126 | ETA: 4.4h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 13/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 6.3922 | Val Loss: 10.0256 | ETA: 4.3h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 14/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 6.3388 | Val Loss: 9.8688 | ETA: 4.2h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 15/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 6.2764 | Val Loss: 9.7304 | ETA: 4.0h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 16/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 6.2010 | Val Loss: 9.5911 | ETA: 3.9h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 17/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 6.1474 | Val Loss: 9.5174 | ETA: 3.8h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 18/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 6.1106 | Val Loss: 9.4386 | ETA: 3.7h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 19/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 6.0842 | Val Loss: 9.3360 | ETA: 3.6h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 20/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 6.0539 | Val Loss: 9.5191 | ETA: 3.5h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved at epoch 20\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 21/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 6.0374 | Val Loss: 9.2900 | ETA: 3.4h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 22/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.9915 | Val Loss: 9.1626 | ETA: 3.3h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 23/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 5.9488 | Val Loss: 9.0885 | ETA: 3.1h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 24/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.3m | Train Loss: 5.9394 | Val Loss: 9.1638 | ETA: 3.0h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 25/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 5.9280 | Val Loss: 9.0781 | ETA: 2.9h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 26/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.8m | Train Loss: 5.8758 | Val Loss: 8.9774 | ETA: 2.8h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 27/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 5.8733 | Val Loss: 8.9116 | ETA: 2.7h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 28/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 5.8518 | Val Loss: 8.9344 | ETA: 2.6h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 29/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 5.8465 | Val Loss: 8.9647 | ETA: 2.4h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 30/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.8275 | Val Loss: 8.9211 | ETA: 2.3h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved at epoch 30\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 31/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.8138 | Val Loss: 8.8721 | ETA: 2.2h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 32/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.8115 | Val Loss: 8.8764 | ETA: 2.1h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 33/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 5.7760 | Val Loss: 8.7723 | ETA: 2.0h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 34/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 5.7643 | Val Loss: 8.7732 | ETA: 1.9h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 35/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.7531 | Val Loss: 8.6936 | ETA: 1.7h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 36/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.7544 | Val Loss: 8.7852 | ETA: 1.6h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 37/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.7298 | Val Loss: 8.7382 | ETA: 1.5h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 38/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 5.7080 | Val Loss: 8.8018 | ETA: 1.4h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 39/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 5.7107 | Val Loss: 8.5873 | ETA: 1.3h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 40/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.6931 | Val Loss: 8.6108 | ETA: 1.2h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved at epoch 40\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 41/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 5.6992 | Val Loss: 8.6905 | ETA: 1.0h\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 42/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.8m | Train Loss: 5.6881 | Val Loss: 8.5810 | ETA: 56m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 43/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.2m | Train Loss: 5.6782 | Val Loss: 8.5956 | ETA: 49m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 44/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.6747 | Val Loss: 8.5740 | ETA: 42m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 45/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.0m | Train Loss: 5.6737 | Val Loss: 8.5622 | ETA: 35m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 46/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 5.6608 | Val Loss: 8.5655 | ETA: 28m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 47/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 7.1m | Train Loss: 5.6459 | Val Loss: 8.6399 | ETA: 21m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 48/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.7m | Train Loss: 5.6472 | Val Loss: 8.4962 | ETA: 14m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 49/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.8m | Train Loss: 5.6296 | Val Loss: 8.4925 | ETA: 7m\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch 50/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Completed in 6.9m | Train Loss: 5.6324 | Val Loss: 8.5515 | ETA: 0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint saved at epoch 50\n",
      "------------------------------------------------------------\n",
      "\n",
      "Saving final model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.419 MB of 3.419 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>▂▄▅▇████▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_loss</td><td>5.63245</td></tr><tr><td>validation_loss</td><td>8.5515</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">epoch-based-logging</strong> at: <a href='https://wandb.ai/sdowell1/esm2-distill/runs/fzsjdf50' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill/runs/fzsjdf50</a><br/> View project at: <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 15 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_170711-fzsjdf50/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Total time: 5.8 hours\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── Hyper-parameters ─────────────────────────────────────\n",
    "teacher_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "student_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_8m_ecoli_finetuning_2/checkpoint-11500\"\n",
    "train_fasta         = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/train.fasta\"\n",
    "valid_fasta         = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/valid.fasta\"\n",
    "\n",
    "output_dir       = \"distilled-esm2-8M\"\n",
    "epochs           = 50  # Increased for more epochs\n",
    "train_bs         = 16\n",
    "eval_bs          = 32\n",
    "lr               = 1e-4\n",
    "mlm_prob         = 0.15\n",
    "T                = 2.0\n",
    "save_epochs      = 10  # Save every 10 epochs\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# ─── Models & Tokenizer ────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/esm2_t30_150M_UR50D\", do_lower_case=False\n",
    ")\n",
    "base_teacher = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t30_150M_UR50D\", output_hidden_states=True, return_dict=True\n",
    ")\n",
    "base_student = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\", output_hidden_states=True, return_dict=True\n",
    ")\n",
    "teacher = PeftModel.from_pretrained(base_teacher, teacher_lora_path)\n",
    "student = PeftModel.from_pretrained(base_student, student_lora_path)\n",
    "\n",
    "# Properly handle parameter freezing\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# For student: freeze only the base model, keep adapters trainable\n",
    "for name, param in student.named_parameters():\n",
    "    if \"lora\" not in name.lower():  # Keep LoRA parameters trainable\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True  # Explicitly set LoRA parameters as trainable\n",
    "\n",
    "# Verify we have trainable parameters\n",
    "trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "student.train()\n",
    "\n",
    "# ─── Data ─────────────────────────────────────────────────\n",
    "ds = load_dataset(\n",
    "    \"text\", data_files={\"train\": train_fasta, \"validation\": valid_fasta}\n",
    ")\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, max_length=1024\n",
    "    )\n",
    "ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds  = ds[\"validation\"]\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=True, mlm_probability=mlm_prob\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=train_bs, shuffle=True, collate_fn=collator\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds, batch_size=eval_bs, shuffle=False, collate_fn=collator\n",
    ")\n",
    "\n",
    "# ─── Optimizer & Scheduler ────────────────────────────────\n",
    "# Only optimize parameters that require gradients\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, student.parameters()), \n",
    "    lr=lr\n",
    ")\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# ─── Mixed-precision setup ─────────────────────────────────\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# ─── W&B setup ─────────────────────────────────────────────\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"esm2-distill\",\n",
    "    name=\"epoch-based-logging\",\n",
    "    config={\n",
    "        \"epochs\": epochs,\n",
    "        \"train_bs\": train_bs,\n",
    "        \"eval_bs\": eval_bs,\n",
    "        \"lr\": lr,\n",
    "        \"mlm_prob\": mlm_prob,\n",
    "        \"temperature\": T,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ─── Distillation loss ────────────────────────────────────\n",
    "def distill_loss(s_logits, t_logits, T):\n",
    "    s = s_logits / T\n",
    "    t = t_logits / T\n",
    "    kld = F.kl_div(\n",
    "        F.log_softmax(s, dim=-1),\n",
    "        F.softmax(t, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    )\n",
    "    return kld * (T * T)\n",
    "\n",
    "# ─── Training loop ────────────────────────────────────────\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress all tqdm output except final result\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_steps = 0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Simple progress indicator without tqdm\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "    print(\"Training...\", end='', flush=True)\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Teacher forward (no gradient needed)\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(**batch)\n",
    "\n",
    "        # Student forward + compute loss\n",
    "        with torch.cuda.amp.autocast():\n",
    "            s_out = student(**batch)\n",
    "            loss_kd = distill_loss(s_out.logits, t_out.logits, T)\n",
    "            \n",
    "            if batch.get(\"labels\") is not None:\n",
    "                ce = F.cross_entropy(\n",
    "                    s_out.logits.view(-1, s_out.logits.size(-1)),\n",
    "                    batch[\"labels\"].view(-1)\n",
    "                )\n",
    "                loss = 0.5 * ce + 0.5 * loss_kd\n",
    "            else:\n",
    "                loss = loss_kd\n",
    "\n",
    "        # Backward + update\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Track epoch training loss\n",
    "        epoch_train_loss += loss.item()\n",
    "        epoch_train_steps += 1\n",
    "        \n",
    "        # Print progress dots every 100 batches\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('.', end='', flush=True)\n",
    "    \n",
    "    print(\" Done!\")\n",
    "    \n",
    "    # End-of-epoch: calculate average training loss\n",
    "    avg_train_loss = epoch_train_loss / epoch_train_steps\n",
    "    \n",
    "    # End-of-epoch validation\n",
    "    print(\"Validating...\", end='', flush=True)\n",
    "    student.eval()\n",
    "    total_val_loss, val_batches = 0.0, 0\n",
    "    \n",
    "    for i, ev_batch in enumerate(eval_loader):\n",
    "        ev_batch = {k: v.to(device) for k, v in ev_batch.items()}\n",
    "        with torch.no_grad():\n",
    "            t_eval = teacher(**ev_batch)\n",
    "            s_eval = student(**ev_batch)\n",
    "            val_loss = distill_loss(s_eval.logits, t_eval.logits, T)\n",
    "        total_val_loss += val_loss.item()\n",
    "        val_batches += 1\n",
    "        \n",
    "        # Print progress dots every 50 batches\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('.', end='', flush=True)\n",
    "    \n",
    "    print(\" Done!\")\n",
    "    avg_val_loss = total_val_loss / val_batches\n",
    "    \n",
    "    # Calculate times\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_time = time.time() - start_time\n",
    "    eta_seconds = (total_time / epoch) * (epochs - epoch)\n",
    "    eta_str = f\"{eta_seconds/3600:.1f}h\" if eta_seconds > 3600 else f\"{eta_seconds/60:.0f}m\"\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"validation_loss\": avg_val_loss,\n",
    "        \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Completed in {epoch_time/60:.1f}m | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | ETA: {eta_str}\")\n",
    "    \n",
    "    student.train()\n",
    "    \n",
    "    # Save checkpoints at specified intervals\n",
    "    if epoch % save_epochs == 0:\n",
    "        ckpt_dir = os.path.join(output_dir, f\"checkpoint-epoch-{epoch}\")\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        student.save_pretrained(ckpt_dir)\n",
    "        wandb.save(f\"{ckpt_dir}/*\")\n",
    "        print(f\"✓ Checkpoint saved at epoch {epoch}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# ─── Final save ────────────────────────────────────────────\n",
    "print(\"\\nSaving final model...\")\n",
    "student.save_pretrained(output_dir)\n",
    "wandb.finish()\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed! Total time: {total_time/3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c5792a-cb0c-4ad5-a649-6e8e72be1855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 84.99 GB\n",
      "Trainable params: 61,440 / 8,016,187 (0.77%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2381c793d0c44f885d5cf53e85e4dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/97597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a275a270e5fb471bb249573a06b4f122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18263 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2530463/1264446787.py:108: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sdowell/scratch/Thesis/distillation/wandb/run-20250513_231545-8wk7j382</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sdowell1/esm2-distill/runs/8wk7j382' target=\"_blank\">epoch-based-logging</a></strong> to <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sdowell1/esm2-distill' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sdowell1/esm2-distill/runs/8wk7j382' target=\"_blank\">https://wandb.ai/sdowell1/esm2-distill/runs/8wk7j382</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n",
      "Training..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2530463/1264446787.py:143: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................. Done!\n",
      "Validating.............. Done!\n",
      "Train Loss: 87.3363 | Val Loss: 64.1484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 3 files into the W&B run directory, call wandb.save again to sync new files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Best checkpoint saved\n",
      "\n",
      "Epoch 2/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Train Loss: 35.6082 | Val Loss: 46.0822\n",
      "✓ Best checkpoint saved\n",
      "\n",
      "Epoch 3/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Train Loss: 28.2499 | Val Loss: 41.5601\n",
      "✓ Best checkpoint saved\n",
      "\n",
      "Epoch 4/50\n",
      "Training................................................................ Done!\n",
      "Validating.............. Done!\n",
      "Train Loss: 25.1349 | Val Loss: 39.7372\n",
      "✓ Best checkpoint saved\n",
      "\n",
      "Epoch 5/50\n",
      "Training............"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import PeftModel\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ─── Hyper-parameters ─────────────────────────────────────\n",
    "teacher_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_150m_ecoli_finetuning_1/checkpoint-19000\"\n",
    "student_lora_path = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/runs/esm_8m_ecoli_finetuning_2/checkpoint-11500\"\n",
    "train_fasta = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/train.fasta\"\n",
    "valid_fasta = \"../BenchmarkingFinetuning/dataset_splits/finetuning_dataset/valid.fasta\"\n",
    "\n",
    "output_dir = \"distilled-esm2-8M\"\n",
    "epochs = 50\n",
    "train_bs = 16\n",
    "eval_bs = 32\n",
    "lr = 1e-4\n",
    "mlm_prob = 0.15\n",
    "T = 2.0\n",
    "save_epochs = 10\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# ─── Models & Tokenizer ────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\", do_lower_case=False)\n",
    "\n",
    "teacher_config = AutoConfig.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
    "teacher_config.output_hidden_states = True\n",
    "teacher_config.return_dict = True\n",
    "base_teacher = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t30_150M_UR50D\", config=teacher_config\n",
    ")\n",
    "\n",
    "student_config = AutoConfig.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    attention_probs_dropout_prob=0.2,\n",
    "    hidden_dropout_prob=0.2\n",
    ")\n",
    "student_config.output_hidden_states = True\n",
    "student_config.return_dict = True\n",
    "base_student = AutoModelForMaskedLM.from_pretrained(\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    config=student_config\n",
    ")\n",
    "\n",
    "teacher = PeftModel.from_pretrained(base_teacher, teacher_lora_path)\n",
    "student = PeftModel.from_pretrained(base_student, student_lora_path)\n",
    "\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for name, param in student.named_parameters():\n",
    "    param.requires_grad = \"lora\" in name.lower()\n",
    "\n",
    "trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in student.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} / {all_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n",
    "\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "student.train()\n",
    "\n",
    "# ─── Data ─────────────────────────────────────────────────\n",
    "ds = load_dataset(\"text\", data_files={\"train\": train_fasta, \"validation\": valid_fasta})\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds = ds[\"validation\"]\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
    "train_loader = DataLoader(train_ds, batch_size=train_bs, shuffle=True, collate_fn=collator)\n",
    "eval_loader = DataLoader(eval_ds, batch_size=eval_bs, shuffle=False, collate_fn=collator)\n",
    "\n",
    "# ─── Optimizer & Scheduler ────────────────────────────────\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, student.parameters()),\n",
    "    lr=lr,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# ─── Mixed-precision setup ─────────────────────────────────\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# ─── W&B setup ─────────────────────────────────────────────\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"esm2-distill\",\n",
    "    name=\"epoch-based-logging\",\n",
    "    config={\"epochs\": epochs, \"train_bs\": train_bs, \"eval_bs\": eval_bs, \"lr\": lr, \"mlm_prob\": mlm_prob, \"temperature\": T}\n",
    ")\n",
    "\n",
    "# ─── Distillation loss ────────────────────────────────────\n",
    "def distill_loss(s_logits, t_logits, T):\n",
    "    s = s_logits / T\n",
    "    t = t_logits / T\n",
    "    kld = F.kl_div(F.log_softmax(s, dim=-1), F.softmax(t, dim=-1), reduction=\"batchmean\")\n",
    "    return kld * (T * T)\n",
    "\n",
    "# ─── Training loop ────────────────────────────────────────\n",
    "start_time = time.time()\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_train_steps = 0\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\\nTraining...\", end='', flush=True)\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_out = teacher(**batch)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            s_out = student(**batch)\n",
    "            loss_kd = distill_loss(s_out.logits, t_out.logits, T)\n",
    "            ce = F.cross_entropy(\n",
    "                s_out.logits.view(-1, s_out.logits.size(-1)),\n",
    "                batch[\"labels\"].view(-1)\n",
    "            ) if batch.get(\"labels\") is not None else 0.0\n",
    "            loss = 0.5 * ce + 0.5 * loss_kd\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        epoch_train_steps += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('.', end='', flush=True)\n",
    "\n",
    "    print(\" Done!\")\n",
    "    avg_train_loss = epoch_train_loss / epoch_train_steps\n",
    "\n",
    "    # Validation\n",
    "    print(\"Validating...\", end='', flush=True)\n",
    "    student.eval()\n",
    "    total_val_loss, val_batches = 0.0, 0\n",
    "\n",
    "    for i, ev_batch in enumerate(eval_loader):\n",
    "        ev_batch = {k: v.to(device) for k, v in ev_batch.items()}\n",
    "        with torch.no_grad():\n",
    "            t_eval = teacher(**ev_batch)\n",
    "            s_eval = student(**ev_batch)\n",
    "            val_loss = distill_loss(s_eval.logits, t_eval.logits, T)\n",
    "        total_val_loss += val_loss.item()\n",
    "        val_batches += 1\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('.', end='', flush=True)\n",
    "\n",
    "    print(\" Done!\")\n",
    "    avg_val_loss = total_val_loss / val_batches\n",
    "\n",
    "    wandb.log({\"train_loss\": avg_train_loss, \"validation_loss\": avg_val_loss, \"learning_rate\": scheduler.get_last_lr()[0], \"epoch\": epoch})\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        ckpt_dir = os.path.join(output_dir, \"best-checkpoint\")\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        student.save_pretrained(ckpt_dir)\n",
    "        wandb.save(f\"{ckpt_dir}/*\")\n",
    "        print(\"✓ Best checkpoint saved\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"⚠️  No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if epoch % save_epochs == 0:\n",
    "        ckpt_dir = os.path.join(output_dir, f\"checkpoint-epoch-{epoch}\")\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        student.save_pretrained(ckpt_dir)\n",
    "        wandb.save(f\"{ckpt_dir}/*\")\n",
    "        print(f\"✓ Checkpoint saved at epoch {epoch}\")\n",
    "    student.train()\n",
    "\n",
    "print(\"\\nSaving final model...\")\n",
    "student.save_pretrained(output_dir)\n",
    "wandb.finish()\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16ddbe-a8ef-474a-873b-79c54ec58963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

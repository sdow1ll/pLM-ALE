{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df7daa2-bea0-4953-943b-9c82ab30922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sdowell/scratch/Thesis/ADP1\n"
     ]
    }
   ],
   "source": [
    "cd /home/sdowell/scratch/Thesis/ADP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1885f9-0df5-4358-a611-0ecd256d98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 23:25:57.954318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-13 23:25:57.970974: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-13 23:25:57.971003: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-13 23:25:57.982413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 23:25:59.169112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained MLM Cross Entropy Loss: 1.8642\n",
      "Finetuned MLM Cross Entropy Loss: 2.8570\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t6_8M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/distillation/distilled-esm2-8M/checkpoint-epoch-50/\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_mlm_loss(model, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes the MLM (masked language model) cross entropy loss for a given sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence as a string.\n",
    "        mask_prob: The probability of masking a token.\n",
    "        device: torch.device to run the computation.\n",
    "    \n",
    "    Returns:\n",
    "        loss: The MLM cross entropy loss.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()  # or model.eval() if you don't want dropout, etc.\n",
    "    \n",
    "    # Tokenize the sequence\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    \n",
    "    # Create labels as a copy of input_ids.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a mask for positions to replace.\n",
    "    # Generate random values in [0, 1) for each token.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    # Create a boolean mask for tokens to mask.\n",
    "    mask = probability_matrix < mask_prob\n",
    "    \n",
    "    # For positions NOT selected for masking, set the corresponding label to -100 so they are ignored.\n",
    "    labels[~mask] = -100\n",
    "    \n",
    "    # Replace the selected input positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    input_ids[mask] = mask_token_id\n",
    "    \n",
    "    # Forward pass: the model automatically computes the loss when labels are provided.\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sequence = \"MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKALIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEAGAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLYRAGQSVERTAQQAAAFVKAYREAVQ\"\n",
    "loss = compute_mlm_loss(model_pretrained, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Pretrained MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "loss = compute_mlm_loss(model_finetuned, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Finetuned MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a30a0f-9316-407f-91e6-f4449249036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained Token Prediction Accuracy: 0.2812 (9/32)\n",
      "finetuned Token Prediction Accuracy: 0.1935 (6/31)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability with which to mask tokens (e.g., 0.15 for 15%).\n",
    "        device: torch.device to run the computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (accuracy, correct, total_masked) where:\n",
    "          - accuracy is the fraction of masked tokens correctly predicted.\n",
    "          - correct is the number of correct predictions.\n",
    "          - total_masked is the total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of the original tokens to serve as labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens according to mask_prob.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace the selected token positions in input_ids with the mask token ID.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    \n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch_size, sequence_length, vocab_size]\n",
    "    \n",
    "    # Get predictions (top candidate from the logits) using argmax.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Consider only the masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    # Calculate the number of correct predictions.\n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    accuracy = correct / total_masked if total_masked > 0 else 0.0\n",
    "    return accuracy, correct, total_masked\n",
    "\n",
    "sequence = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_pretrained, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"pretrained Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_finetuned, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"finetuned Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aea29c6-a2e4-4dbf-ac88-f3b2d5257ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 469/469 [00:09<00:00, 50.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 469 sequences.\n",
      "pretrained Overall Token Prediction Accuracy: 0.1965 (9423/47957)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 469/469 [00:09<00:00, 49.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 469 sequences.\n",
      "finetuned Overall Token Prediction Accuracy: 0.2446 (11686/47771)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "base_model_name = \"facebook/esm2_t6_8M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a given sequence from a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability of masking a token (default is 15%).\n",
    "        device: Torch device on which to run computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (correct, total_masked) where:\n",
    "          - correct: number of masked tokens correctly predicted.\n",
    "          - total_masked: total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of input_ids for ground-truth labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace tokens at masked positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch, seq_length, vocab_size]\n",
    "    \n",
    "    # Get predicted token IDs.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Evaluate only on masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    return correct, total_masked\n",
    "\n",
    "# ----- Main script to process FASTA file with a progress bar -----\n",
    "\n",
    "fasta_file = \"/home/sdowell/scratch/Thesis/BenchmarkingFinetuning/dataset_splits/finetuning_dataset/test.fasta\"\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"pretrained Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n",
    "\n",
    "\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"finetuned Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f78f8-00ae-4cbe-828d-ab1a936162aa",
   "metadata": {},
   "source": [
    "# ESM-2 Pretrained Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66bb4e5b-2000-4c09-8e67-43ec29d09898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 469it [00:08, 54.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000        70\n",
      "       <eos>  1.0000000000 1.0000000000 1.0000000000        59\n",
      "           A  0.1623203832 0.1401654412 0.1504315660      4352\n",
      "           C  0.4672897196 0.5223880597 0.4933051445       670\n",
      "           D  0.2033755274 0.1916500994 0.1973387922      2515\n",
      "           E  0.1417518652 0.3074617920 0.1940425532      3337\n",
      "           F  0.2912621359 0.0665188470 0.1083032491      1804\n",
      "           G  0.2363601311 0.3773468760 0.2906590801      3249\n",
      "           H  0.7938144330 0.1015831135 0.1801169591       758\n",
      "           I  0.3655462185 0.0763492760 0.1263157895      2279\n",
      "           K  0.1733905579 0.1957996769 0.1839150228      3095\n",
      "           L  0.1791778976 0.5633474576 0.2718813906      4720\n",
      "           M  0.9241379310 0.1012849584 0.1825613079      1323\n",
      "           N  0.5243243243 0.0562645012 0.1016238869      1724\n",
      "           P  0.1956034096 0.1981818182 0.1968841725      2200\n",
      "           Q  0.2815533981 0.0162283156 0.0306878307      1787\n",
      "           R  0.2120294245 0.1404413872 0.1689655172      3489\n",
      "           S  0.1036496350 0.0274237157 0.0433720220      2589\n",
      "           T  0.1957671958 0.0589641434 0.0906307410      2510\n",
      "           V  0.1643433602 0.1388286334 0.1505123467      3227\n",
      "           W  0.5376344086 0.0578034682 0.1043841336       865\n",
      "           X  0.7500000000 0.7500000000 0.7500000000         4\n",
      "           Y  0.2835820896 0.0466257669 0.0800842993      1630\n",
      "\n",
      "    accuracy                      0.1956440650     48256\n",
      "   macro avg  0.3994310455 0.2667242325 0.2650441654     48256\n",
      "weighted avg  0.2541383516 0.1956440650 0.1703039862     48256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_token_predictions_for_metrics(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Returns token-level predictions and labels for masked positions, to support precision/recall/F1.\n",
    "    \n",
    "    Returns:\n",
    "        Two lists: true token IDs and predicted token IDs at masked positions.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "\n",
    "    # Ensure mask token is defined\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    true_tokens = labels[mask_positions].tolist()\n",
    "    predicted_tokens = predictions[mask_positions].tolist()\n",
    "\n",
    "    return true_tokens, predicted_tokens\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55db1d-01aa-4414-8985-5cc05aee1d15",
   "metadata": {},
   "source": [
    "# ESM-2 fine-tuned Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b313046c-6cfc-44fa-86fa-4f760dce7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 469it [00:09, 48.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 0.9661016949 0.9827586207        59\n",
      "       <eos>  1.0000000000 0.5660377358 0.7228915663        53\n",
      "           A  0.1571646084 0.4321935334 0.2305067924      4299\n",
      "           C  0.8794326241 0.5462555066 0.6739130435       681\n",
      "           D  0.5100671141 0.1871921182 0.2738738739      2436\n",
      "           E  0.2095214481 0.2565270188 0.2306537464      3294\n",
      "           F  0.2355263158 0.1028144744 0.1431427429      1741\n",
      "           G  0.3323833274 0.2777612385 0.3026273111      3359\n",
      "           H  0.2081447964 0.0589743590 0.0919080919       780\n",
      "           I  0.2682682683 0.1178540018 0.1637641308      2274\n",
      "           K  0.4443277311 0.1372040221 0.2096654275      3083\n",
      "           L  0.2272926338 0.5626626193 0.3237881340      4612\n",
      "           M  0.6120218579 0.1728395062 0.2695547533      1296\n",
      "           N  0.3649815043 0.1734036321 0.2351072280      1707\n",
      "           P  0.2342215989 0.0779281381 0.1169467787      2143\n",
      "           Q  0.2085967130 0.0918196995 0.1275115920      1797\n",
      "           R  0.2287581699 0.3095518868 0.2630919569      3392\n",
      "           S  0.4790159190 0.1281455672 0.2021991448      2583\n",
      "           T  0.3532388664 0.1341275942 0.1944289694      2602\n",
      "           V  0.2266857963 0.2445820433 0.2352941176      3230\n",
      "           W  0.2500000000 0.0543209877 0.0892494929       810\n",
      "           X  1.0000000000 0.2500000000 0.4000000000         4\n",
      "           Y  0.4116279070 0.2142857143 0.2818471338      1652\n",
      "\n",
      "    accuracy                      0.2471025539     47887\n",
      "   macro avg  0.4278816174 0.2635905692 0.2941184630     47887\n",
      "weighted avg  0.3107389489 0.2471025539 0.2354118236     47887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec7cad2c-6077-48a7-a3c9-9e6742d16af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 469it [00:36, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pretrained Model Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000        67\n",
      "       <eos>  1.0000000000 1.0000000000 1.0000000000        77\n",
      "           A  0.4179775281 0.4384724187 0.4279797515      4242\n",
      "           C  0.7202702703 0.7454545455 0.7326460481       715\n",
      "           D  0.3835068054 0.3880113406 0.3857459231      2469\n",
      "           E  0.3353526612 0.4807692308 0.3951057864      3224\n",
      "           F  0.4971464807 0.4260869565 0.4588820603      1840\n",
      "           G  0.5311727547 0.5985762922 0.5628637951      3231\n",
      "           H  0.7008797654 0.3208053691 0.4401473297       745\n",
      "           I  0.5675029869 0.4190560212 0.4821111393      2267\n",
      "           K  0.2875275938 0.5024108004 0.3657423657      3111\n",
      "           L  0.4108950453 0.6286937029 0.4969794603      4907\n",
      "           M  0.7707509881 0.2950075643 0.4266958425      1322\n",
      "           N  0.4785932722 0.1899271845 0.2719374457      1648\n",
      "           P  0.4022503516 0.5202364711 0.4536981955      2199\n",
      "           Q  0.4889406286 0.2267818575 0.3098487643      1852\n",
      "           R  0.5520481022 0.4375930891 0.4882020605      3357\n",
      "           S  0.4423611111 0.2556179775 0.3240081384      2492\n",
      "           T  0.5069848661 0.3444049031 0.4101718860      2529\n",
      "           V  0.3886834320 0.3268034826 0.3550675676      3216\n",
      "           W  0.5486238532 0.3598074609 0.4345930233       831\n",
      "           X  1.0000000000 1.0000000000 1.0000000000         3\n",
      "           Y  0.4970845481 0.4281230383 0.4600337268      1593\n",
      "\n",
      "    accuracy                      0.4355508271     47937\n",
      "   macro avg  0.5621110020 0.4927234655 0.5079330570     47937\n",
      "weighted avg  0.4619973325 0.4355508271 0.4313195713     47937\n",
      "\n",
      "\n",
      "=== Finetuned Model Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000        67\n",
      "       <eos>  1.0000000000 0.8311688312 0.9078014184        77\n",
      "           A  0.9952996475 0.9983498350 0.9968224079      4242\n",
      "           C  1.0000000000 0.9902097902 0.9950808152       715\n",
      "           D  0.9963369963 0.9914945322 0.9939098660      2469\n",
      "           E  0.9965933726 0.9981389578 0.9973655664      3224\n",
      "           F  0.9983686786 0.9978260870 0.9980973091      1840\n",
      "           G  0.9984476871 0.9953574745 0.9969001860      3231\n",
      "           H  0.9919786096 0.9959731544 0.9939718687       745\n",
      "           I  0.9951541850 0.9964711072 0.9958122107      2267\n",
      "           K  0.9983943481 0.9993571199 0.9988755020      3111\n",
      "           L  0.9973523422 0.9979620950 0.9976571254      4907\n",
      "           M  0.9954579864 0.9947049924 0.9950813470      1322\n",
      "           N  0.9897032102 0.9915048544 0.9906032131      1648\n",
      "           P  0.9945454545 0.9949977262 0.9947715390      2199\n",
      "           Q  0.9951508621 0.9973002160 0.9962243797      1852\n",
      "           R  0.9955396967 0.9973190349 0.9964285714      3357\n",
      "           S  0.9927855711 0.9939807384 0.9933827953      2492\n",
      "           T  0.9913283406 0.9944642151 0.9928938018      2529\n",
      "           V  0.9981279251 0.9947139303 0.9964180034      3216\n",
      "           W  0.9963985594 0.9987966306 0.9975961538       831\n",
      "           X  1.0000000000 1.0000000000 1.0000000000         3\n",
      "           Y  0.9987429290 0.9974890144 0.9981155779      1593\n",
      "\n",
      "    accuracy                      0.9959738824     47937\n",
      "   macro avg  0.9963350610 0.9890252320 0.9923395504     47937\n",
      "weighted avg  0.9959785023 0.9959738824 0.9959619908     47937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from Bio import SeqIO\n",
    "\n",
    "# ---------- 1. Helper functions ----------\n",
    "\n",
    "def prepare_masked_input(tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Prepares masked input_ids and labels for consistent evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        masked input_ids, ground truth labels, attention_mask, mask_positions\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer does not have a mask token.\")\n",
    "    \n",
    "    masked_input_ids = input_ids.clone()\n",
    "    masked_input_ids[mask_positions] = mask_token_id\n",
    "\n",
    "    return masked_input_ids, labels, attention_mask, mask_positions\n",
    "\n",
    "def compute_predictions(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes model predictions.\n",
    "    \n",
    "    Returns:\n",
    "        predictions tensor\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "# ---------- 2. Evaluation loop ----------\n",
    "\n",
    "def evaluate_models(fasta_file, model_pretrained, model_finetuned, tokenizer, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Evaluates both models on the same masked inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    all_true_tokens = []\n",
    "\n",
    "    all_preds_pretrained = []\n",
    "    all_preds_finetuned = []\n",
    "\n",
    "    for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "        seq = str(record.seq).strip()\n",
    "        if not seq:\n",
    "            continue\n",
    "\n",
    "        # Step 1: Prepare masked inputs\n",
    "        masked_input_ids, labels, attention_mask, mask_positions = prepare_masked_input(tokenizer, seq, mask_prob=mask_prob, device=device)\n",
    "\n",
    "        # Step 2: Predictions\n",
    "        preds_pretrained = compute_predictions(model_pretrained, masked_input_ids, attention_mask)\n",
    "        preds_finetuned = compute_predictions(model_finetuned, masked_input_ids, attention_mask)\n",
    "\n",
    "        # Step 3: Select only masked positions\n",
    "        true_at_mask = labels[mask_positions].tolist()\n",
    "        preds_pretrained_at_mask = preds_pretrained[mask_positions].tolist()\n",
    "        preds_finetuned_at_mask = preds_finetuned[mask_positions].tolist()\n",
    "\n",
    "        all_true_tokens.extend(true_at_mask)\n",
    "        all_preds_pretrained.extend(preds_pretrained_at_mask)\n",
    "        all_preds_finetuned.extend(preds_finetuned_at_mask)\n",
    "\n",
    "    # ---------- 3. Generate reports ----------\n",
    "\n",
    "    id2token = tokenizer.convert_ids_to_tokens\n",
    "\n",
    "    true_tokens = [id2token(t) for t in all_true_tokens]\n",
    "    preds_pretrained_tokens = [id2token(p) for p in all_preds_pretrained]\n",
    "    preds_finetuned_tokens = [id2token(p) for p in all_preds_finetuned]\n",
    "\n",
    "    print(\"=== Pretrained Model Report ===\")\n",
    "    print(classification_report(true_tokens, preds_pretrained_tokens, zero_division=0, digits=10))\n",
    "\n",
    "    print(\"\\n=== Finetuned Model Report ===\")\n",
    "    print(classification_report(true_tokens, preds_finetuned_tokens, zero_division=0, digits=10))\n",
    "\n",
    "# ---------- 4. Usage example ----------\n",
    "\n",
    "# Assuming you already have these:\n",
    "# model_pretrained, model_finetuned, tokenizer, fasta_file, device\n",
    "\n",
    "evaluate_models(fasta_file, model_pretrained, model_finetuned, tokenizer, mask_prob=0.15, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a683556-33d2-421e-9453-d1d81861878b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

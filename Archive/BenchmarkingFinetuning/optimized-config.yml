# Data paths
train_path: "./dataset_splits/finetuning_dataset/train.fasta"
#train_path: "scratch/Thesis/ADP1/finetuning_data/train"
eval_path: "./dataset_splits/finetuning_dataset/valid.fasta"
base_model: "facebook/esm2_t6_8M_UR50D"
#base_model: "facebook/esm2_t30_150M_UR50D"
#base_model: "hugohrban/progen2-small"
#base_model: "facebook/esm2_t33_650M_UR50D"

# Weights & Biases (WandB) configuration
wandb_project: "esm2_8m_student_distill"
#wandb_notes: "Optimized training with gradient accumulation and stable memory usage"

# Model parameters
#hidden_dropout_prob: 0.2
#attention_probs_dropout_prob: 0.2

# Masked Language Model (MLM) training
#mlm_probability: 0.15  # Probability of masking tokens for MLM training

# Early stopping
#early_stopping_patience: 5
#early_stopping_threshold: 0.001

training_args:
  output_dir: runs/esm2_8m_student_distill_3
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 16
  fp16: true

  # ← EVALUATE once per epoch
  evaluation_strategy: "epoch"
  # ← LOGGING once per epoch
  logging_strategy: "epoch"

  # report metrics to WandB
  report_to: ["wandb"]

  # you can still save checkpoints every epoch
  save_strategy: "epoch"
  load_best_model_at_end: true
  num_train_epochs: 100
 

# config.yaml
train_path: '/home/idies/workspace/Storage/sdowell/persistent/ALEdb/split_data/train.fasta'
eval_path: '/home/idies/workspace/Storage/sdowell/persistent/ALEdb/split_data/valid.fasta'
base_model: 'facebook/esm2_t6_8M_UR50D'
wandb_project: 'FIX_esm_8m_ecoli_finetuning'

training_args:

  output_dir: runs/esm_8m_ecoli_finetuning_1
  run_name: "ESM2-8M-finetuning-run1"
  num_train_epochs: 10000
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 64
  eval_steps: 500
  learning_rate: 0.0001

  # cannot use 128 or 256 for batch sizes...
  # 5 epochs is approximately ~24 hrs with batch size 64 and eval steps 100 to finetune the model
  # actually the above is a lie... takes way longer. need to figure out what is going on

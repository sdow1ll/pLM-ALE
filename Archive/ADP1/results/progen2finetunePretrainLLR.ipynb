{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa906dd6-f225-450d-b1b8-41ad7c0f6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 18:22:40.378910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-18 18:22:40.396502: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-18 18:22:40.396534: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-18 18:22:40.408757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-18 18:22:41.589914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for hugohrban/progen2-small contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/hugohrban/progen2-small.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for hugohrban/progen2-small contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/hugohrban/progen2-small.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for hugohrban/progen2-small contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/hugohrban/progen2-small.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for hugohrban/progen2-small contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/hugohrban/progen2-small.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ProGenForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model                | Mutation   |   logP(wt) |   logP(mut) |       LLR |\n",
      "|:---------------------|:-----------|-----------:|------------:|----------:|\n",
      "| ProGen2 (pretrained) | F33I       |  -1.9043   |   -1.46858  |  0.435722 |\n",
      "| ProGen2 (finetuned)  | F33I       |  -0.179315 |   -1.87186  | -1.69254  |\n",
      "| ProGen2 (pretrained) | D58N       |  -1.77341  |   -2.95379  | -1.18038  |\n",
      "| ProGen2 (finetuned)  | D58N       |  -0.198515 |   -1.74838  | -1.54987  |\n",
      "| ProGen2 (pretrained) | A75V       |  -1.54974  |   -4.85515  | -3.30541  |\n",
      "| ProGen2 (finetuned)  | A75V       |  -0.514432 |   -0.942083 | -0.42765  |\n",
      "| ProGen2 (pretrained) | Q72H       |  -0.883275 |   -3.46503  | -2.58176  |\n",
      "| ProGen2 (finetuned)  | Q72H       |  -0.248708 |   -1.52757  | -1.27886  |\n",
      "| ProGen2 (pretrained) | V85A       |  -0.103677 |   -5.3091   | -5.20542  |\n",
      "| ProGen2 (finetuned)  | V85A       |  -0.485737 |   -0.964421 | -0.478683 |\n",
      "| ProGen2 (pretrained) | V154F      |  -0.33779  |   -5.13948  | -4.80169  |\n",
      "| ProGen2 (finetuned)  | V154F      |  -0.746392 |   -0.665322 |  0.08107  |\n",
      "| ProGen2 (pretrained) | Y180F      |  -0.345715 |   -1.44847  | -1.10276  |\n",
      "| ProGen2 (finetuned)  | Y180F      |  -6.051    |   -0.002503 |  6.0485   |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# --- 1. Setup & model loading ----------------------------------------------\n",
    "\n",
    "base_model_name    = \"hugohrban/progen2-small\"\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/progen2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# pretrained & finetuned models\n",
    "model_pretrained   = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
    "model_with_adapter = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
    "model_finetuned    = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint).to(device)\n",
    "\n",
    "# --- 2. Sequence & mutations -----------------------------------------------\n",
    "\n",
    "DgoA_seq  = (\n",
    "    \"MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA\"\n",
    "    \"LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA\"\n",
    "    \"GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY\"\n",
    "    \"RAGQSVERTAQQAAAFVKAYREAVQ\"\n",
    ")\n",
    "mutations = ['F33I','D58N','A75V','Q72H','V85A','V154F','Y180F']\n",
    "\n",
    "# --- 3. Utility functions --------------------------------------------------\n",
    "\n",
    "def parse_mutation(mut_str):\n",
    "    \"\"\"Parse mutation like 'F33I' â†’ (wt='F', pos=33, mut='I')\"\"\"\n",
    "    wt      = mut_str[0]\n",
    "    pos     = int(mut_str[1:-1])\n",
    "    mutant  = mut_str[-1]\n",
    "    return wt, pos, mutant\n",
    "\n",
    "def compute_mutation_llr_autoregressive(model, tokenizer, sequence, mutation, device):\n",
    "    \"\"\"\n",
    "    Returns (llr, log_prob_wt, log_prob_mut) for an AR model.\n",
    "    \"\"\"\n",
    "    wt, pos, mut = parse_mutation(mutation)\n",
    "    idx   = pos - 1  # zeroâ€‘based\n",
    "    if sequence[idx] != wt:\n",
    "        print(f\"Warning: expected {wt} at {pos}, got {sequence[idx]}\")\n",
    "    \n",
    "    # prefix up toâ€”but not includingâ€”the mutated position\n",
    "    prefix = sequence[:idx]\n",
    "    inputs = tokenizer(prefix, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits  # [1, L, V]\n",
    "        next_logits = logits[0, -1, :]   # [V]\n",
    "        probs = torch.softmax(next_logits, dim=-1)\n",
    "    \n",
    "    # convert singleâ€‘letter tokens to IDs\n",
    "    wt_id  = tokenizer.convert_tokens_to_ids(wt)\n",
    "    mut_id = tokenizer.convert_tokens_to_ids(mut)\n",
    "    \n",
    "    # avoid log(0)\n",
    "    eps = 1e-12\n",
    "    p_wt  = probs[wt_id].item()\n",
    "    p_mut = probs[mut_id].item()\n",
    "    lp_wt  = math.log(p_wt  + eps)\n",
    "    lp_mut = math.log(p_mut + eps)\n",
    "    \n",
    "    llr = lp_mut - lp_wt\n",
    "    return llr, lp_wt, lp_mut\n",
    "\n",
    "# --- 4. Compute & report -----------------------------------\n",
    "\n",
    "records = []\n",
    "for mutation in mutations:\n",
    "    for model, label in [\n",
    "        (model_pretrained, \"ProGen2 (pretrained)\"),\n",
    "        (model_finetuned,  \"ProGen2 (finetuned)\")\n",
    "    ]:\n",
    "        llr, lp_wt, lp_mut = compute_mutation_llr_autoregressive(\n",
    "            model, tokenizer, DgoA_seq, mutation, device\n",
    "        )\n",
    "        records.append({\n",
    "            \"Model\":     label,\n",
    "            \"Mutation\":  mutation,\n",
    "            \"logP(wt)\":  f\"{lp_wt:.6f}\",\n",
    "            \"logP(mut)\": f\"{lp_mut:.6f}\",\n",
    "            \"LLR\":       f\"{llr:.6f}\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records, columns=[\"Model\",\"Mutation\",\"logP(wt)\",\"logP(mut)\",\"LLR\"])\n",
    "print(df.to_markdown(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516cc6b-bb53-4f69-8120-2113547784d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

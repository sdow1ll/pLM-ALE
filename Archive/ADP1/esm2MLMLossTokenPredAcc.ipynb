{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1885f9-0df5-4358-a611-0ecd256d98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:30:43.481496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-27 16:30:43.498205: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-27 16:30:43.498232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-27 16:30:43.509811: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 16:30:44.507209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained MLM Cross Entropy Loss: 1.3430\n",
      "Finetuned MLM Cross Entropy Loss: 0.0133\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Example inputs\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_mlm_loss(model, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes the MLM (masked language model) cross entropy loss for a given sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Protein sequence as a string.\n",
    "        mask_prob: The probability of masking a token.\n",
    "        device: torch.device to run the computation.\n",
    "    \n",
    "    Returns:\n",
    "        loss: The MLM cross entropy loss.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()  # or model.eval() if you don't want dropout, etc.\n",
    "    \n",
    "    # Tokenize the sequence\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    \n",
    "    # Create labels as a copy of input_ids.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a mask for positions to replace.\n",
    "    # Generate random values in [0, 1) for each token.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    # Create a boolean mask for tokens to mask.\n",
    "    mask = probability_matrix < mask_prob\n",
    "    \n",
    "    # For positions NOT selected for masking, set the corresponding label to -100 so they are ignored.\n",
    "    labels[~mask] = -100\n",
    "    \n",
    "    # Replace the selected input positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    input_ids[mask] = mask_token_id\n",
    "    \n",
    "    # Forward pass: the model automatically computes the loss when labels are provided.\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sequence = \"MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKALIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEAGAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLYRAGQSVERTAQQAAAFVKAYREAVQ\"\n",
    "loss = compute_mlm_loss(model_pretrained, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Pretrained MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "loss = compute_mlm_loss(model_finetuned, sequence, mask_prob=0.15, device=device)\n",
    "print(f\"Finetuned MLM Cross Entropy Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a30a0f-9316-407f-91e6-f4449249036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained Token Prediction Accuracy: 0.3929 (11/28)\n",
      "finetuned Token Prediction Accuracy: 1.0000 (24/24)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability with which to mask tokens (e.g., 0.15 for 15%).\n",
    "        device: torch.device to run the computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (accuracy, correct, total_masked) where:\n",
    "          - accuracy is the fraction of masked tokens correctly predicted.\n",
    "          - correct is the number of correct predictions.\n",
    "          - total_masked is the total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of the original tokens to serve as labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens according to mask_prob.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace the selected token positions in input_ids with the mask token ID.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    \n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch_size, sequence_length, vocab_size]\n",
    "    \n",
    "    # Get predictions (top candidate from the logits) using argmax.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Consider only the masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    # Calculate the number of correct predictions.\n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    accuracy = correct / total_masked if total_masked > 0 else 0.0\n",
    "    return accuracy, correct, total_masked\n",
    "\n",
    "sequence = (\n",
    "    'MQWQTKLPLIAILRGITPDEALAHVGAVIDAGFDAVEIPLNSPQWEQSIPAIVDAYGDKA'\n",
    "    'LIGAGTVLKPEQVDALARMGCQLIVTPNIHSEVIRRAVGYGMTVCPGCATATEAFTALEA'\n",
    "    'GAQALKIFPSSAFGPQYIKALKAVLPSDIAVFAVGGVTPENLAQWIDAGCAGAGLGSDLY'\n",
    "    'RAGQSVERTAQQAAAFVKAYREAVQ'\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_pretrained, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"pretrained Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n",
    "accuracy, correct, total_masked = compute_token_prediction_accuracy(\n",
    "    model_finetuned, tokenizer, sequence, mask_prob=0.15, device=device\n",
    ")\n",
    "print(f\"finetuned Token Prediction Accuracy: {accuracy:.4f} ({correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aea29c6-a2e4-4dbf-ac88-f3b2d5257ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [00:51<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "pretrained Overall Token Prediction Accuracy: 0.5504 (32944/59852)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1953/1953 [01:07<00:00, 28.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1953 sequences.\n",
      "finetuned Overall Token Prediction Accuracy: 0.9837 (58985/59963)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoModelForMaskedLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from autoamp.evolveFinetune import *\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from Bio import SeqIO \n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "base_model_name = \"facebook/esm2_t30_150M_UR50D\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "adapter_checkpoint = \"/home/sdowell/scratch/Thesis/ADP1/runs/esm2_dgoa_finetune_1/checkpoint-3000\"\n",
    "\n",
    "# Load models\n",
    "model_pretrained = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_with_adapter = AutoModelForMaskedLM.from_pretrained(base_model_name)\n",
    "model_finetuned = PeftModel.from_pretrained(model_with_adapter, adapter_checkpoint)\n",
    "\n",
    "def compute_token_prediction_accuracy(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Computes token prediction accuracy for a given sequence from a masked language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The masked language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        sequence: A string representing the protein (or other) sequence.\n",
    "        mask_prob: The probability of masking a token (default is 15%).\n",
    "        device: Torch device on which to run computations.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (correct, total_masked) where:\n",
    "          - correct: number of masked tokens correctly predicted.\n",
    "          - total_masked: total number of masked tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize input sequence.\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    # Create a copy of input_ids for ground-truth labels.\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create a random mask for tokens.\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "    \n",
    "    # Replace tokens at masked positions with the mask token.\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"The tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "    \n",
    "    # Forward pass through the model.\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: [batch, seq_length, vocab_size]\n",
    "    \n",
    "    # Get predicted token IDs.\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Evaluate only on masked positions.\n",
    "    masked_labels = labels[mask_positions]\n",
    "    masked_predictions = predictions[mask_positions]\n",
    "    \n",
    "    correct = (masked_predictions == masked_labels).sum().item()\n",
    "    total_masked = mask_positions.sum().item()\n",
    "    \n",
    "    return correct, total_masked\n",
    "\n",
    "# ----- Main script to process FASTA file with a progress bar -----\n",
    "\n",
    "fasta_file = \"/home/sdowell/scratch/Thesis/ADP1/finetuning_data/test/dgoa_mutants_test.fasta\"\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"pretrained Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n",
    "\n",
    "\n",
    "\n",
    "total_correct = 0\n",
    "total_masked = 0\n",
    "n_sequences = 0\n",
    "\n",
    "# Count number of records in FASTA for progress bar (optional, if file is large)\n",
    "records = list(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "n_records = len(records)\n",
    "\n",
    "# Iterate over sequences with a progress bar.\n",
    "for record in tqdm(records, desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    correct, masked = compute_token_prediction_accuracy(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    total_correct += correct\n",
    "    total_masked += masked\n",
    "    n_sequences += 1\n",
    "\n",
    "# Compute overall accuracy.\n",
    "overall_accuracy = total_correct / total_masked if total_masked > 0 else 0.0\n",
    "\n",
    "print(f\"\\nProcessed {n_sequences} sequences.\")\n",
    "print(f\"finetuned Overall Token Prediction Accuracy: {overall_accuracy:.4f} ({total_correct}/{total_masked})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f78f8-00ae-4cbe-828d-ab1a936162aa",
   "metadata": {},
   "source": [
    "# ESM-2 Pretrained Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66bb4e5b-2000-4c09-8e67-43ec29d09898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 1953it [00:51, 38.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       292\n",
      "       <eos>  1.0000000000 1.0000000000 1.0000000000       300\n",
      "           A  0.5594525235 0.6846733668 0.6157612278      9552\n",
      "           C  0.0000000000 0.0000000000 0.0000000000      1150\n",
      "           D  0.3456896552 0.3436161097 0.3446497636      2334\n",
      "           E  0.5600841441 0.7114228457 0.6267470943      2994\n",
      "           F  0.3869565217 0.2918032787 0.3327102804      1830\n",
      "           G  0.9061837796 0.8583941606 0.8816418330      5480\n",
      "           H  0.0372670807 0.0109489051 0.0169252468       548\n",
      "           I  0.5784687592 0.4726749760 0.5202479884      4172\n",
      "           K  0.3698347107 0.3231046931 0.3448940270      2216\n",
      "           L  0.4502942580 0.6065995286 0.5168888077      4667\n",
      "           M  0.8897338403 0.4952380952 0.6363018355       945\n",
      "           N  0.6953748006 0.6733590734 0.6841898784      1295\n",
      "           P  0.6525709748 0.8504527814 0.7384857335      3865\n",
      "           Q  0.3439306358 0.0727161625 0.1200504414      3273\n",
      "           R  0.2930315664 0.4920000000 0.3673012318      2000\n",
      "           S  0.5879904875 0.4204931973 0.4903321765      2352\n",
      "           T  0.4996003197 0.4562043796 0.4769172072      2740\n",
      "           V  0.4572574178 0.5452285332 0.4973831124      5229\n",
      "           W  0.2704081633 0.0620608899 0.1009523810       854\n",
      "           X  0.0000000000 0.0000000000 0.0000000000        10\n",
      "           Y  0.5969884854 0.5253312549 0.5588723051      1283\n",
      "\n",
      "    accuracy                      0.5471952308     59381\n",
      "   macro avg  0.4991790489 0.4737531405 0.4726631553     59381\n",
      "weighted avg  0.5315416848 0.5471952308 0.5268425893     59381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_token_predictions_for_metrics(model, tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Returns token-level predictions and labels for masked positions, to support precision/recall/F1.\n",
    "    \n",
    "    Returns:\n",
    "        Two lists: true token IDs and predicted token IDs at masked positions.\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "\n",
    "    # Ensure mask token is defined\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer does not have a mask token.\")\n",
    "    input_ids[mask_positions] = mask_token_id\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    true_tokens = labels[mask_positions].tolist()\n",
    "    predicted_tokens = predictions[mask_positions].tolist()\n",
    "\n",
    "    return true_tokens, predicted_tokens\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_pretrained, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55db1d-01aa-4414-8985-5cc05aee1d15",
   "metadata": {},
   "source": [
    "# ESM-2 fine-tuned Recall, Precision, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b313046c-6cfc-44fa-86fa-4f760dce7ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 1953it [01:07, 28.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       279\n",
      "       <eos>  1.0000000000 0.9894366197 0.9946902655       284\n",
      "       <unk>  0.0000000000 0.0000000000 0.0000000000         1\n",
      "           A  0.9862832758 0.9901060792 0.9881909804      9804\n",
      "           C  0.9945945946 0.9901345291 0.9923595506      1115\n",
      "           D  0.9664000000 0.9829129373 0.9745865268      2458\n",
      "           E  0.9854088904 0.9837398374 0.9845736566      2952\n",
      "           F  0.9704109589 0.9435269046 0.9567801189      1877\n",
      "           G  0.9931494502 0.9965629522 0.9948532731      5528\n",
      "           H  0.9877551020 0.8673835125 0.9236641221       558\n",
      "           I  0.9909274194 0.9764092376 0.9836147592      4027\n",
      "           K  0.9808917197 0.9800000000 0.9804456571      2200\n",
      "           L  0.9957654033 0.9917756221 0.9937665082      4742\n",
      "           M  0.9859813084 0.9836829837 0.9848308051       858\n",
      "           N  0.9825479930 0.9615713066 0.9719464825      1171\n",
      "           P  0.9875559980 0.9932415519 0.9903906153      3995\n",
      "           Q  0.9718804921 0.9928186715 0.9822380107      3342\n",
      "           R  0.9866730129 0.9899713467 0.9883194279      2094\n",
      "           S  0.9773987207 0.9662731872 0.9718041128      2372\n",
      "           T  0.9862810530 0.9797421731 0.9830007391      2715\n",
      "           V  0.9710556186 0.9805157593 0.9757627602      5235\n",
      "           W  0.9988700565 0.9977426637 0.9983060418       886\n",
      "           X  1.0000000000 0.4736842105 0.6428571429        19\n",
      "           Y  0.9588100686 0.9928909953 0.9755529686      1266\n",
      "\n",
      "    accuracy                      0.9838067516     59778\n",
      "   macro avg  0.9441100473 0.9168384617 0.9263556052     59778\n",
      "weighted avg  0.9838460395 0.9838067516 0.9837270055     59778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_trues = []\n",
    "all_preds = []\n",
    "\n",
    "# Use same file path and device setup as before\n",
    "for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "    seq = str(record.seq).strip()\n",
    "    if not seq:\n",
    "        continue\n",
    "    trues, preds = compute_token_predictions_for_metrics(model_finetuned, tokenizer, seq, mask_prob=0.15, device=device)\n",
    "    all_trues.extend(trues)\n",
    "    all_preds.extend(preds)\n",
    "\n",
    "# Optionally convert to token strings for readability\n",
    "id2token = tokenizer.convert_ids_to_tokens\n",
    "true_tokens = [id2token(t) for t in all_trues]\n",
    "pred_tokens = [id2token(p) for p in all_preds]\n",
    "\n",
    "# Print precision, recall, F1 for each amino acid\n",
    "print(classification_report(true_tokens, pred_tokens, zero_division=0, digits=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b2778a-5173-4c13-86e4-e6776eb2e411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 1953it [01:43, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pretrained Model Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       298\n",
      "       <eos>  1.0000000000 1.0000000000 1.0000000000       282\n",
      "           A  0.5714886106 0.6824934752 0.6220778627      9962\n",
      "           C  0.0000000000 0.0000000000 0.0000000000      1146\n",
      "           D  0.3593489149 0.3420738975 0.3504986770      2517\n",
      "           E  0.5663481953 0.7207024654 0.6342695794      2961\n",
      "           F  0.3711126469 0.2978369384 0.3304615385      1803\n",
      "           G  0.9025263952 0.8545162442 0.8778653952      5602\n",
      "           H  0.0264900662 0.0076335878 0.0118518519       524\n",
      "           I  0.5746632273 0.4868868383 0.5271460497      4118\n",
      "           K  0.3732503888 0.3208556150 0.3450754853      2244\n",
      "           L  0.4626911787 0.6148634777 0.5280324401      4871\n",
      "           M  0.8865784499 0.5016042781 0.6407103825       935\n",
      "           N  0.6792777300 0.6616415410 0.6703436572      1194\n",
      "           P  0.6391454965 0.8399089530 0.7259016393      3954\n",
      "           Q  0.3552971576 0.0827814570 0.1342773438      3322\n",
      "           R  0.2825524576 0.4885685885 0.3580404298      2012\n",
      "           S  0.5694282380 0.4190639760 0.4828097947      2329\n",
      "           T  0.5082288401 0.4740497076 0.4905446293      2736\n",
      "           V  0.4652593085 0.5302140557 0.4956175299      5279\n",
      "           W  0.2477876106 0.0665083135 0.1048689139       842\n",
      "           X  0.0000000000 0.0000000000 0.0000000000         8\n",
      "           Y  0.5748449956 0.5078247261 0.5392604902      1278\n",
      "\n",
      "    accuracy                      0.5486324460     60217\n",
      "   macro avg  0.4963617351 0.4739142668 0.4725936387     60217\n",
      "weighted avg  0.5327972365 0.5486324460 0.5292688818     60217\n",
      "\n",
      "\n",
      "=== Finetuned Model Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <cls>  1.0000000000 1.0000000000 1.0000000000       298\n",
      "       <eos>  1.0000000000 0.9680851064 0.9837837838       282\n",
      "           A  0.9846384040 0.9908652881 0.9877420323      9962\n",
      "           C  0.9921602787 0.9938917976 0.9930252833      1146\n",
      "           D  0.9625730994 0.9809296782 0.9716646989      2517\n",
      "           E  0.9844278944 0.9821006417 0.9832628910      2961\n",
      "           F  0.9686609687 0.9428729895 0.9555930298      1803\n",
      "           G  0.9928838285 0.9962513388 0.9945647331      5602\n",
      "           H  0.9932432432 0.8416030534 0.9111570248       524\n",
      "           I  0.9906496063 0.9776590578 0.9841114642      4118\n",
      "           K  0.9869604317 0.9781639929 0.9825425246      2244\n",
      "           L  0.9950627443 0.9930199138 0.9940402795      4871\n",
      "           M  0.9902702703 0.9796791444 0.9849462366       935\n",
      "           N  0.9843205575 0.9463986600 0.9649871904      1194\n",
      "           P  0.9901664145 0.9931714719 0.9916666667      3954\n",
      "           Q  0.9682446339 0.9912703191 0.9796221925      3322\n",
      "           R  0.9812807882 0.9900596421 0.9856506680      2012\n",
      "           S  0.9766738661 0.9708029197 0.9737295435      2329\n",
      "           T  0.9907646842 0.9802631579 0.9854859453      2736\n",
      "           V  0.9705716963 0.9808675886 0.9756924816      5279\n",
      "           W  0.9964285714 0.9940617577 0.9952437574       842\n",
      "           X  1.0000000000 0.6250000000 0.7692307692         8\n",
      "           Y  0.9649657273 0.9913928013 0.9780007719      1278\n",
      "\n",
      "    accuracy                      0.9835428533     60217\n",
      "   macro avg  0.9854325091 0.9603656661 0.9706845204     60217\n",
      "weighted avg  0.9836153242 0.9835428533 0.9834791541     60217\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from Bio import SeqIO\n",
    "\n",
    "# ---------- 1. Helper functions ----------\n",
    "\n",
    "def prepare_masked_input(tokenizer, sequence, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Prepares masked input_ids and labels for consistent evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        masked input_ids, ground truth labels, attention_mask, mask_positions\n",
    "    \"\"\"\n",
    "    encoded = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "    labels = input_ids.clone()\n",
    "    probability_matrix = torch.rand(input_ids.shape).to(device)\n",
    "    mask_positions = probability_matrix < mask_prob\n",
    "\n",
    "    mask_token_id = tokenizer.mask_token_id\n",
    "    if mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer does not have a mask token.\")\n",
    "    \n",
    "    masked_input_ids = input_ids.clone()\n",
    "    masked_input_ids[mask_positions] = mask_token_id\n",
    "\n",
    "    return masked_input_ids, labels, attention_mask, mask_positions\n",
    "\n",
    "def compute_predictions(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes model predictions.\n",
    "    \n",
    "    Returns:\n",
    "        predictions tensor\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "# ---------- 2. Evaluation loop ----------\n",
    "\n",
    "def evaluate_models(fasta_file, model_pretrained, model_finetuned, tokenizer, mask_prob=0.15, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Evaluates both models on the same masked inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    all_true_tokens = []\n",
    "\n",
    "    all_preds_pretrained = []\n",
    "    all_preds_finetuned = []\n",
    "\n",
    "    for record in tqdm(SeqIO.parse(fasta_file, \"fasta\"), desc=\"Processing sequences\"):\n",
    "        seq = str(record.seq).strip()\n",
    "        if not seq:\n",
    "            continue\n",
    "\n",
    "        # Step 1: Prepare masked inputs\n",
    "        masked_input_ids, labels, attention_mask, mask_positions = prepare_masked_input(tokenizer, seq, mask_prob=mask_prob, device=device)\n",
    "\n",
    "        # Step 2: Predictions\n",
    "        preds_pretrained = compute_predictions(model_pretrained, masked_input_ids, attention_mask)\n",
    "        preds_finetuned = compute_predictions(model_finetuned, masked_input_ids, attention_mask)\n",
    "\n",
    "        # Step 3: Select only masked positions\n",
    "        true_at_mask = labels[mask_positions].tolist()\n",
    "        preds_pretrained_at_mask = preds_pretrained[mask_positions].tolist()\n",
    "        preds_finetuned_at_mask = preds_finetuned[mask_positions].tolist()\n",
    "\n",
    "        all_true_tokens.extend(true_at_mask)\n",
    "        all_preds_pretrained.extend(preds_pretrained_at_mask)\n",
    "        all_preds_finetuned.extend(preds_finetuned_at_mask)\n",
    "\n",
    "    # ---------- 3. Generate reports ----------\n",
    "\n",
    "    id2token = tokenizer.convert_ids_to_tokens\n",
    "\n",
    "    true_tokens = [id2token(t) for t in all_true_tokens]\n",
    "    preds_pretrained_tokens = [id2token(p) for p in all_preds_pretrained]\n",
    "    preds_finetuned_tokens = [id2token(p) for p in all_preds_finetuned]\n",
    "\n",
    "    print(\"=== Pretrained Model Report ===\")\n",
    "    print(classification_report(true_tokens, preds_pretrained_tokens, zero_division=0, digits=10))\n",
    "\n",
    "    print(\"\\n=== Finetuned Model Report ===\")\n",
    "    print(classification_report(true_tokens, preds_finetuned_tokens, zero_division=0, digits=10))\n",
    "\n",
    "# ---------- 4. Usage example ----------\n",
    "\n",
    "# Assuming you already have these:\n",
    "# model_pretrained, model_finetuned, tokenizer, fasta_file, device\n",
    "\n",
    "evaluate_models(fasta_file, model_pretrained, model_finetuned, tokenizer, mask_prob=0.15, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe24e43-414d-415e-ae24-ff54a15af152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

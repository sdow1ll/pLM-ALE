# Data paths
train_path: "./finetuning_data/train/dgoa_mutants_train.fasta"
eval_path: "./finetuning_data/valid/dgoa_mutants_validation.fasta"
#base_model: "facebook/esm2_t30_150M_UR50D"
base_model: "hugohrban/progen2-small"
#base_model: "facebook/esm2_t33_650M_UR50D"

# Weights & Biases (WandB) configuration
wandb_project: "progen2_151m_dgoa_finetuning"
#wandb_notes: "Optimized training with gradient accumulation and stable memory usage"

# Model parameters
#hidden_dropout_prob: 0.2
#attention_probs_dropout_prob: 0.2

# Masked Language Model (MLM) training
#mlm_probability: 0.15  # Probability of masking tokens for MLM training

# Early stopping
#early_stopping_patience: 5
#early_stopping_threshold: 0.001

# Training arguments (ENSURED ALL PARAMETERS EXIST IN TrainingArguments)
training_args:
  output_dir: "runs/progen2_dgoa_finetune_1"
  run_name: "progen2_dg0a_finetune_1" 

  # Batch size (optimized for memory usage)
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 8  
  fp16: true
  # Training duration
  num_train_epochs: 100
 
